{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc25d6b",
   "metadata": {},
   "source": [
    "# : take a multiple pdf with text,image,table\n",
    "1. fetch the data from pdf\n",
    "2. at lesat there should be 200 pages\n",
    "3. if chunking(use the sementic chunking technique) required do chunking and then embedding\n",
    "4. store it inside the vector database(use any of them 1. mongodb 2. astradb 3. opensearch 4.milvus) ## i have not discuss then you need to explore\n",
    "5. create a index with all three index machnism(Flat, HNSW, IVF) ## i have not discuss then you need to explore\n",
    "6. create a retriever pipeline\n",
    "7. check the retriever time(which one is fastet)\n",
    "8. print the accuray score of every similarity search\n",
    "9. perform the reranking either using BM25 or MMR ## i have not discuss then you need to explore\n",
    "10. then write a prompt template\n",
    "11. generte a oputput through llm\n",
    "12. render that output over the DOCx ## i have not discuss then you need to explore\n",
    "as a additional tip: you can follow rag playlist from my youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7578c42",
   "metadata": {},
   "source": [
    "### Load the pdf with text , images and tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e928965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders.parsers import TesseractBlobParser\n",
    "\n",
    "\n",
    "loader = PyMuPDFLoader(\n",
    "    file_path = \"/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf\",\n",
    "    extract_images = True, \n",
    "    images_parser = TesseractBlobParser(), \n",
    "    extract_tables = \"markdown\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e5b10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42cfa726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='A Survey of Context Engineering for Large\\nLanguage Models\\nLingrui Mei1,6,†\\nJiayu Yao1,6,†\\nYuyao Ge1,6,†\\nYiwei Wang2\\nBaolong Bi1,6,†\\nYujun Cai3\\nJiazhi Liu1\\nMingyu Li1\\nZhong-Zhi Li6\\nDuzhen Zhang6\\nChenlin Zhou4\\nJiayi Mao5\\nTianze Xia6\\nJiafeng Guo1,6,†\\nShenghua Liu1,6,†,\\n1 Institute of Computing Technology, Chinese Academy of Sciences,\\n2 University of California, Merced, 3 The University of Queensland\\n4 Peking University, 5 Tsinghua University,\\n6 University of Chinese Academy of Sciences\\nAbstract: The performance of Large Language Models (LLMs) is fundamentally determined by the contextual\\ninformation provided during inference. This survey introduces Context Engineering, a formal discipline\\nthat transcends simple prompt design to encompass the systematic optimization of information payloads\\nfor LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational\\nComponents and the sophisticated Implementations that integrate them into intelligent systems. We first\\nexamine the foundational Components: (1) Context Retrieval and Generation, encompassing prompt-based\\ngeneration and external knowledge acquisition; (2) Context Processing, addressing long sequence processing,\\nself-refinement, and structured information integration; and (3) Context Management, covering memory\\nhierarchies, compression, and optimization. We then explore how these components are architecturally\\nintegrated to create sophisticated System Implementations: (1) Retrieval-Augmented Generation (RAG),\\nincluding modular, agentic, and graph-enhanced architectures; (2) Memory Systems, enabling persistent\\ninteractions; (3) Tool-Integrated Reasoning, for function calling and environmental interaction; and (4)\\nMulti-Agent Systems, coordinating communication and orchestration. Through this systematic analysis of over\\n1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical\\nresearch gap: a fundamental asymmetry exists between model capabilities. While current models, augmented\\nby advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they\\nexhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a\\ndefining priority for future research. Ultimately, this survey provides a unified framework for both researchers\\nand engineers advancing context-aware AI.\\n† Also affiliated with: (1)Key Laboratory of Network Data Science and Technology, ICT, CAS; (2)State Key\\nLaboratory of AI Safety\\nCorresponding Author\\nKeywords: Context Engineering, Large Language Models, LLM Agent, Multi-Agent Systems\\nDate: July 17, 2025\\nCode Repository: https://github.com/Meirtz/Awesome-Context-Engineering\\nContact: meilingrui23b@ict.ac.cn, liushenghua@ict.ac.cn\\narXiv:2507.13334v1  [cs.CL]  17 Jul 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Contents\\n1\\nIntroduction\\n4\\n2\\nRelated Work\\n5\\n3\\nWhy Context Engineering?\\n7\\n3.1\\nDefinition of Context Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n8\\n3.2\\nWhy Context Engineering\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.2.1\\nCurrent Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.2.2\\nPerformance Enhancement\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.2.3\\nResource Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n11\\n3.2.4\\nFuture Potential\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n4\\nFoundational Components\\n12\\n4.1\\nContext Retrieval and Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n12\\n4.1.1\\nPrompt Engineering and Context Generation . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n4.1.2\\nExternal Knowledge Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n4.1.3\\nDynamic Context Assembly\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n4.2\\nContext Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n4.2.1\\nLong Context Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n16\\n4.2.2\\nContextual Self-Refinement and Adaptation\\n. . . . . . . . . . . . . . . . . . . . . . .\\n18\\n4.2.3\\nMultimodal Context\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n20\\n4.2.4\\nRelational and Structured Context\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n4.3\\nContext Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n4.3.1\\nFundamental Constraints\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n4.3.2\\nMemory Hierarchies and Storage Architectures\\n. . . . . . . . . . . . . . . . . . . . .\\n24\\n4.3.3\\nContext Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n25\\n4.3.4\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n26\\n5\\nSystem Implementations\\n27\\n5.1\\nRetrieval-Augmented Generation\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n5.1.1\\nModular RAG Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n27\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='5.1.2\\nAgentic RAG Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n28\\n5.1.3\\nGraph-Enhanced RAG\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n29\\n5.1.4\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n30\\n5.2\\nMemory Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n5.2.1\\nMemory Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n5.2.2\\nMemory-Enhanced Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n5.2.3\\nEvaluation and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n35\\n5.3\\nTool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n37\\n5.3.1\\nFunction Calling Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n37\\n5.3.2\\nTool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n39\\n5.3.3\\nAgent-Environment Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n40\\n5.4\\nMulti-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n5.4.1\\nCommunication Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n42\\n5.4.2\\nOrchestration Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n5.4.3\\nCoordination Strategies\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n44\\n6\\nEvaluation\\n45\\n6.1\\nEvaluation Frameworks and Methodologies\\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n6.1.1\\nComponent-Level Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n6.1.2\\nSystem-Level Integration Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n6.2\\nBenchmark Datasets and Evaluation Paradigms\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.2.1\\nFoundational Component Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.2.2\\nSystem Implementation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n6.3\\nEvaluation Challenges and Emerging Paradigms . . . . . . . . . . . . . . . . . . . . . . . . .\\n48\\n6.3.1\\nMethodological Limitations and Biases . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.3.2\\nEmerging Evaluation Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n6.3.3\\nSafety and Robustness Assessment\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n50\\n7\\nFuture Directions and Open Challenges\\n50\\n7.1\\nFoundational Research Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n51\\n7.1.1\\nTheoretical Foundations and Unified Frameworks . . . . . . . . . . . . . . . . . . . .\\n51\\n7.1.2\\nScaling Laws and Computational Efficiency\\n. . . . . . . . . . . . . . . . . . . . . . .\\n51\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='7.1.3\\nMulti-Modal Integration and Representation . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n7.2\\nTechnical Innovation Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n52\\n7.2.1\\nNext-Generation Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n7.2.2\\nAdvanced Reasoning and Planning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n53\\n7.2.3\\nComplex Context Organization and Solving Graph Problems . . . . . . . . . . . . . .\\n54\\n7.2.4\\nIntelligent Context Assembly and Optimization\\n. . . . . . . . . . . . . . . . . . . . .\\n54\\n7.3\\nApplication-Driven Research Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n7.3.1\\nDomain Specialization and Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n7.3.2\\nLarge-Scale Multi-Agent Coordination\\n. . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\n7.3.3\\nHuman-AI Collaboration and Integration . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n7.4\\nDeployment and Societal Impact Considerations . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n7.4.1\\nScalability and Production Deployment . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n7.4.2\\nSafety, Security, and Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n57\\n7.4.3\\nEthical Considerations and Responsible Development . . . . . . . . . . . . . . . . . .\\n57\\n8\\nConclusion\\n58\\n1. Introduction\\nThe advent of LLMs has marked a paradigm shift in artificial intelligence, demonstrating unprecedented\\ncapabilities in natural language understanding, generation, and reasoning [103, 1059, 453]. However, the\\nperformance and efficacy of these models are fundamentally governed by the context they receive. This\\ncontext—ranging from simple instructional prompts to sophisticated external knowledge bases—serves as\\nthe primary mechanism through which their behavior is steered, their knowledge is augmented, and their\\ncapabilities are unleashed. As LLMs have evolved from basic instruction-following systems into the core\\nreasoning engines of complex applications, the methods for designing and managing their informational\\npayloads have correspondingly evolved into the formal discipline of Context Engineering [25, 1256, 1060].\\nThe landscape of context engineering has expanded at an explosive rate, resulting in a proliferation\\nof specialized yet fragmented research domains. We conceptualize this landscape as being composed of\\nfoundational components and their subsequent implementations. The foundational components represent the\\nsystematic pipeline of context engineering through three critical phases: Context Retrieval and Generation,\\nencompassing prompt-based generation and external knowledge acquisition [25, 591, 48]; Context Process-\\ning, involving long sequence processing, self-refinement mechanisms, and structured information integration\\n[196, 735, 489]; and Context Management, addressing memory hierarchies, compression techniques, and\\noptimization strategies [1362, 1074, 813].\\nThese foundational components serve as the building blocks for more complex, application-oriented im-\\nplementations that bridge LLMs to external realities. These systems include Advanced Retrieval-Augmented\\nGeneration (RAG), which has evolved into modular and agentic architectures for dynamic knowledge\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='injection [591, 312, 965, 311]; explicit Memory Systems that mimic human cognitive faculties for persistent\\ninformation retention [1182, 935, 1362]; and the entire ecosystem of Intelligent Agent Systems. This\\nlatter category represents the pinnacle of context engineering, where agents leverage Function Calling\\nand Tool-Integrated Reasoning to interact with the world [931, 858, 663], and rely on sophisticated\\nAgent Communication protocols and Context Orchestration to achieve complex goals in multi-agent\\nconfigurations [356, 246, 894, 128].\\nWhile each of these domains has generated substantial innovation, they are predominantly studied in\\nisolation. This fragmented development obscures the fundamental connections between techniques and\\ncreates significant barriers for researchers seeking to understand the broader landscape and practitioners\\naiming to leverage these methods effectively. The field urgently requires a unified framework that sys-\\ntematically organizes these diverse techniques, clarifies their underlying principles, and illuminates their\\ninterdependencies.\\nTo address this critical gap, this survey provides the first comprehensive and systematic review of\\nContext Engineering for LLMs. Our primary contribution is a novel, structured taxonomy that classifies\\nthe multifaceted techniques used to design, manage, and optimize context. This taxonomy organizes the\\nfield into coherent categories, distinguishing between foundational Components and their integration into\\nsophisticated System Implementations. Through this framework, we: (1) provide a clear and structured\\noverview of the state-of-the-art across each domain; (2) analyze the core mechanisms, strengths, and\\nlimitations of different approaches; and (3) identify overarching challenges and chart promising directions\\nfor future research. This work serves as both a technical roadmap for navigating the complex landscape of\\ncontext engineering and a foundation for fostering deeper understanding and catalyzing future innovation.\\nThe remainder of this paper is organized as follows. After discussing related work and formally defin-\\ning Context Engineering, we first examine the Foundational Components of the field, covering Context\\nRetrieval and Generation, Context Processing, and Context Management. We then explore their System\\nImplementations, including Retrieval-Augmented Generation, Memory Systems, Tool-Integrated Reasoning,\\nand Multi-Agent Systems. Finally, we discuss evaluation methodologies, future research directions, and con-\\nclude the survey. Figure 1 provides a comprehensive overview of our taxonomy, illustrating the hierarchical\\norganization of techniques and their relationships within the Context Engineering landscape.\\n2. Related Work\\nThe rapid maturation of LLMs has spurred a significant body of survey literature aiming to map its multifaceted\\nlandscape. This existing work, while valuable, has largely focused on specific vertical domains within the\\nbroader field of what we define as Context Engineering. Our survey seeks to complement these efforts by\\nproviding a horizontal, unifying taxonomy that distinguishes between foundational components and their\\nintegration into complex systems, thereby bridging these specialized areas.\\nFoundational Components\\nNumerous surveys have addressed the foundational Components of context\\nengineering that form the core technical capabilities for effective context manipulation. The challenge of\\nContext Retrieval and Generation encompasses both prompt engineering methodologies and external\\nknowledge acquisition techniques. Surveys on prompt engineering have cataloged the vast array of techniques\\nfor guiding LLM behavior, from basic few-shot methods to advanced, structured reasoning frameworks\\n[25, 253, 1313]. External knowledge retrieval and integration techniques, particularly through knowledge\\ngraphs and structured data sources, are reviewed in works that survey representation techniques, integration\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='Context Engineering\\nFoundational\\nComponents (§4)\\nContext Generation\\nRetrieval & (§4.1)\\ne.g., Chain-of-Thought [1138], Zero-shot CoT [553], ToT [1246], GoT [69], Self-consistency [1114],\\nReAct [1245], Auto-CoT [1099], Automatic Prompt [307] , CLEAR Framework [702], RAG [591],\\nCognitive Prompting [558], KAPING [48], Dynamic Assembly [307], etc.\\nContext\\nProcessing (§4.2)\\ne.g., Mamba [1258], LongNet [216], FlashAttention [196], Ring Attention [676], YaRN [833],\\nInfini-attention [792], StreamingLLM [1176], InfLLM [1175], Self-Refine [735], Reflexion [956],\\nStructGPT [489], GraphFormers [1221], KG Integration [1321], Long CoT [147], MLLMs [49], etc.\\nContext\\nManagement (§4.3)\\ne.g., Context Compression [317], StreamingLLM [1176], KV Cache Management [1389],\\nHeavy Hitter Oracle [1333], Hierarchical Memory [499], Recurrent Context Compression [441],\\nActivation Refilling [859], Context Window Management [1074], etc.\\nImplementations (§5)\\nRetrieval-Augmented\\nGeneration (§5.1)\\ne.g., FlashRAG [500], KRAGEN [749], ComposeRAG [1159], Self-RAG [41], CDF-RAG [531],\\nGraphRAG [374], LightRAG [360], HippoRAG [366], RAPTOR [928], RAG-Gym [1183],\\nAgentic RAG Systems [965], Graph-Enhanced RAG [832], Modular RAG Architectures [312], etc.\\nMemory\\nSystems (§5.2)\\ne.g., MemoryBank [1362], MemLLM [779], Self-Controlled Memory [649], REMEMBERER [1299],\\nMemOS [637], Charlie Mnemonic [578], RecMind [1115], Sandbox [455], LongMemEval [1171],\\nMADail-Bench [386], MEMENTO [566], A-MEM [1202], CAMELoT [393], Architectures [1182],\\nShort-term & Long-term Memory [935], MemGPT [813], Memory-Enhanced Agents [571], etc.\\nTool-Integrated\\nReasoning (§5.3)\\ne.g., Toolformer [931], ReAct [1245], Gorilla [828], ToolLLM [867], Granite-FunctionCalling [5],\\nProgram-Aided Language Models [305], ToRA [341], ReTool [270], Chameleon [709], a1 [760],\\nAPI-Bank [615], MCP-RADAR [310], GTA benchmark [1090], PLAY2PROMPT [259], etc.\\nMulti-Agent\\nSystems (§5.4)\\ne.g., KQML [280], FIPA ACL [1146], MCP protocols [37], A2A [1007], ACP [462], ANP [1],\\nAutoGen [1158], MetaGPT [408], CAMEL [600], CrewAI [184], Swarm Agent [808],\\n3S orchestrator [893], SagaLLM [128], Communication Protocols [1210], Orchestration [894],\\nCoordination Strategies [625], Agent Communication Languages [356], CoA [1327], etc.\\nEvaluation (§6)\\nEvaluation\\nFrameworks (§6.1)\\ne.g., Component-Level Assessment [835], System-Level Integration [1132], Self-Refinement [735],\\nMCP-RADAR [310], LongMemEval [1171], BFCL Tool Evaluation [829], SagaLLM [128],\\nBrittleness Assessment [1259], Contextual Calibration [380], Multi-dimensional Feedback [284], etc.\\nBenchmark\\nDatasets (§6.2)\\ne.g., GAIA [772], GTA [1090], WebArena [1368], VideoWebArena [476], Deep Research Bench [87],\\nStableToolBench [359], NesTools [373], ToolHop [1255], T-Eval [157], BFCL [829],\\nNarrativeQA [550], MEMENTO [566], API-Bank [615], Mind2Web [202], SWE-Bench [494], etc.\\nEvaluation\\nChallenges (§6.3)\\ne.g., Performance Gap Assessment [772, 1090], Memory System Isolation Problems [1330, 1171],\\nO(n2) Scaling Limitations [731, 295], Transactional Integrity [128], Multi-Tool Coordination [310],\\nSelf-Validation Dependencies [390], Context Handling Failures [210], Attribution Challenges [1113],\\nSafety-oriented Evaluation [87], Agent Assessment [965], Orchestration Evaluation [893], etc.\\nFuture Directions\\n& Challenges (§7)\\nFoundational\\nResearch (§7.1)\\ne.g., Theoretical Foundations [1132], Scaling Laws [731], O(n2) Computational Challenges [295],\\nMulti-modal Integration [476], Compositional Understanding [835], Context Optimization [663],\\nFrameworks for Multi-agent Coordination [128], Information-theoretic Analysis [310], etc.\\nTechnical\\nInnovation (§7.2)\\ne.g., LongMamba [1258], Sliding Attention [295], Memory-Augmented Architectures [1362],\\nModular RAG [312], GraphRAG [374], Context Assembly Optimization [1132],\\nTool-Integrated Reasoning [310], Agentic Systems [965],Self-Refinement Mechanisms [735], etc.\\nApplication-Driven\\nResearch (§7.3)\\ne.g., Domain Specialization [87], Healthcare Applications [386], Protocol Standardization [246],\\nMCP/A2A/ACP/ANP Protocols [616], Human-AI Collaboration [1368], Security Issues [926],\\nProduction Deployment Scalability [1227], Safety [965] and Ethical Considerations [835], etc.\\nFigure 1: The taxonomy of Context Engineering in Large Language Models is categorized into founda-\\ntional components, system implementations, evaluation methodologies, and future directions. Each area\\nencompasses specific techniques and frameworks that collectively advance the systematic optimization of\\ninformation payloads for LLMs.\\nparadigms, and applications in enhancing the factual grounding of LLMs [483, 428, 817, 889].\\nThe domain of Context Processing addresses the technical challenges of handling long sequences,\\nself-refinement mechanisms, and structured information integration. Long context processing is addressed\\nin surveys analyzing techniques for extending context windows, optimizing attention mechanisms, and\\nmanaging memory efficiently [831, 645, 1289, 268]. The internal cognitive processes of LLMs are increasingly\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='surveyed, with works on self-contextualizing techniques and self-improvement paradigms gaining prominence\\n[1329, 227, 1167, 935].\\nFinally, Context Management literature focuses on memory hierarchies, compression techniques, and\\noptimization strategies that enable effective information organization and retrieval within computational\\nconstraints. While comprehensive surveys specifically dedicated to context management as a unified domain\\nremain limited, related work on memory systems and context compression techniques provides foundational\\ninsights into these critical capabilities.\\nSystem Implementation\\nIn parallel, the literature has extensively covered the System Implementations\\nthat integrate foundational components into sophisticated architectures addressing real-world application\\nrequirements. The domain of RAG has received substantial attention, with foundational surveys tracing its\\ndevelopment and impact on mitigating hallucinations [311, 253, 1131]. More recent work has surveyed the\\nevolution towards modular, agentic, and graph-enhanced RAG architectures [162, 622, 120, 312, 1391].\\nMemory Systems that enable persistent interactions and cognitive architectures have been explored\\nthrough surveys focusing on memory-enhanced agents and their applications. The broader category of\\nLLM-based Agents serves as a foundational area, with comprehensive overviews of autonomous agents,\\ntheir architecture, planning, and methodologies [1091, 719, 277, 843, 1340, 498, 1272].\\nTool-Integrated Reasoning encompassing function calling mechanisms and agent-environment inter-\\naction are well-documented, exploring the evolution from single-tool systems to complex orchestration\\nframeworks [663, 858, 771, 867]. The evolution towards Multi-Agent Systems (MAS) represents another\\nfocal point, with surveys detailing MAS workflows, infrastructure, communication protocols, and coordination\\nmechanisms [625, 356, 246, 1235, 38, 503, 187, 458].\\nEvaluation\\nThe critical aspect of evaluating these complex systems has been thoroughly reviewed, with\\nworks analyzing benchmarks and methodologies for assessing component-level and system-level capabilities\\nand performance [1259, 380, 835, 310]. This evaluation literature spans both foundational component\\nassessment and integrated system evaluation paradigms.\\nOur Contribution\\nWhile these surveys provide indispensable, in-depth analyses of their respective domains,\\nthey inherently present a fragmented view of the field. The connections between RAG as a form of external\\nmemory, tool use as a method for context acquisition, and prompt engineering as the language for orchestrat-\\ning these components are often left implicit. Our work distinguishes itself by proposing Context Engineering\\nas a unifying abstraction that explicitly separates foundational components from their integration in complex\\nimplementations. By organizing these disparate fields into a single, coherent taxonomy, this survey aims to\\nelucidate the fundamental relationships between them, providing a holistic map of how context is generated,\\nprocessed, managed, and utilized to steer the next generation of intelligent systems.\\n3. Why Context Engineering?\\nAs Large Language Models (LLMs) evolve from simple instruction-following systems into the core reasoning\\nengines of complex, multi-faceted applications, the methods used to interact with them must also evolve. The\\nterm “prompt engineering,” while foundational, is no longer sufficient to capture the full scope of designing,\\nmanaging, and optimizing the information payloads required by modern AI systems. These systems do not\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Open Resource\\nClose Resource\\n2020\\n2021\\n2023\\n2024\\n2025\\n2025.07\\nMemory Systems\\nAdvanced RAG\\nTool-Augmented Reasoning\\nSelf-RAG\\nDPR\\nRAFT\\nHippoRAG\\nDeepSeek-R1\\nMulti-Agent Systems\\nNTM\\nMEM0\\nMEM1\\nReAct\\nGorilla\\nToolACE\\nReTool\\nCAMEL\\nAutoGen\\nCrewAI\\nSagaLLM\\nACP \\nAgentOrchestra\\nProAgent\\nRecMind\\nICX-MT\\nGenRead\\nGSM-IC\\nAdaptive-RAG\\nCRAG\\nRAGFusion\\nRECITE\\nRETRO\\nKGI-Slot\\nMulti-Head RAG\\nLightRAG\\nCDF-RAG\\nGraphRAG\\nPlanRAG\\nRag-gym\\nModular RAG\\nGRAG\\nHM-RAG\\nComposeRAG\\nFlahRAG\\nStructMem-LLM\\nSCM\\nMinerva\\nMEMENTO\\nGenerative Agents\\nConstitutional AI\\nTiM\\nSparrow\\nCAMELoT\\nLarimar\\nExplicit-Memory\\u2009Agent\\nEthical\\u2009LTM\\u2009Assistants\\nWebGPT\\nReflexion\\nA-MEM\\nMemLLM\\nRecallM\\nGranite-Function Calling\\nALMs Survey\\nTool Learning Survey\\nToolformer\\nChameleon\\nAdvancing TALMs\\nSecure A2A\\nSwarm\\nIPA Assistant\\n3S Orchestrator\\nKQML\\nAgentSpotter Call-Graph Profiling\\nANP\\nA2A\\nFIPA ACL\\nSmurfs\\nCoA\\nChatCoT\\nAPI-Bank\\nToolLLM\\nToRA\\nRAG\\nMCP\\nChatDev\\nMetaGPT\\nRAPTOR\\nOP-RAG\\nOpenAI-O1\\nStreamingRAG\\nARIST\\nBFCL\\nToolPlanner\\nGTA\\nPlay2Prompt\\nMCP-RADAR\\nMemGPT\\nMemoryBank\\nREMEMBERER\\nHuggingGPT\\nMemorySandbox\\nMAIDial\\nMemOS\\nImplementation\\nACL\\nFigure 2: Context Engineering Evolution Timeline: A comprehensive visualization of the development trajec-\\ntory of Context Engineering implementations from 2020 to 2025, showing the evolution from foundational\\nRAG systems to sophisticated multi-agent architectures and tool-integrated reasoning systems.\\noperate on a single, static string of text; they leverage a dynamic, structured, and multifaceted information\\nstream. To address this, we introduce and formalize the discipline of Context Engineering.\\n3.1. Definition of Context Engineering\\nTo formally define Context Engineering, we begin with the standard probabilistic model of an autoregressive\\nLLM. The model, parameterized by θ, generates an output sequence Y = (y1, . . . , yT) given an input context\\nC by maximizing the conditional probability:\\nPθ(Y|C) =\\nT\\n∏︁\\nt=1\\nPθ(yt|y<t, C)\\n(1)\\nHistorically, in the paradigm of prompt engineering, the context C was treated as a monolithic, static string\\nof text, i.e., C = prompt. This view is insufficient for modern systems.\\nContext Engineering re-conceptualizes the context C as a dynamically structured set of informational\\ncomponents, c1, c2, . . . , cn. These components are sourced, filtered, and formatted by a set of functions, and\\nfinally orchestrated by a high-level assembly function, A:\\nC = A(c1, c2, . . . , cn)\\n(2)\\nThe components ci are not arbitrary; they map directly to the core technical domains of this survey:\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='• cinstr: System instructions and rules (Context Retrieval and Generation, Sec. 4.1).\\n• cknow: External knowledge, retrieved via functions like RAG or from integrated knowledge graphs\\n(RAG, Sec. 5.1; Context Processing, Sec. 4.2).\\n• ctools: Definitions and signatures of available external tools (Function Calling & Tool-Integrated\\nReasoning, Sec. 5.3).\\n• cmem: Persistent information from prior interactions (Memory Systems, Sec. 5.2; Context Manage-\\nment, Sec. 4.3).\\n• cstate: The dynamic state of the user, world, or multi-agent system (Multi-Agent Systems & Orchestra-\\ntion, Sec. 5.4).\\n• cquery: The user’s immediate request.\\nThe Optimization Problem of Context Engineering.\\nFrom this perspective, Context Engineering is the\\nformal optimization problem of finding the ideal set of context-generating functions (which we denote\\ncollectively as F = {A, Retrieve, Select, . . . }) that maximizes the expected quality of the LLM’s output.\\nGiven a distribution of tasks T , the objective is:\\nF ∗= arg max\\nF\\nEτ∼T [Reward(Pθ(Y|CF(τ)), Y∗\\nτ )]\\n(3)\\nwhere τ is a specific task instance, CF(τ) is the context generated by the functions in F for that task, and\\nY∗\\nτ is the ground-truth or ideal output. This optimization is subject to hard constraints, most notably the\\nmodel’s context length limit, |C| ≤Lmax.\\nMathematical Principles and Theoretical Frameworks.\\nThis formalization reveals deeper mathematical\\nprinciples. The assembly function A is a form of Dynamic Context Orchestration, a pipeline of formatting\\nand concatenation operations, A = Concat ◦(Format1, . . . , Formatn), where each function must be optimized\\nfor the LLM’s architectural biases (e.g., attention patterns).\\nThe retrieval of knowledge, cknow = Retrieve(. . . ), can be framed as an Information-Theoretic Optimal-\\nity problem. The goal is to select knowledge that maximizes the mutual information with the target answer\\nY∗, given the query cquery:\\nRetrieve∗= arg max\\nRetrieve I(Y∗; cknow|cquery)\\n(4)\\nThis ensures that the retrieved context is not just semantically similar, but maximally informative for solving\\nthe task.\\nFurthermore, the entire process can be viewed through the lens of Bayesian Context Inference. Instead of\\ndeterministically constructing the context, we infer the optimal context posterior P(C|cquery, History, World).\\nUsing Bayes’ theorem, this posterior is proportional to the likelihood of the query given the context and the\\nprior probability of the context’s relevance:\\nP(C|cquery, . . . ) ∝P(cquery|C) · P(C|History, World)\\n(5)\\nThe decision-theoretic objective is then to find the context C∗that maximizes the expected reward over the\\ndistribution of possible answers:\\nC∗= arg max\\nC\\n∫︁\\nP(Y|C, cquery) · Reward(Y, Y∗) dY · P(C|cquery, . . . )\\n(6)\\nThis Bayesian formulation provides a principled way to handle uncertainty, perform adaptive retrieval by\\nupdating priors, and maintain belief states over context in multi-step reasoning tasks.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Dimension\\nPrompt Engineering\\nContext Engineering\\nModel\\nC = prompt (static string)\\nC = A(c1, c2, . . . , cn) (dynamic, structured assembly)\\nTarget\\narg maxprompt Pθ(Y|prompt)\\nF ∗= arg maxF Eτ∼T [Reward(Pθ(Y|CF(τ)), Y∗\\nτ )]\\nComplexity\\nManual or automated search over a string space.\\nSystem-level optimization of F = {A, Retrieve, Select, . . . }.\\nInformation\\nInformation content is fixed within the prompt.\\nAims to maximize task-relevant information under constraint |C| ≤Lmax.\\nState\\nPrimarily stateless.\\nInherently stateful, with explicit components for cmem and cstate.\\nScalability\\nBrittleness increases with length and complexity.\\nManages complexity through modular composition.\\nError Analysis\\nManual inspection and iterative refinement.\\nSystematic evaluation and debugging of individual context functions.\\nTable 1: Comparison of Prompt Engineering and Context Engineering Paradigms.\\nComparison of Paradigms\\nThe formalization of Context Engineering highlights its fundamental distinctions\\nfrom traditional prompt engineering. The following table summarizes the key differences.\\nIn summary, Context Engineering provides the formal, systematic framework required to build, under-\\nstand, and optimize the sophisticated, context-aware AI systems that are coming to define the future of the\\nfield. It shifts the focus from the “art” of prompt design to the “science” of information logistics and system\\noptimization.\\nContext Scaling\\nContext scaling encompasses two fundamental dimensions that collectively define the\\nscope and sophistication of contextual information processing. The first dimension, length scaling, addresses\\nthe computational and architectural challenges of processing ultra-long sequences, extending context windows\\nfrom thousands to millions of tokens while maintaining coherent understanding across extended narratives,\\ndocuments, and interactions. This involves sophisticated attention mechanisms, memory management\\ntechniques, and architectural innovations that enable models to maintain contextual coherence over vastly\\nextended input sequences.\\nThe second, equally critical dimension is multi-modal and structural scaling, which expands context\\nbeyond simple text to encompass multi-dimensional, dynamic, cross-modal information structures. This\\nincludes temporal context (understanding time-dependent relationships and sequences), spatial context\\n(interpreting location-based and geometric relationships), participant states (tracking multiple entities and\\ntheir evolving conditions), intentional context (understanding goals, motivations, and implicit objectives),\\nand cultural context (interpreting communication within specific social and cultural frameworks).\\nModern context engineering must address both dimensions simultaneously, as real-world applications\\nrequire models to process not only lengthy textual information but also diverse data types including struc-\\ntured knowledge graphs, multimodal inputs (text, images, audio, video), temporal sequences, and implicit\\ncontextual cues that humans naturally understand. This multi-dimensional approach to context scaling\\nrepresents a fundamental shift from parameter scaling toward developing systems capable of understanding\\ncomplex, ambiguous contexts that mirror the nuanced nature of human intelligence in facing a complex\\nworld [1036].\\n10\\n\\n\\n\\n\\n\\n\\n\\n\\n|Dimension|Prompt Engineering|Context Engineering|\\n|---|---|---|\\n|**Model**<br>_C_ = prompt (static string)<br>_C_ =_ A_(_c_1,_ c_2, . . . ,_ cn_) (dynamic, structured assembly)|**Model**<br>_C_ = prompt (static string)<br>_C_ =_ A_(_c_1,_ c_2, . . . ,_ cn_) (dynamic, structured assembly)|**Model**<br>_C_ = prompt (static string)<br>_C_ =_ A_(_c_1,_ c_2, . . . ,_ cn_) (dynamic, structured assembly)|\\n|**Target**|arg maxprompt _Pθ_(_Y|_prompt)|_F ∗_= arg max_F_ **E**_τ∼T_ [Reward(_Pθ_(_Y|CF_(_τ_)),_ Y∗_<br>_τ_ )]|\\n|**Complexity**|Manual or automated search over a string space.|System-level optimization of_ F_ =_ {A_, Retrieve, Select, . . ._ }_.|\\n|**Information**|Information content is fxed within the prompt.|Aims to maximize task-relevant information under constraint_ |C| ≤L_max.|\\n|**State**|Primarily stateless.|Inherently stateful, with explicit components for_ c_mem and_ c_state.|\\n|**Scalability**|Brittleness increases with length and complexity.|Manages complexity through modular composition.|\\n|**Error Analysis**|Manual inspection and iterative refnement.|Systematic evaluation and debugging of individual context functions.|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='3.2. Why Context Engineering\\n3.2.1. Current Limitations\\nLarge Language Models face critical technical barriers necessitating sophisticated context engineering\\napproaches. The self-attention mechanism imposes quadratic computational and memory overhead as\\nsequence length increases, creating substantial obstacles to processing extended contexts and significantly\\nimpacting real-world applications such as chatbots and code comprehension models [1017, 977]. Commercial\\ndeployment compounds these challenges through repeated context processing that introduces additional\\nlatency and token-based pricing costs [1017].\\nBeyond computational constraints, LLMs demonstrate concerning reliability issues including frequent\\nhallucinations, unfaithfulness to input context, problematic sensitivity to input variations, and responses\\nthat appear syntactically correct while lacking semantic depth or coherence [951, 1279, 523].\\nThe prompt engineering process presents methodological challenges through approximation-driven and\\nsubjective approaches that focus narrowly on task-specific optimization while neglecting individual LLM\\nbehavior [800]. Despite these challenges, prompt engineering remains critical for effective LLM utilization\\nthrough precise and contextually rich prompts that reduce ambiguity and enhance response consistency\\n[964].\\n3.2.2. Performance Enhancement\\nContext engineering delivers substantial performance improvements through techniques like retrieval-\\naugmented generation and superposition prompting, achieving documented improvements including 18-fold\\nenhancement in text navigation accuracy, 94% success rates, and significant gains from careful prompt\\nconstruction and automatic optimization across specialized domains [267, 768, 681].\\nStructured prompting techniques, particularly chain-of-thought approaches, enable complex reasoning\\nthrough intermediate steps while enhancing element-aware summarization capabilities that integrate fine-\\ngrained details from source documents [1138, 750, 1120]. Few-shot learning implementations through\\ncarefully selected demonstration examples yield substantial performance gains, including 9.90% improve-\\nments in BLEU-4 scores for code summarization and 175.96% in exact match metrics for bug fixing [306].\\nDomain-specific context engineering proves especially valuable in specialized applications, with execution-\\naware debugging frameworks achieving up to 9.8% performance improvements on code generation bench-\\nmarks and hardware design applications benefiting from specialized testbench generation and security\\nproperty verification [1360, 873, 44]. These targeted approaches bridge the gap between general-purpose\\nmodel training and specialized domain requirements.\\n3.2.3. Resource Optimization\\nContext engineering provides efficient alternatives to resource-intensive traditional approaches by enabling\\nintelligent content filtering and direct knowledge transmission through carefully crafted prompts [630, 670].\\nLLMs can generate expected responses even when relevant information is deleted from input context,\\nleveraging contextual clues and prior knowledge to optimize context length usage while maintaining\\nresponse quality, particularly valuable in domains with significant data acquisition challenges [630, 670].\\nSpecialized optimization techniques further enhance efficiency gains through context awareness and\\nresponsibility tuning that significantly reduce token consumption, dynamic context optimization employing\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='precise token-level content selection, and attention steering mechanisms for long-context inference [426, 944,\\n350]. These approaches maximize information density while reducing processing overhead and maintaining\\nperformance quality [944, 350].\\n3.2.4. Future Potential\\nContext engineering enables flexible adaptation mechanisms through in-context learning that allows models\\nto adapt to new tasks without explicit retraining, with context window size directly influencing available\\nexamples for task adaptation [617]. Advanced techniques integrate compression and selection mechanisms\\nfor efficient model editing while maintaining contextual coherence [619]. This adaptability proves especially\\nvaluable in low-resource scenarios, enabling effective utilization across various prompt engineering techniques\\nincluding zero-shot approaches, few-shot examples, and role context without requiring domain-specific\\nfine-tuning [924, 129, 1075].\\nSophisticated context engineering techniques including in-context learning, chain-of-thought, tree-of-\\nthought, and planning approaches establish foundations for nuanced language understanding and generation\\ncapabilities while optimizing retrieval and generation processes for robust, context-aware AI applications\\n[797, 974].\\nFuture research directions indicate substantial potential for advancing context-sensitive applications\\nthrough chain-of-thought augmentation with logit contrast mechanisms [953], better leveraging different\\ncontext types across domains, particularly in code intelligence tasks combining syntax, semantics, execution\\nflow, and documentation [1094], and understanding optimal context utilization strategies as advanced\\nlanguage models continue demonstrating prompt engineering’s persistent value [1079]. Evolution toward\\nsophisticated filtering and selection mechanisms represents a critical pathway for addressing transformer\\narchitectures’ scaling limitations while maintaining performance quality.\\n4. Foundational Components\\nContext Engineering is built upon three fundamental components that collectively address the core chal-\\nlenges of information management in large language models: Context Retrieval and Generation sources\\nappropriate contextual information through prompt engineering, external knowledge retrieval, and dynamic\\ncontext assembly; Context Processing transforms and optimizes acquired information through long sequence\\nprocessing, self-refinement mechanisms, and structured data integration; and Context Management tackles\\nefficient organization and utilization of contextual information through addressing fundamental constraints,\\nimplementing sophisticated memory hierarchies, and developing compression techniques. These foundational\\ncomponents establish the theoretical and practical basis for all context engineering implementations, forming\\na comprehensive framework where each component addresses distinct aspects of the context engineering\\npipeline while maintaining synergistic relationships that enable comprehensive contextual optimization and\\neffective context engineering strategies.\\n4.1. Context Retrieval and Generation\\nContext Retrieval and Generation forms the foundational layer of context engineering, encompassing the\\nsystematic retrieval and construction of relevant information for LLMs. This component addresses the critical\\nchallenge of sourcing appropriate contextual information through three primary mechanisms: prompt-based\\ngeneration that crafts effective instructions and reasoning frameworks, external knowledge retrieval that\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Foundational Components\\nContext Retrieval and Generation \\nExternal Knowledge \\nRetrieval\\nDynamic Context \\nAssembly\\nPrompt Engineering and \\nContext Generation \\nContext Processing\\nLong Sequence \\nProcessing \\nSelf-Refinement and \\nAdaptation\\nRelational and Structured \\nInformation Integration\\nContext Management \\nFundamental \\nConstraints\\nMemory Hierarchies and \\nStorage Architectures\\nContext \\nCompression\\nContext\\nEngineering\\nLong-Term\\nMemory\\nAvailable\\nTools\\nUser\\nPrompt\\nRetrieved\\nInformation\\nFigure 3: Context Engineering Framework: A comprehensive taxonomy of Context Engineering components\\nincluding Context Retrieval and Generation, Context Processing, and Context Management, integrated\\ninto System Implementations such as RAG systems, memory architectures, tool-integrated reasoning, and\\nmulti-agent coordination mechanisms.\\naccesses dynamic information sources, and dynamic context assembly that orchestrates acquired components\\ninto coherent, task-optimized contexts.\\n4.1.1. Prompt Engineering and Context Generation\\nPrompt engineering and context generation forms the foundational layer of context retrieval, encompassing\\nstrategic input design that combines art and science to craft effective instructions for LLMs. The CLEAR\\nFramework—conciseness, logic, explicitness, adaptability, and reflectiveness—governs effective prompt\\nconstruction, while core architecture integrates task instructions, contextual information, input data, and\\noutput indicators [702, 1133, 569, 209, 25].\\nZero-Shot and Few-Shot Learning Paradigms\\nZero-shot prompting enables task performance without prior\\nexamples, relying exclusively on instruction clarity and pre-trained knowledge [1361, 336, 553, 67, 1046].\\nFew-shot prompting extends this capability by incorporating limited exemplars to guide model responses,\\ndemonstrating task execution through strategic example selection [1361, 401, 103, 546, 788, 1371]. In-\\ncontext learning facilitates adaptation to novel tasks without parameter updates by leveraging demonstration\\nexamples within prompts, with performance significantly influenced by example selection and ordering\\nstrategies [365, 103, 1287, 1016, 920, 846, 1139, 348, 576].\\nChain-of-Thought Foundations\\nChain-of-Thought (CoT) prompting decomposes complex problems into\\nintermediate reasoning steps, mirroring human cognition [1138, 401, 336, 939, 603]. Zero-shot CoT\\nuses trigger phrases like “Let’s think step by step,” improving MultiArith accuracy from 17.7% to 78.7%\\n[553, 1099, 472, 662], with Automatic Prompt Engineer refinements yielding additional gains [1215, 526].\\nTree-of-Thoughts (ToT) organizes reasoning as hierarchical structures with exploration, lookahead, and\\nbacktracking capabilities, increasing Game of 24 success rates from 4% to 74% [1246, 217, 557, 598].\\nGraph-of-Thoughts (GoT) models reasoning as arbitrary graphs with thoughts as vertices and dependencies\\nas edges, improving quality by 62% and reducing costs by 31% compared to ToT [69, 826, 1366].\\n13\\n\\n\\n\\n\\n\\n\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n|**Context Retrieval and Generation**|**Context Retrieval and Generation**|**Context Retrieval and Generation**|\\n|**Context Retrieval and Generation**||External Knowledge<br>Retrieval|\\n\\n\\n|Col1|Dynamic Context<br>Assembly|\\n|---|---|\\n\\n\\n|Col1|Context Processing|Col3|\\n|---|---|---|\\n||||\\n|||Self-Refinement and<br>Adaptation|\\n\\n\\n|Col1|Long Sequence<br>Processing|\\n|---|---|\\n\\n\\n|Col1|Relational and Structured<br>Information Integration|\\n|---|---|\\n\\n\\n|Col1|Col2|Col3|\\n|---|---|---|\\n||**Context Management**|**Context Management**|\\n||||\\n\\n\\n|Col1|Fundamental<br>Constraints|\\n|---|---|\\n\\n\\n|Col1|Memory Hierarchies and<br>Storage Architectures|\\n|---|---|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Cognitive Architecture Integration\\nCognitive prompting implements structured human-like operations\\nincluding goal clarification, decomposition, filtering, abstraction, and pattern recognition, enabling systematic\\nmulti-step task resolution through deterministic, self-adaptive, and hybrid variants [558, 557, 1205, 1164].\\nGuilford’s Structure of Intellect model provides psychological foundations for categorizing cognitive operations\\nsuch as pattern recognition, memory retrieval, and evaluation, enhancing reasoning clarity, coherence, and\\nadaptability [556, 191]. Advanced implementations incorporate cognitive tools as modular reasoning\\noperations, with GPT-4.1 performance on AIME2024 increasing from 26.7% to 43.3% through structured\\ncognitive operation sequences [243, 1030].\\nMethod\\nDescription\\nSelf-Refine [735, 916]\\nEnables LLMs to improve outputs through iterative feedback and refinement cycles using the same model as the generator,\\nfeedback provider, and refiner, without supervised training.\\nMulti-Aspect Feedback [799]\\nIntegrates multiple feedback modules (frozen LMs and external tools), each focusing on specific error categories to enable\\nmore comprehensive, independent evaluation.\\nN-CRITICS [789]\\nImplements an ensemble of critics that evaluate an initial output. Compiled feedback from the generating LLM and other\\nmodels guides refinement until a stopping criterion is met.\\nISR-LLM [1373]\\nImproves LLM-based planning by translating natural language to formal specifications, creating an initial plan, and then\\nsystematically refining it with a validator.\\nSELF [704]\\nTeaches LLMs meta-skills (self-feedback, self-refinement) with limited examples, then has the model continuously self-\\nevolve by generating and filtering its own training data.\\nProMiSe [884]\\nAddresses self-refinement in smaller LMs using principle-guided iterative refinement, combining proxy metric thresholds\\nwith few-shot refinement and rejection sampling.\\nA2R [577]\\nAugments LLMs through Metric-based Iterative Feedback Learning, using explicit evaluation across multiple dimensions\\n(e.g., correctness) to generate feedback and refine outputs.\\nExperience Refinement [857]\\nEnables LLM agents to refine experiences during task execution by learning from recent (successive) or all previous\\n(cumulative) experiences, prioritizing high-quality ones.\\nI-SHEEP [654]\\nAllows LLMs to continuously self-align from scratch by generating, assessing, filtering, and training on high-quality\\nsynthetic datasets without external guidance.\\nCaP [1271]\\nUses external tools to refine chain-of-thought (CoT) responses, addressing the limitation of models that get stuck in\\nnon-correcting reasoning loops.\\nAgent-R [1277]\\nEnables language agents to reflect “on the fly” through iterative self-training, using Monte Carlo Tree Search (MCTS) to\\nconstruct training data that corrects erroneous paths.\\nGenDiE [610]\\nEnhances context faithfulness with sentence-level optimization, combining generative and discriminative training to give\\nLLMs self-generation and self-scoring capabilities.\\nSelf-Developing [466]\\nEnables LLMs to autonomously discover, implement, and refine their own improvement algorithms by generating them as\\ncode, evaluating them, and using DPO to recursively improve.\\nSR-NLE [1121]\\nImproves the faithfulness of post-hoc natural language explanations via an iterative critique and refinement process using\\nself-feedback and feature attribution.\\nTable 2: Self-refinement methods in large language models and their key characteristics.\\n4.1.2. External Knowledge Retrieval\\nExternal knowledge retrieval represents a critical component of context retrieval, addressing fundamental\\nlimitations of parametric knowledge through dynamic access to external information sources including\\ndatabases, knowledge graphs, and document collections.\\nRetrieval-Augmented Generation Fundamentals\\nRAG combines parametric knowledge stored in model\\nparameters with non-parametric information retrieved from external sources, enabling access to current,\\ndomain-specific knowledge while maintaining parameter efficiency [591, 311, 253]. FlashRAG provides\\ncomprehensive evaluation and modular implementation of RAG systems, while frameworks like KRAGEN\\n14\\n\\n\\n\\n\\n\\n\\n\\n\\n|Method|Description|\\n|---|---|\\n|**Self-Refne** [735, 916]|Enables LLMs to improve outputs through iterative feedback and refnement cycles using the same model as the generator,<br>feedback provider, and refner, without supervised training.|\\n|**Multi-Aspect Feedback** [799]|Integrates multiple feedback modules (frozen LMs and external tools), each focusing on specifc error categories to enable<br>more comprehensive, independent evaluation.|\\n|**N-CRITICS** [789]|Implements an ensemble of critics that evaluate an initial output. Compiled feedback from the generating LLM and other<br>models guides refnement until a stopping criterion is met.|\\n|**ISR-LLM** [1373]|Improves LLM-based planning by translating natural language to formal specifcations, creating an initial plan, and then<br>systematically refning it with a validator.|\\n|**SELF** [704]|Teaches LLMs meta-skills (self-feedback, self-refnement) with limited examples, then has the model continuously self-<br>evolve by generating and fltering its own training data.|\\n|**ProMiSe** [884]|Addresses self-refnement in smaller LMs using principle-guided iterative refnement, combining proxy metric thresholds<br>with few-shot refnement and rejection sampling.|\\n|**A2R** [577]|Augments LLMs through Metric-based Iterative Feedback Learning, using explicit evaluation across multiple dimensions<br>(e.g., correctness) to generate feedback and refne outputs.|\\n|**Experience Refnement** [857]|Enables LLM agents to refne experiences during task execution by learning from recent (successive) or all previous<br>(cumulative) experiences, prioritizing high-quality ones.|\\n|**I-SHEEP** [654]|Allows LLMs to continuously self-align from scratch by generating, assessing, fltering, and training on high-quality<br>synthetic datasets without external guidance.|\\n|**CaP** [1271]|Uses external tools to refne chain-of-thought (CoT) responses, addressing the limitation of models that get stuck in<br>non-correcting reasoning loops.|\\n|**Agent-R** [1277]|Enables language agents to refect “on the fy” through iterative self-training, using Monte Carlo Tree Search (MCTS) to<br>construct training data that corrects erroneous paths.|\\n|**GenDiE** [610]|Enhances context faithfulness with sentence-level optimization, combining generative and discriminative training to give<br>LLMs self-generation and self-scoring capabilities.|\\n|**Self-Developing** [466]|Enables LLMs to autonomously discover, implement, and refne their own improvement algorithms by generating them as<br>code, evaluating them, and using DPO to recursively improve.|\\n|**SR-NLE** [1121]|Improves the faithfulness of post-hoc natural language explanations via an iterative critique and refnement process using<br>self-feedback and feature attribution.|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='and ComposeRAG demonstrate advanced retrieval strategies with substantial performance improvements\\nacross diverse benchmarks [500, 749, 1159].\\nSelf-RAG introduces adaptive retrieval mechanisms where models dynamically decide when to retrieve\\ninformation and generate special tokens to control retrieval timing and quality assessment [41]. Advanced\\nimplementations include RAPTOR for hierarchical document processing, HippoRAG for memory-inspired\\nretrieval architectures, and Graph-Enhanced RAG systems that leverage structured knowledge representations\\nfor improved information access [928, 366, 360].\\nKnowledge Graph Integration and Structured Retrieval\\nKnowledge graph integration addresses struc-\\ntured information retrieval through frameworks like KAPING, which retrieves relevant facts based on semantic\\nsimilarities and prepends them to prompts without requiring model training [48, 673]. KARPA provides\\ntraining-free knowledge graph adaptation through pre-planning, semantic matching, and relation path\\nreasoning, achieving state-of-the-art performance on knowledge graph question answering tasks [258].\\nThink-on-Graph enables sequential reasoning over knowledge graphs to locate relevant triples, conducting\\nexploration to retrieve related information from external databases while generating multiple reasoning\\npathways [1000, 720]. StructGPT implements iterative reading-then-reasoning approaches that construct\\nspecialized functions to collect relevant evidence from structured data sources [489].\\nAgentic and Modular Retrieval Systems\\nAgentic RAG systems treat retrieval as dynamic operations\\nwhere agents function as intelligent investigators analyzing content and cross-referencing information\\n[648, 162, 965]. These systems incorporate sophisticated planning and reflection mechanisms requiring\\nintegration of task decomposition, multi-plan selection, and iterative refinement capabilities [438, 1183].\\nModular RAG architectures enable flexible composition of retrieval components through standardized\\ninterfaces and plug-and-play designs. Graph-Enhanced RAG systems leverage structured knowledge represen-\\ntations for improved information access, while Real-time RAG implementations address dynamic information\\nrequirements in streaming applications [312, 1391].\\n4.1.3. Dynamic Context Assembly\\nDynamic context assembly represents the sophisticated orchestration of acquired information components\\ninto coherent, task-optimized contexts that maximize language model performance while respecting compu-\\ntational constraints.\\nAssembly Functions and Orchestration Mechanisms\\nThe assembly function A encompasses template-\\nbased formatting, priority-based selection, and adaptive composition strategies that must adapt to varying task\\nrequirements, model capabilities, and resource constraints [702, 1133, 569]. Contemporary orchestration\\nmechanisms manage agent selection, context distribution, and interaction flow control in multi-agent systems,\\nenabling effective cooperation through user input processing, contextual distribution, and optimal agent\\nselection based on capability assessment [894, 53, 171].\\nAdvanced orchestration frameworks incorporate intent recognition, contextual memory maintenance,\\nand task dispatching components for intelligent coordination across domain-specific agents. The Swarm\\nAgent framework utilizes real-time outputs to direct tool invocations while addressing limitations in static\\ntool registries and bespoke communication frameworks [808, 263, 246].\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Multi-Component Integration Strategies\\nContext assembly must address cross-modal integration chal-\\nlenges, incorporating diverse data types including text, structured knowledge, temporal sequences, and\\nexternal tool interfaces while maintaining coherent semantic relationships [529, 1221, 496]. Verbalization\\ntechniques convert structured data including knowledge graph triples, table rows, and database records\\ninto natural language sentences, enabling seamless integration with existing language systems without\\narchitectural modifications [12, 782, 1064, 13].\\nProgramming language representations of structured data, particularly Python implementations for\\nknowledge graphs and SQL for databases, outperform traditional natural language representations in\\ncomplex reasoning tasks by leveraging inherent structural properties [1166]. Multi-level structurization\\napproaches reorganize input text into layered structures based on linguistic relationships, while structured\\ndata representations leverage existing LLMs to extract structured information and represent key elements as\\ngraphs, tables, or relational schemas [681, 1125, 1324].\\nAutomated Assembly Optimization\\nAutomated prompt engineering addresses manual optimization limi-\\ntations through systematic prompt generation and refinement algorithms. Automatic Prompt Engineer (APE)\\nemploys search algorithms for optimal prompt discovery, while LM-BFF introduces automated pipelines com-\\nbining prompt-based fine-tuning with dynamic demonstration incorporation, achieving up to 30% absolute\\nimprovement across NLP tasks [307, 417, 590]. Promptbreeder implements self-referential evolutionary\\nsystems where LLMs improve both task-prompts and mutation-prompts governing these improvements\\nthrough natural selection analogies [275, 508].\\nSelf-refine enables iterative output improvement through self-critique and revision across multiple\\niterations, with GPT-4 achieving approximately 20% absolute performance improvement through this\\nmethodology [735, 670]. Multi-agent collaborative frameworks simulate specialized team dynamics with\\nagents assuming distinct roles (analysts, coders, testers), resulting in 29.9-47.1% relative improvement in\\nPass@1 metrics compared to single-agent approaches [434, 1257].\\nTool integration frameworks combine Chain-of-Thought reasoning with external tool execution, automat-\\ning intermediate reasoning step generation as executable programs strategically incorporating external data.\\nLangChain provides comprehensive framework support for sequential processing chains, agent development,\\nand web browsing capabilities, while specialized frameworks like Auto-GPT and Microsoft’s AutoGen facilitate\\ncomplex AI agent development through user-friendly interfaces [963, 1087, 25, 867].\\n4.2. Context Processing\\nContext Processing focuses on transforming and optimizing acquired contextual information to maximize its\\nutility for LLMs. This component addresses challenges in handling ultra-long sequence contexts, enables\\niterative self-refinement and adaptation mechanisms, and facilitates integration of multimodal, relational\\nand structured information into coherent contextual representations.\\n4.2.1. Long Context Processing\\nUltra-long sequence context processing addresses fundamental computational challenges arising from\\ntransformer self-attention’s O(n2) complexity, which creates significant bottlenecks as sequence lengths\\nincrease and substantially impacts real-world applications [1059, 731, 295, 268, 416]. Increasing Mistral-7B\\ninput from 4K to 128K tokens requires 122-fold computational increase, while memory constraints during\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='prefilling and decoding stages create substantial resource demands, with Llama 3.1 8B requiring up to 16GB\\nper 128K-token request [1032, 1227, 425].\\nArchitectural Innovations for Long Context\\nState Space Models (SSMs) maintain linear computational\\ncomplexity and constant memory requirements through fixed-size hidden states, with models like Mamba\\noffering efficient recurrent computation mechanisms that scale more effectively than traditional transformers\\n[1258, 347, 346]. Dilated attention approaches like LongNet employ exponentially expanding attentive\\nfields as token distance grows, achieving linear computational complexity while maintaining logarithmic\\ndependency between tokens, enabling processing of sequences exceeding one billion tokens [216].\\nToeplitz Neural Networks (TNNs) model sequences with relative position encoded Toeplitz matrices,\\nreducing space-time complexity to log-linear and enabling extrapolation from 512 training tokens to 14,000\\ninference tokens [868, 869]. Linear attention mechanisms reduce complexity from O(N2) to O(N) by\\nexpressing self-attention as linear dot-products of kernel feature maps, achieving up to 4000× speedup\\nwhen processing very long sequences [522]. Alternative approaches like non-attention LLMs break quadratic\\nbarriers by employing recursive memory transformers and other architectural innovations [547].\\nPosition Interpolation and Context Extension\\nPosition interpolation techniques enable models to process\\nsequences beyond original context window limitations by intelligently rescaling position indices rather than\\nextrapolating to unseen positions [150]. Neural Tangent Kernel (NTK) approaches provide mathematically\\ngrounded frameworks for context extension, with YaRN combining NTK interpolation with linear interpolation\\nand attention distribution correction [833, 471, 1021].\\nLongRoPE achieves 2048K token context windows through two-stage approaches: first fine-tuning\\nmodels to 256K length, then conducting positional interpolation to reach maximum context length [218].\\nPosition Sequence Tuning (PoSE) demonstrates impressive sequence length extensions up to 128K tokens\\nby combining multiple positional interpolation strategies [1377]. Self-Extend techniques enable LLMs to\\nprocess long contexts without fine-tuning by employing bi-level attention strategies—grouped attention and\\nneighbor attention—to capture dependencies among distant and adjacent tokens [499].\\nOptimization Techniques for Efficient Processing\\nGrouped-Query Attention (GQA) partitions query heads\\ninto groups that share key and value heads, striking a balance between multi-query attention and multi-\\nhead attention while reducing memory requirements during decoding [16, 1341]. FlashAttention exploits\\nasymmetric GPU memory hierarchy to achieve linear memory scaling instead of quadratic requirements,\\nwith FlashAttention-2 providing approximately twice the speed through reduced non-matrix multiplication\\noperations and optimized work distribution [196, 195].\\nRing Attention with Blockwise Transformers enables handling extremely long sequences by distributing\\ncomputation across multiple devices, leveraging blockwise computation while overlapping communication\\nwith attention computation [676]. Sparse attention techniques include Shifted sparse attention (S2-Attn) in\\nLongLoRA and SinkLoRA with SF-Attn, which achieve 92% of full attention perplexity improvement with\\nsignificant computation savings [1304, 1217].\\nEfficient Selective Attention (ESA) proposes token-level selection of critical information through query\\nand key vector compression into lower-dimensional representations, enabling processing of sequences up to\\n256K tokens [1084]. BigBird combines local attention with global tokens that attend to entire sequences,\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='plus random connections, enabling efficient processing of sequences up to 8× longer than previously possible\\n[1285].\\nMemory Management and Context Compression\\nMemory management strategies include Rolling Buffer\\nCache techniques that maintain fixed attention spans, reducing cache memory usage by approximately\\n8× on 32K token sequences [1341]. StreamingLLM enables processing infinitely long sequences without\\nfine-tuning by retaining critical “attention sink” tokens together with recent KV cache entries, demonstrating\\nup to 22.2× speedup over sliding window recomputation with sequences up to 4 million tokens [1176].\\nInfini-attention incorporates compressive memory into vanilla attention, combining masked local attention\\nwith long-term linear attention in single Transformer blocks, enabling processing of infinitely long inputs\\nwith bounded memory and computation [792]. Heavy Hitter Oracle (H2O) presents efficient KV cache\\neviction policies based on observations that small token portions contribute most attention value, improving\\nthroughput by up to 29× while reducing latency by up to 1.9× [1333].\\nContext compression techniques like QwenLong-CPRS implement dynamic context optimization mecha-\\nnisms enabling multi-granularity compression guided by natural language instructions [944]. InfLLM stores\\ndistant contexts in additional memory units and employs efficient mechanisms to retrieve token-relevant\\nunits for attention computation, allowing models pre-trained on sequences of a few thousand tokens to\\neffectively process sequences up to 1,024K tokens [1175].\\n4.2.2. Contextual Self-Refinement and Adaptation\\nSelf-refinement enables LLMs to improve outputs through cyclical feedback mechanisms mirroring human\\nrevision processes, leveraging self-evaluation through conversational self-interaction via prompt engineering\\ndistinct from reinforcement learning approaches [735, 916, 25, 1211].\\nFoundational Self-Refinement Frameworks\\nThe Self-Refine framework uses the same model as generator,\\nfeedback provider, and refiner, demonstrating that identifying and fixing errors is often easier than producing\\nperfect initial solutions [735, 1313, 227]. Reflexion maintains reflective text in episodic memory buffers\\nfor future decision-making through linguistic feedback [956], while structured guidance proves essential as\\nsimplistic prompting often fails to enable reliable self-correction [672, 587].\\nMulti-Aspect Feedback integrates frozen language models and external tools focusing on specific error\\ncategories to enable more comprehensive, independent evaluation [799]. The N-CRITICS framework\\nimplements ensemble-based evaluation where initial outputs are assessed by both generating LLMs and other\\nmodels, with compiled feedback guiding refinement until task-specific stopping criteria are fulfilled [789].\\nThe A2R framework adopts explicit evaluation across multiple dimensions including correctness and\\ncitation quality, formulating natural language feedback for each aspect and iteratively refining outputs [577].\\nISR-LLM improves LLM-based planning by translating natural language to formal specifications, creating an\\ninitial plan, and then systematically refining it with a validator [1373].\\nMeta-Learning and Autonomous Evolution\\nSELF teaches LLMs meta-skills (self-feedback, self-refinement)\\nwith limited examples, then has the model continuously self-evolve by generating and filtering its own\\ntraining data [704]. Self-rewarding mechanisms enable models to improve autonomously through iterative\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='self-judgment, where a single model adopts dual roles as performer and judge, maximizing rewards it assigns\\nitself [1163, 1278].\\nThe Creator framework extends this paradigm by enabling LLMs to create and use their own tools through\\na four-module process encompassing creation, decision-making, execution, and recognition [946, 856]. The\\nSelf-Developing framework represents the most autonomous approach, enabling LLMs to discover, implement,\\nand refine their own improvement algorithms through iterative cycles generating algorithmic candidates as\\nexecutable code [466].\\nIn-context learning fundamentally represents a form of meta-learning where models learn optimiza-\\ntion strategies during pre-training that generalize across diverse tasks, enabling rapid adaptation to novel\\nchallenges during inference [179, 1165]. Meta-in-context learning demonstrates that in-context learning\\nabilities can be recursively improved through in-context learning itself, adaptively reshaping model priors\\nover expected tasks and modifying in-context learning strategies [177].\\nMemory-Augmented Adaptation Frameworks\\nMemory augmentation represents a powerful approach\\nfor implementing meta-learning through frameworks like Memory of Amortized Contexts, which uses\\nfeature extraction and memory-augmentation to compress information from new documents into compact\\nmodulations stored in memory banks [1011]. Context-aware Meta-learned Loss Scaling addresses outdated\\nknowledge challenges by meta-training small autoregressive models to dynamically reweight language\\nmodeling loss for each token during online fine-tuning [430].\\nDecision-Pretrained Transformers demonstrate how transformers can be trained to perform in-context\\nreinforcement learning, solving previously unseen RL problems by generalizing beyond pretraining distribu-\\ntion [1013, 582]. Context-based meta-reinforcement learning methods enhance performance through direct\\nsupervision of context encoders, improving sample efficiency compared to end-to-end training approaches\\n[1072].\\nLong Chain-of-Thought and Advanced Reasoning\\nLong Chain-of-Thought has emerged as a significant\\nevolution characterized by substantially longer reasoning traces enabling thorough problem exploration, as\\nimplemented in advanced models including OpenAI-o1, DeepSeek-R1, QwQ, and Gemini 2.0 Flash Thinking\\n[147, 718, 1214]. LongCoT effectiveness appears linked to context window capacity, with empirical evidence\\nsuggesting larger context windows often lead to stronger reasoning performance [1229].\\nExtended reasoning enables self-reflection and error correction mechanisms allowing models to identify\\nand rectify mistakes during problem-solving processes [1334]. The effectiveness of increasing reasoning\\nstep length, even without adding new information, considerably enhances reasoning abilities across multiple\\ndatasets through test-time scaling [1345].\\nOptimization strategies address computational inefficiencies due to verbose reasoning traces through\\nself-generated shorter reasoning paths via best-of-N sampling, adaptive reasoning modes including Zero-\\nThinking and Less-Thinking approaches, and explicit compact CoT methods reducing token usage while\\nmaintaining reasoning quality [791, 1348, 697]. Auto Long-Short Reasoning enables dynamic adjustment\\nof reasoning path length according to question complexity, helping models decide when longer chains are\\nnecessary [715].\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='4.2.3. Multimodal Context\\nMultimodal Large Language Models (MLLMs) extend context engineering beyond text by integrating diverse\\ndata modalities including vision, audio, and 3D environments into unified contextual representations. This\\nexpansion introduces new challenges in modality fusion, cross-modal reasoning, and long-context processing\\nwhile enabling sophisticated applications that leverage rich multimodal contextual understanding.\\nMultimodal Context Integration\\nFoundational Techniques\\nMultimodal MLLMs expand upon traditional LLMs by integrating data from\\ndiverse modalities like vision, audio, and 3D environments [105, 49, 957]. A primary integration method\\nconverts visual inputs into discrete tokens concatenated with text tokens, conditioning the LLM’s generative\\nprocess on a combined representation [1286]. This is often facilitated by Visual Prompt Generators (VPGs)\\ntrained on image-caption pairs to map visual features into the LLM’s embedding space [607]. The dominant\\narchitectural paradigm connects specialized, external multimodal encoders—such as CLIP for vision or CLAP\\nfor audio—to the LLM backbone via alignment modules like Q-Former or simple MLPs [19, 86, 609, 1130],\\na modular design that allows for independent encoder updates without retraining the entire model [618].\\nAdvanced Integration Strategies\\nMore sophisticated approaches enable deeper modality fusion. Cross-\\nmodal attention mechanisms learn fine-grained dependencies between textual and visual tokens directly\\nwithin the LLM’s embedding space, enhancing semantic understanding for tasks like image editing [564,\\n901, 102]. To manage lengthy inputs, hierarchical designs process modalities in stages to ensure scalability\\n[155], while the “browse-and-concentrate” paradigm fuses the contexts of multiple images before LLM\\ningestion to overcome the limitations of isolated processing [1134]. Some research bypasses the adaptation\\nof text-only LLMs, opting for unified training paradigms that jointly pre-train models on multimodal data and\\ntext corpora from the start to mitigate alignment challenges [1381, 1224]. Other methods leverage text as a\\nuniversal semantic space, using LLM in-context learning to improve generalization across diverse modality\\ncombinations [1050]. For video, context integration techniques range from prompt tuning to adapter-based\\nmethods that transform video content into a sequence for reasoning [1080]. The development of these\\nmodels is often constrained by the need for vast, high-quality multimodal data and significant computational\\nresources [1295, 609, 211].\\nCore Challenges in Multimodal Context Processing\\nModality Bias and Reasoning Deficiencies\\nA primary obstacle in MLLM development is modality bias,\\nwhere models favor textual inputs, generating plausible but multimodally ungrounded responses by relying\\non learned linguistic patterns rather than integrated visual or auditory information [1358, 24, 315, 1325].\\nThis issue is exacerbated by training methodologies; for instance, VPGs trained on simple image-captioning\\ntasks learn to extract only salient features for captions, neglecting other visual details crucial for more\\ncomplex, instruction-based tasks, which fundamentally limits deep multimodal understanding [607, 504].\\nConsequently, MLLMs frequently struggle with fine-grained spatial or temporal reasoning, such as precise\\nobject localization or understanding detailed event sequences in videos [1031, 957], particularly in complex\\ndomains like social media where interpreting the interplay of text and images to understand misinformation\\nor sarcasm is difficult [505]. Effective multimodal reasoning requires not just comprehending each modality\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='but also inferring their combined holistic meaning [385]. Compounding these issues is our limited mecha-\\nnistic understanding of MLLMs themselves; their internal workings are largely a black box, hindering the\\ndevelopment of better architectures [1274].\\nAdvanced Contextual Capabilities and Future Directions\\nIn-Context and Long-Context Learning\\nA key capability of MLLMs is in-context learning, where models\\nadapt to new tasks from multimodal examples in the prompt without weight updates [1397, 1398, 551].\\nLink-context learning (LCL) enhances this by providing demonstrations with explicit causal links, improving\\ngeneralization [1012]. However, in-context learning is constrained by fixed context windows, as image\\ntokens consume significant space, limiting many-shot learning [437]. Performance is also sensitive to input\\norder and the relative importance of each modality varies by task [1020, 1197]. Processing long multimodal\\ncontexts, crucial for applications like video analysis, remains a major research frontier [1086]. Innovations\\ninclude adaptive hierarchical token compression for video [1119], variable visual position encoding (V2PE)\\n[1381], specialized modules like ContextQFormer for conversational memory [589], and dynamic, query-\\naware frame selection for video [581]. MLLMs also show emergent communication efficiency over extended\\ninteractions, a phenomenon still under investigation [436].\\nEmerging Applications\\nThe ability to process rich multimodal context is unlocking new applications.\\nMLLMs are used for predictive reasoning, such as forecasting human activity from visual scenes [1382], and\\nhave demonstrated impressive perception and cognitive capabilities across various multimodal benchmarks\\n[290]. In VQA, context is leveraged for more precise answers, for instance, by prompting the MLLM to\\ngenerate its own descriptive text context of an image [1346] or by integrating external knowledge via RAG\\n[993, 105]. Other applications include planning digital actions based on sensory inputs [605], enhancing\\nsurgical decision support through memory-augmented context comprehension [418], and enabling nuanced\\nvideo understanding by integrating visual information with speech and audio cues [642, 1193, 7]. Researchers\\nhave also extended MLLMs to emerging modalities like tactile information, event data, and graph structures\\n[1358, 1023, 1213]. The growing importance of these real-world use cases has spurred the development of\\ncomprehensive evaluation frameworks to assess contextual comprehension [1109]. These advancements\\nenable applications previously impossible with text-only models, such as image captioning and sophisticated\\nmultimodal reasoning [1173, 677, 139].\\n4.2.4. Relational and Structured Context\\nLarge language models face fundamental constraints processing relational and structured data including\\ntables, databases, and knowledge graphs due to text-based input requirements and sequential architecture\\nlimitations [489, 47, 1136]. Linearization often fails to preserve complex relationships and structural\\nproperties, with performance degrading when information is dispersed throughout contexts [586, 585, 938].\\nKnowledge Graph Embeddings and Neural Integration\\nAdvanced encoding strategies address structural\\nlimitations through knowledge graph embeddings that transform entities and relationships into numerical\\nvectors, enabling efficient processing within language model architectures [12, 1250, 930, 1194]. Graph\\nneural networks capture complex relationships between entities, facilitating multi-hop reasoning across\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='knowledge graph structures through specialized architectures like GraphFormers that nest GNN components\\nalongside transformer blocks [974, 404, 1221, 483].\\nGraphToken demonstrates substantial improvements by explicitly representing structural information,\\nachieving up to 73 percentage points enhancement on graph reasoning tasks through parameter-efficient\\nencoding functions [836]. Heterformer and other hybrid GNN-LM architectures perform contextualized text\\nencoding and heterogeneous structure encoding in unified models, addressing the computational challenges\\nof scaling these integrated systems [496, 465, 751].\\nMethod\\nApproach\\nPerformance\\nKey Innovation\\nODA [1001]\\nObservation-driven agent framework\\n12.87% and 8.9% improvements\\nRecursive observation with action-reflection\\nRAG-KG [1206]\\nHistorical issue KG construction\\n77.6% MRR, 0.32 BLEU improvement\\nQuery parsing and sub-graph retrieval\\nKARPA [258]\\nTraining-free KG adaptation\\nState-of-the-art KGQA performance\\nPre-planning relation paths\\nFaithful Reasoning [720]\\nPlanning-retrieval-reasoning framework\\nN/A\\nLLM-KG synergy with relation paths\\nTable 3: Knowledge graph integration methods for enhanced reasoning in large language models.\\nVerbalization and Structured Data Representations\\nVerbalization techniques convert structured data\\nincluding knowledge graph triples, table rows, and database records into natural language sentences, enabling\\nseamless integration with existing language systems without architectural modifications [12, 782, 1064, 13].\\nMulti-level structurization approaches reorganize input text into layered structures based on linguistic\\nrelationships, while structured data representations leverage existing LLMs to extract structured information\\nand represent key elements as graphs, tables, or relational schemas [681, 1125, 1324, 1035, 602].\\nProgramming language representations of structured data, particularly Python implementations for\\nknowledge graphs and SQL for databases, outperform traditional natural language representations in\\ncomplex reasoning tasks by leveraging inherent structural properties [1166]. Resource-efficient approaches\\nusing structured matrix representations offer promising directions for reducing parameter counts while\\nmaintaining performance on structured data tasks [343].\\nIntegration Frameworks and Synergized Approaches\\nThe integration of knowledge graphs with language\\nmodels follows distinct paradigms characterized by different implementation strategies and performance\\ntrade-offs [817, 1140]. Pre-training integration methods like K-BERT inject knowledge graph triples during\\ntraining to internalize factual knowledge, while inference-time approaches enable real-time knowledge\\naccess without requiring complete model retraining [690, 1237, 712].\\nKG-enhanced LLMs incorporate structured knowledge to improve factual grounding through retrieval-\\nbased augmentation methods like KAPING, which retrieves relevant facts based on semantic similarities\\nand prepends them to prompts without requiring model training [48, 673, 591]. More sophisticated\\nimplementations embed KG-derived representations directly into model latent spaces through adapter\\nmodules and cross-attention mechanisms, with Text2Graph mappers providing linking between input text\\nand KG embedding spaces [132, 1066, 428].\\nSynergized approaches create unified systems where both technologies play equally important roles,\\naddressing fundamental limitations through bidirectional reasoning driven by data and knowledge [817,\\n853, 1111]. GreaseLM facilitates deep interaction across all model layers, allowing language context\\nrepresentations to be grounded by structured world knowledge while linguistic nuances inform graph\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n|Method|Approach|Performance|Key Innovation|\\n|---|---|---|---|\\n|**ODA** [1001]<br>Observation-driven agent framework<br>12.87% and 8.9% improvements<br>Recursive observation with action-refection|**ODA** [1001]<br>Observation-driven agent framework<br>12.87% and 8.9% improvements<br>Recursive observation with action-refection|**ODA** [1001]<br>Observation-driven agent framework<br>12.87% and 8.9% improvements<br>Recursive observation with action-refection|**ODA** [1001]<br>Observation-driven agent framework<br>12.87% and 8.9% improvements<br>Recursive observation with action-refection|\\n|**RAG-KG** [1206]|Historical issue KG construction|77.6% MRR, 0.32 BLEU improvement|Query parsing and sub-graph retrieval|\\n|**KARPA** [258]|Training-free KG adaptation|State-of-the-art KGQA performance|Pre-planning relation paths|\\n|**Faithful Reasoning** [720]|Planning-retrieval-reasoning framework|N/A|LLM-KG synergy with relation paths|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='representations [1321]. QA-GNN implements bidirectional attention mechanisms connecting question-\\nanswering contexts and knowledge graphs through joint graph formation and mutual representation updates\\nvia graph-based message passing [1250, 974].\\nApplications and Performance Enhancement\\nStructured data integration significantly enhances LLM\\ncapabilities across multiple dimensions, with knowledge graphs providing structured information that reduces\\nhallucinations by grounding responses in verifiable facts and improving factual accuracy through clearly\\ndefined information sources [1002, 1342, 200, 565]. Knowledge graphs enhance reasoning capabilities by\\nproviding structured entity relationships that enable complex multi-hop reasoning and logical inferences, with\\ntheir rich repository of hierarchical knowledge significantly improving precision and reliability of inferences\\n[1166, 208, 1018].\\nReal-world applications demonstrate substantial improvements across specialized domains. Healthcare\\nsystems combine structured medical knowledge with contextual understanding through Retrieval-Augmented\\nGeneration frameworks to improve disease progression modeling and clinical decision-making [842, 583].\\nScientific research platforms organize findings into structured knowledge supporting hypothesis generation\\nand research gap identification, while business analytics systems balance rule-based precision with AI pattern\\nrecognition for more actionable insights [1326, 1062].\\nQuestion answering systems benefit from natural language interfaces over structured data sources, with\\nintegration creating more robust systems capable of handling multimodal queries and providing personalized\\nresponses that overcome static knowledge base limitations [1317, 1116, 914, 1206]. Research demonstrates\\nthat structured knowledge representations can improve summarization performance by 40% and 14%\\nacross public datasets compared to unstructured memory approaches, with Chain-of-Key strategies providing\\nadditional performance gains through dynamic structured memory updates [459].\\nMethod\\nData Type\\nIntegration Method\\nKey Innovation\\nTask Scope\\nK-LAMP [48]\\nKnowledge graphs\\nRetrieval-based augmentation\\nKAPING framework\\nZero-shot QA\\nPan et al. [817]\\nKnowledge graphs\\nPre-training & inference integration\\nSynergized LLMs + KGs\\nMulti-domain reasoning\\nStructLM [1392]\\nTables, graphs, databases\\nInstruction tuning\\n1.1M example dataset\\n18 datasets, 8 SKG tasks\\nShao et al. [938]\\nTables, databases, KGs\\nLinearization methods\\nSchema linking & syntax prediction\\nText-to-SQL tasks\\nTable 4: Representative approaches for structured data integration in large language models.\\n4.3. Context Management\\nContext Management addresses the efficient organization, storage, and utilization of contextual information\\nwithin LLMs. This component tackles fundamental constraints imposed by finite context windows, develops\\nsophisticated memory hierarchies and storage architectures, and implements compression techniques to\\nmaximize information density while maintaining accessibility and coherence.\\n4.3.1. Fundamental Constraints\\nLLMs face fundamental constraints in context management stemming from finite context window sizes inher-\\nent in most architectures, which significantly reduce model efficacy on tasks requiring deep understanding\\nof lengthy documents while imposing substantial computational demands that hinder applications requiring\\nquick responses and high throughput [1074]. Although extending context windows enables models to handle\\n23\\n\\n\\n\\n\\n\\n\\n\\n\\n|Method|Data Type|Integration Method|Key Innovation|Task Scope|\\n|---|---|---|---|---|\\n|**K-LAMP** [48]<br>Knowledge graphs<br>Retrieval-based augmentation<br>KAPING framework<br>Zero-shot QA|**K-LAMP** [48]<br>Knowledge graphs<br>Retrieval-based augmentation<br>KAPING framework<br>Zero-shot QA|**K-LAMP** [48]<br>Knowledge graphs<br>Retrieval-based augmentation<br>KAPING framework<br>Zero-shot QA|**K-LAMP** [48]<br>Knowledge graphs<br>Retrieval-based augmentation<br>KAPING framework<br>Zero-shot QA|**K-LAMP** [48]<br>Knowledge graphs<br>Retrieval-based augmentation<br>KAPING framework<br>Zero-shot QA|\\n|**Pan et al.** [817]|Knowledge graphs|Pre-training & inference integration|Synergized LLMs + KGs|Multi-domain reasoning|\\n|**StructLM** [1392]|Tables, graphs, databases|Instruction tuning|1.1M example dataset|18 datasets, 8 SKG tasks|\\n|**Shao et al.** [938]|Tables, databases, KGs|Linearization methods|Schema linking & syntax prediction|Text-to-SQL tasks|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='entire documents and capture longer-range dependencies, traditional transformer architectures experience\\nquadratic computational complexity growth as sequence length increases, making processing extremely long\\ntexts prohibitively expensive [999]. While innovative approaches like LongNet have reduced this complexity\\nto linear, balancing window size and generalization capabilities remains challenging [999, 216].\\nEmpirical evidence reveals the “lost-in-the-middle” phenomenon, where LLMs struggle to access informa-\\ntion positioned in middle sections of long contexts, performing significantly better when relevant information\\nappears at the beginning or end of inputs [128, 685, 648]. This positional bias severely impacts performance\\nin extended chain-of-thought reasoning tasks where critical earlier results become susceptible to forgetting,\\nwith performance degrading drastically by as much as 73% compared to performance with no prior context\\n[128, 1138, 377].\\nLLMs inherently process each interaction independently, lacking native mechanisms to maintain state\\nacross sequential exchanges and robust self-validation mechanisms, constraints stemming from fundamental\\nlimits identified in Gödel’s incompleteness theorems [128, 368]. This fundamental statelessness necessitates\\nexplicit management systems to maintain coherent operation sequences and ensure robust failure recovery\\nmechanisms [128]. Context management faces opposing challenges of context window overflow, where\\nmodels “forget” prior context due to exceeding window limits, and context collapse, where enlarged context\\nwindows or conversational memory cause models to fail in distinguishing between different conversational\\ncontexts [985]. Research demonstrates that claimed benefits of chain-of-thought prompting don’t stem from\\ngenuine algorithmic learning but rather depend on problem-specific prompts, with benefits deteriorating\\nas problem complexity increases [984]. The computational overhead of long-context processing creates\\nadditional challenges in managing key-value caches which grow substantially with input length, creating\\nbottlenecks in both latency and accuracy, while multi-turn and longitudinal interaction challenges further\\ncomplicate context management as limited effective context hinders longitudinal knowledge accumulation\\nand token demands of many-shot prompts constrain space available for system and user inputs while slowing\\ninference [911, 719, 389].\\n4.3.2. Memory Hierarchies and Storage Architectures\\nModern LLM memory architectures employ sophisticated hierarchical designs organized into methodological\\napproaches to overcome fixed context window limitations. OS-inspired hierarchical memory systems imple-\\nment virtual memory management concepts, with MemGPT exemplifying this approach through systems\\nthat page information between limited context windows (main memory) and external storage, similar to tra-\\nditional operating systems [813]. These architectures consist of main context containing system instructions,\\nFIFO message queues, and writable scratchpads, alongside external context holding information accessible\\nthrough explicit function calls, with memory management through function-calling capabilities enabling\\nautonomous paging decisions [831]. PagedAttention, inspired by virtual memory and paging techniques in\\noperating systems, manages key-value cache memory in LLMs [57].\\nDynamic memory organizations implement innovative systems based on cognitive principles, with\\nMemoryBank using Ebbinghaus Forgetting Curve theory to dynamically adjust memory strength according\\nto time and significance [1202, 1362]. ReadAgent employs episode pagination to segment content, memory\\ngisting to create concise representations, and interactive look-up for information retrieval [1202]. Compressor-\\nretriever architectures support life-long context management by using base model forward functions to\\ncompress and retrieve context, ensuring end-to-end differentiability [1236].\\nArchitectural adaptations enhance model memory capabilities through internal modifications including\\naugmented attention mechanisms, refined key-value cache mechanisms, and modified positional encodings\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='[160, 1352]. Knowledge-organization methods structure memory into interconnected semantic networks en-\\nabling adaptive management and flexible retrieval, while retrieval mechanism-oriented approaches integrate\\nsemantic retrieval with memory forgetting mechanisms [515, 1362, 444].\\nSystem configurations balance efficiency and scalability through organizational approaches where cen-\\ntralized systems coordinate tasks efficiently but struggle with scalability as topics increase, leading to\\ncontext overflow, while decentralized systems reduce context overflow but increase response time due to\\ninter-agent querying [396]. Hybrid approaches balance shared knowledge with specialized processing for\\nsemi-autonomous operation, addressing challenges in balancing computational efficiency with contextual\\nfidelity while mitigating memory saturation where excessive storage of past interactions leads to retrieval\\ninefficiencies [160, 396]. Context Manager Components provide fundamental capabilities for snapshot\\ncreation, restoration of intermediate generation states, and overall context window management for LLMs\\n[757].\\n4.3.3. Context Compression\\nContext compression techniques enable LLMs to handle longer contexts efficiently by reducing computational\\nand memory burden while preserving critical information. Autoencoder-based compression achieves signif-\\nicant context reduction through In-context Autoencoder (ICAE), which achieves 4× context compression\\nby condensing long contexts into compact memory slots that LLMs can directly condition on, significantly\\nenhancing models’ ability to handle extended contexts with improved latency and memory usage during\\ninference [317]. Recurrent Context Compression (RCC) efficiently expands context window length within\\nconstrained storage space, addressing challenges of poor model responses when both instructions and context\\nare compressed by implementing instruction reconstruction techniques [441].\\nMemory-augmented approaches enhance context management through kNN-based memory caches that\\nstore key-value pairs of past inputs for later lookup, improving language modeling capabilities through\\nretrieval-based mechanisms [393]. Contrastive learning approaches enhance memory retrieval accuracy, while\\nside networks address memory staleness without requiring LLM fine-tuning, and consolidated representation\\nmethods dynamically update past token representations, enabling arbitrarily large context windows without\\nbeing limited by fixed memory slots [393].\\nHierarchical caching systems implement sophisticated multi-layer approaches, with Activation Refilling\\n(ACRE) employing Bi-layer KV Cache where layer-1 cache captures global information compactly and layer-2\\ncache provides detailed local information, dynamically refilling L1 cache with query-relevant entries from\\nL2 cache to integrate broad understanding with specific details [859]. Infinite-LLM addresses dynamic\\ncontext length management through DistAttention for distributing attention computation across GPU clusters,\\nliability mechanisms for borrowing memory across instances, and global planning coordination [935].\\nKCache optimizes inference by storing K Cache in high-bandwidth memory while keeping V Cache in CPU\\nmemory, selectively copying key information based on attention calculations [935].\\nMulti-agent distributive processing represents an emerging approach using LLM-based multi-agent meth-\\nods to handle massive inputs in distributed manner, addressing core bottlenecks in knowledge synchronization\\nand reasoning processes when dealing with extensive external knowledge [699]. Analysis of real-world\\nkey-value cache access patterns reveals high cache reusability in workloads like RAG and agents, highlighting\\nthe need for efficient distributed caching systems with optimized metadata management to reduce redun-\\ndancy and improve speed [1389]. These compression techniques can be combined with other long-context\\nmodeling approaches to further enhance LLMs’ capacity to process and utilize extended contexts efficiently\\nwhile reducing computational overhead and preserving information integrity [317].\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Method\\nStrategy\\nEfficiency\\nAccuracy\\nLength Mgmt\\nScalability\\nO1-Pruner [718]\\nRL fine-tuning\\nN/A\\n+Acc, -Overhead\\nAuto pruning\\n+Efficiency\\nInftyThink [1214]\\nIterative + summarization\\nComplexity reduction\\n+3-13%\\nIterative control\\nScalable\\nLong-CoT Survey [147]\\nLong CoT + reasoning\\n+Efficiency frameworks\\n+Complex domains\\nDeep exploration\\nTest-time scaling\\nPREMISE [1273]\\nPrompt opt + diagnostics\\nGradient-inspired opt\\nMaintained/+Acc\\n-87.5% tokens\\nPerformance maintained\\nPrune-on-Logic [721]\\nStructure-aware pruning\\nSelective pruning\\n+Accuracy\\nSelective framework\\nLogic-based opt\\nTable 5: Long-chain reasoning methods and their characteristics in large language models. O1-Pruner\\nuses reinforcement learning-style fine-tuning to shorten reasoning chains while maintaining accuracy.\\nInftyThink employs iterative reasoning with intermediate summarization to reduce computational complexity.\\nLong-CoT Survey explores long chain-of-thought characteristics that enhance reasoning abilities through\\nefficiency improvements and enhanced knowledge frameworks. PREMISE optimizes prompts with trace-level\\ndiagnostics using gradient-inspired optimization, achieving 87.5% token reduction. Prune-on-Logic performs\\nstructure-aware pruning of logic graphs through selective removal of low-utility reasoning steps.\\n4.3.4. Applications\\nEffective context management extends LLMs’ capabilities beyond simple question-answering to enable\\nsophisticated applications leveraging comprehensive contextual understanding across multiple domains.\\nDocument processing and analysis capabilities enable LLMs to handle entire documents or comprehend\\nfull articles rather than fragments, allowing for contextually relevant responses through comprehensive\\nunderstanding of input material, particularly valuable for inherently long sequential data such as gene\\nsequences, legal documents, and technical literature where maintaining coherence across extensive content\\nis critical [999].\\nExtended reasoning capabilities facilitated by context management techniques support complex reasoning\\nrequiring maintenance and building upon intermediate results across extended sequences. By capturing\\nlonger-range dependencies, these systems support multi-step problem solving where later reasoning de-\\npends on earlier calculations or deductions, enabling sophisticated applications in fields requiring extensive\\ncontextual awareness like complex decision support systems and scientific research assistance [999, 160].\\nCollaborative and multi-agent systems benefit from effective context management in multi-turn dialogues\\nor sequential tasks where maintaining consistent state and synchronizing internal information between\\ncollaborating models is essential [154]. These capabilities support applications including distributed task\\nprocessing, collaborative content creation, and multi-agent problem-solving where contextual coherence\\nacross multiple interactions must be maintained [154].\\nEnhanced conversational interfaces leverage robust context management to seamlessly handle extensive\\nconversations without losing thread coherence, enabling more natural, persistent dialogues that closely\\nresemble human conversations [883]. Task-oriented LLM systems benefit from structured context manage-\\nment approaches, with sliding window storage implementing minimal context management systems that\\npermanently append prompts and responses to context stores, and Retrieval-Augmented Generation systems\\nsupplementing LLMs with access to external sources of dynamic information [212, 926]. These capabili-\\nties support applications like personalized virtual assistants, long-term tutoring systems, and therapeutic\\nconversational agents that maintain continuity across extended interactions [883].\\nMemory-augmented applications implement strategies enabling LLMs to persistently store, manage,\\n26\\n\\n\\n\\n\\n\\n\\n\\n\\n|Method|Strategy|Efcfi iency|Accuracy|Length Mgmt|Scalability|\\n|---|---|---|---|---|---|\\n|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|**O1-Pruner** [718]<br>RL fne-tuning<br>N/A<br>+Acc, -Overhead<br>Auto pruning<br>+Efciency|\\n|**InftyThink** [1214]|Iterative + summarization|Complexity reduction|+3-13%|Iterative control|Scalable|\\n|**Long-CoT Survey** [147]|Long CoT + reasoning|+Efciency frameworks|+Complex domains|Deep exploration|Test-time scaling|\\n|**PREMISE** [1273]|Prompt opt + diagnostics|Gradient-inspired opt|Maintained/+Acc|-87.5% tokens|Performance maintained|\\n|**Prune-on-Logic** [721]|Structure-aware pruning|Selective pruning|+Accuracy|Selective framework|Logic-based opt|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 26}, page_content='and dynamically retrieve relevant contextual information, supporting applications requiring knowledge\\naccumulation over time through building personalized user models via continuous interaction, implementing\\neffective knowledge management across extended interactions, and supporting long-term planning scenarios\\ndepending on historical context [160]. Advanced memory frameworks like Contextually-Aware Intelligent\\nMemory (CAIM) enhance long-term interactions by incorporating cognitive AI principles through modules that\\nenable storage and retrieval of user-specific information while supporting contextual and time-based relevance\\nfiltering [1143]. Memory management for LLM agents incorporates processes analogous to human memory\\nreconsolidation, including deduplication, merging, and conflict resolution, with approaches like Reflective\\nMemory Management combining prospective and retrospective reflection for dynamic summarization and\\nretrieval optimization [1167, 382]. Case-based reasoning systems provide theoretical foundations for\\nLLM agent memory through architectural components that enable cognitive integration and persistent\\ncontext storage techniques that implement caching strategies for faster provisioning of necessary context\\n[383, 381]. The benefits extend beyond processing longer texts to fundamentally enhancing LLM interaction\\nquality through improved comprehension, more relevant responses, and greater continuity across extended\\nengagements, significantly expanding LLMs’ utility and resolving limitations imposed by restricted context\\nwindows [883].\\n5. System Implementations\\nBuilding upon the foundational components of Context Engineering, this section examines sophisticated\\nsystem implementations that integrate these components into practical, intelligent architectures. These\\nimplementations represent the evolution from theoretical frameworks to deployable systems that leverage\\ncontext engineering principles. We present four major categories of system implementations. RAG systems\\ndemonstrate external knowledge integration through modular architectures and graph-enhanced approaches.\\nMemory Systems showcase persistent context management through sophisticated memory architectures\\nenabling long-term learning. Tool-Integrated Reasoning transforms language models into world inter-\\nactors through function calling and environment interaction. Multi-Agent Systems present coordinated\\napproaches through communication protocols and orchestration mechanisms. Each implementation builds\\nupon foundational components while addressing specific challenges in context utilization, demonstrating\\nhow theoretical principles translate into practical systems.\\n5.1. Retrieval-Augmented Generation\\nRetrieval-Augmented Generation bridges the gap between parametric knowledge and dynamic information\\naccess by integrating external knowledge sources with language model generation. This implementation\\nenables models to access current, domain-specific information through modular architectures, agentic\\nframeworks, and graph-enhanced approaches that extend beyond static training data.\\n5.1.1. Modular RAG Architectures\\nModular RAG shifts from linear retrieval-generation architectures toward reconfigurable frameworks with\\nflexible component interaction [311, 1131, 591]. Unlike Naive RAG and Advanced RAG’s query rewriting,\\nModular RAG introduces hierarchical architectures: top-level RAG stages, middle-level sub-modules, and\\nbottom-level operational units [312, 730]. This transcends linear structures through routing, scheduling,\\nand fusion mechanisms enabling dynamic reconfiguration [312].\\nThe formal representation RAG = R, G operates through sophisticated module arrangements enabling\\n27'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 27}, page_content='Retrieval-Augmented Generation\\nModular RAG Architectures\\nAgentic RAG Systems\\nGraph-Enhanced RAG\\nExternal\\nInformation\\nApplications\\nInformation \\nSelection \\nMemory \\nOptimization\\nKnowledge \\nRetrieval \\nDynamic\\nFrameworks\\nFigure 4: Retrieval-Augmented Generation Framework: Overview of RAG system architectures including\\nModular RAG, Agentic RAG Systems, and Graph-Enhanced RAG approaches for external context integration.\\nRewrite-Retrieve-Read models and Generate-Read approaches, incorporating adaptive search modules,\\nRAGFusion for multi-query processing, routing modules for optimal data source selection, and hybrid\\nretrieval strategies addressing retrieval accuracy and context relevance [311, 491, 908, 1045, 880, 95].\\nContemporary frameworks demonstrate significant improvements in retrieval accuracy and trustworthi-\\nness [1372]. FlashRAG provides a modular toolkit with 5 core modules and 16 subcomponents enabling\\nindependent adjustment and pipeline combination [500]. KRAGEN enhances biomedical problem-solving\\nby integrating knowledge graphs with vector databases, utilizing biomedical knowledge graph-optimized\\nprompt generation to address hallucination in complex reasoning [397, 749, 973]. ComposeRAG implements\\natomic modules for Question Decomposition and Query Rewriting, incorporating self-reflection mechanisms\\nfor iterative refinement [1159]. This modularity facilitates integration with fine-tuning and reinforcement\\nlearning, enabling customization for specific applications and comprehensive toolkits supporting diverse NLP\\ntasks [312, 912, 4].\\n5.1.2. Agentic RAG Systems\\nAgentic RAG embeds autonomous AI agents into the RAG pipeline, enabling dynamic, context-sensitive\\noperations guided by continuous reasoning [965, 277]. These systems leverage reflection, planning, tool use,\\nand multi-agent collaboration to manage retrieval strategies dynamically and adapt workflows to complex\\ntask requirements [965]. RAG and agent workflows align through query rewriting corresponding to semantic\\ncomprehension, while retrieval phases correspond to planning and execution [622].\\nLLM-based autonomous agents extend basic language model capabilities through multimodal perception,\\ntool utilization, and external memory integration [1160, 1091, 931, 843]. External long-term memory serves\\nas a knowledge datastore enabling agents to incorporate and access information over extended periods\\n[1160, 382]. Unlike static approaches, Agentic RAG treats retrieval as dynamic operation where agents\\nfunction as intelligent investigators analyzing content and cross-referencing information [648, 162].\\nImplementation paradigms encompass prompt-based methods requiring no additional training and\\ntraining-based approaches optimizing models through reinforcement learning for strategic tool invocation\\n[648, 1318, 965]. Advanced systems enable LLM agents to query vector databases, access SQL databases, or\\nutilize APIs within single workflows, with methodological advances focusing on reasoning capabilities, tool\\nintegration, memory mechanisms, and instruction fine-tuning for autonomous decision-making [703, 6].\\nCore capabilities include reasoning and planning components through task decomposition, multi-plan\\nselection, and memory-augmented planning strategies enabling agents to break down complex tasks and\\n28'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 28}, page_content='select appropriate strategies [438, 439]. PlanRAG improves decision-making through plan-then-retrieve\\napproaches, enabling agents to evaluate multiple information sources and optimize retrieval strategies,\\nwhile SLA management frameworks address reconfigurable multi-agent architectures [162, 461]. Tool\\nutilization enables systems to employ diverse resources including search engines, calculators, and APIs,\\nwith frameworks like ReAct and Reflexion exemplifying how interleaving reasoning with actions enhances\\nadaptability [162, 1160, 956]. Memory mechanisms provide external long-term storage, while adaptive\\nretrieval strategies enable autonomous analysis of complexity and context [162, 1128].\\nSelf-reflection and adaptation mechanisms enable Agentic RAG systems to operate in dynamic envi-\\nronments through iterative feedback loops refining operations based on previous interaction outcomes\\n[1183, 686]. Advanced memory systems like MemoryBank implement update mechanisms inspired by the\\nEbbinghaus Forgetting Curve, enhancing agents’ ability to retrieve and apply learnings from past interactions\\n[1362, 165]. CDF-RAG employs closed-loop processes combining causal graph retrieval with reinforcement\\nlearning-driven query refinement and hallucination correction [531]. Self-RAG trains models that retrieve\\npassages on demand while reflecting on retrievals and generations, using reflection tokens to control behavior\\nduring inference [239, 41].\\n5.1.3. Graph-Enhanced RAG\\nGraph-based Retrieval-Augmented Generation shifts from document-oriented approaches toward structured\\nknowledge representations capturing entity relationships, domain hierarchies, and semantic connections\\n[120, 1353, 360, 1391]. This enables extraction of specific reasoning paths providing relevant information\\nto language models while supporting multi-hop reasoning through structured pathway navigation [120].\\nGraph structures minimize context drift and hallucinations by leveraging interconnectivity for enhanced\\ncontext-aware retrieval and logical coherence [512, 806].\\nKnowledge graphs serve as foundational representations encapsulating entities and interrelationships\\nin structured formats enabling efficient querying and semantic relationship capture [162, 1058]. Graph-\\nbased knowledge representations categorize into knowledge-based GraphRAG using graphs as knowledge\\ncarriers, index-based GraphRAG employing graphs as indexing tools, and hybrid GraphRAG combining\\nboth approaches [1199]. Sophisticated implementations include GraphRAG’s hierarchical indexing with\\ncommunity detection, PIKE’s multi-level heterogeneous knowledge graphs organizing documents into three-\\nlayer hierarchies, and EMG-RAG’s Editable Memory Graph architecture [313].\\nGraph Neural Networks enhance RAG systems by addressing limitations in handling structured knowledge,\\nwith GNNs excelling at capturing entity associations and improving knowledge consistency [228, 116]. GNN-\\nRAG implementations adopt lightweight architectures for effective knowledge graph element retrieval,\\nimproving graph structure capture before interfacing with language models [1370, 162]. The integration\\nprocess encompasses graph building through node and edge extraction, retrieval based on queries, and\\ngeneration incorporating retrieved information [1370].\\nMulti-hop reasoning capabilities enable graph-based systems to synthesize information across multiple\\nconnected knowledge graph nodes, facilitating complex query resolution requiring interconnected fact\\nintegration [1058, 166]. These systems employ structured representations capturing semantic relationships\\nbetween entities and domain hierarchies in ways that unstructured text cannot [1058, 166]. Advanced\\nframeworks like Hierarchical Lexical Graph preserve statement provenance while clustering topics for flexible\\nretrieval and linking entities for graph-based traversal [329]. Systems like GraphRAG, LightRAG, and\\nderivatives implement dual-level retrieval, hierarchical indexing, and graph-enhanced strategies enabling\\nrobust multilevel reasoning [1174, 313].\\n29'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 29}, page_content='Prominent architectures demonstrate diverse approaches to graph-enhanced retrieval, with optimization\\nstrategies showing significant improvements in retrieval effectiveness [106]. LightRAG integrates graph\\nstructures with vector representations through dual-level retrieval paradigms improving efficiency and content\\nquality [412, 717]. HippoRAG leverages Personalized PageRank over knowledge graphs achieving notable\\nimprovements in multi-hop question answering [1088, 746, 366]. HyperGraphRAG proposes hypergraph\\nstructured representations advancing beyond binary relations [717]. RAPTOR provides hierarchical summary\\ntree construction for recursive context generation, while PathRAG introduces pruning techniques for graph-\\nbased retrieval [1349, 928, 134]. These structured approaches enable transparent reasoning pathways\\nwith explicit entity connections, reducing noise and improving semantic understanding while overcoming\\ntraditional RAG challenges [1174, 512].\\n5.1.4. Applications\\nReal-time RAG systems address critical challenges in production environments where dynamic knowledge\\nbases require continuous updates and low-latency responses [1339, 528]. Core challenges include efficient\\ndeployment and processing pipeline optimization, with existing frameworks lacking plug-and-play solutions\\nnecessitating system-level optimizations [1339]. Integration of streaming data introduces complications as\\ntraditional architectures demonstrate poor accuracy with frequently changing information and decreased\\nefficiency as document volumes grow [514].\\nDynamic retrieval mechanisms advance over static approaches by continuously updating strategies\\nduring generation, adjusting goals and semantic vector spaces in real-time based on generation states and\\nidentified knowledge gaps [384]. Current limitations in determining optimal retrieval timing and query\\nformulation are addressed through Chain-of-Thought reasoning, iterative retrieval processes, decomposed\\nprompting, and LLM-generated content for dynamic retrieval enabling adaptive information selection, with\\napproaches extending to adaptive control mechanisms enhancing generation quality through reflective tags\\n[992, 530, 85, 533, 1239].\\nLow-latency retrieval approaches leverage graph-based methods demonstrating significant promise in\\nspeed-accuracy optimization, with dense passage retrieval techniques providing foundational improvements\\n[519]. LightRAG’s dual-level retrieval system enhances information discovery while integrating graph\\nstructures with vector representations for efficient entity relationship retrieval, reducing response times\\nwhile maintaining relevance [360]. Multi-stage retrieval pipelines optimize computational efficiency through\\ntechniques like graph-based reranking, enabling dynamic access to current information while reducing\\nstorage requirements [974].\\nScalability solutions incorporate distributed processing architectures with efficient data partitioning,\\nquery optimization, and fault tolerance mechanisms adapting to changing stream conditions [1040, 35].\\nMemory optimization through transformed heavy hitters streaming algorithms intelligently filters irrelevant\\ndocuments while maintaining quality, particularly valuable for frequently changing content [514]. Produc-\\ntion frameworks demonstrate efficiency gains through modular RAG architectures supporting pre-retrieval\\nprocesses like query expansion and post-retrieval refinements such as compression and selection, enabling\\nfine-tuning of individual components [1069].\\nIncremental indexing and dynamic knowledge updates ensure systems adapt to new information without\\nfull retraining, particularly crucial in rapidly evolving domains like cybersecurity and climate finance\\napplications [830, 1056]. Modern frameworks incorporate dynamic knowledge retrieval methods enabling\\ncontinuous strategy adjustment based on evolving input and contextual information, enhancing interactivity\\nand semantic understanding while increasing applicability across cross-domain integration [384]. Advanced\\n30'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 30}, page_content='agent-based approaches demonstrate sophisticated task allocation capabilities in complex environments, such\\nas coordinated UAV operations requiring real-time decision-making, with applications extending to grounded\\nplanning for embodied agents [1315, 975]. Dynamic Retrieval Augmented Generation frameworks like\\nDRAGON-AI showcase specialized implementations for ontology generation, combining textual and logical\\ncomponents while incorporating self-memory mechanisms enabling iterative improvement [1043]. These\\nadvances represent significant evolution toward seamlessly integrating real-time knowledge with flexible\\nretrieval capabilities in dynamic environments.\\n5.2. Memory Systems\\nMemory Systems enable LLMs to transcend stateless interactions by implementing persistent information\\nstorage, retrieval, and utilization mechanisms. This implementation transforms models from pattern-matching\\nprocessors into sophisticated agents capable of learning, adaptation, and long-term contextual understanding\\nacross extended interactions.\\nMemory Systems\\nMemory-Enhanced \\nAgents\\nEvaluation and \\nChallenges\\nMemory \\nArchitectures\\nUltra-long Context\\nContext Window\\nSelf-Attention\\nExternal Memory\\nHierarchical Memory\\nFigure 5: Memory Systems Framework: Overview of memory architectures, memory-enhanced agents, and\\nevaluation challenges for ultra-long context processing in LLMs.\\n5.2.1. Memory Architectures\\nMemory distinguishes sophisticated language systems from pattern-matching models, enabling information\\nprocessing, storage, and utilization across natural language tasks [1182, 1167, 296]. LLMs face considerable\\nmemory system constraints despite breakthroughs in text generation and multi-turn conversations [1182].\\nNeural memory mechanisms struggle with inadequate structured information storage and reliance on\\napproximate vector similarity calculations rather than precise symbolic operations, challenging accurate\\nstorage and retrieval for multi-hop reasoning [423]. These limitations represent critical challenges for\\ndeveloping AI systems operating effectively in complex real-world applications [544].\\nMemory Classification Frameworks\\nLLM memory systems can be organized into multiple classification\\nframeworks. The primary temporal classification divides memory into three categories: sensory memory\\n(input prompts), short-term memory (immediate context processing), and long-term memory (external\\ndatabases or dedicated structures) [935]. From a persistence perspective, short-term memory includes key-\\nvalue caches and hidden states existing only within single sessions, while long-term memory encompasses\\ntext-based storage and knowledge embedded in model parameters, persisting across multiple interaction\\ncycles [935, 818].\\n31'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 31}, page_content='Implementation-based classifications identify parametric memory (knowledge encoded in model weights),\\nephemeral activation memory (context-limited runtime states), and plaintext memory accessed through\\nRetrieval-Augmented Generation methods [637]. Current implementations lack sophisticated lifecycle\\nmanagement and multi-modal integration, limiting long-term knowledge evolution. Feed-forward network\\nlayers serve as key-value tables storing memory, functioning as “inner lexicon” for word retrieval and creating\\nmechanisms analogous to human associative memory [518, 325, 326, 764, 464]. These classification schemes\\nreflect attempts to develop LLM memory architectures paralleling human cognitive systems [1167].\\nShort-Term Memory Mechanisms\\nShort-term memory in LLMs operates through the context window,\\nserving as working memory maintaining immediate access to previously processed tokens [1282]. This\\nfunctionality is implemented through key-value caches storing token representations but disappearing when\\nsessions terminate [891]. Architectural variations demonstrate significant differences: transformer-based\\nmodels implement working memory systems flexibly retrieving individual token representations across\\narbitrary delays, while LSTM architectures maintain coarser, rapidly-decaying semantic representations\\nweighted toward earliest items [40].\\nModern LLM short-term memory frequently manifests as in-context learning, reflecting models’ ability\\nto acquire and process information temporarily within context windows [1180, 103]. This enables few-\\nshot learning and task adaptation without parameter updates. Research identifies three primary memory\\nconfigurations: full memory (utilizing entire context history), limited memory (using context subsets), and\\nmemory-less operation (without historical context) [1044]. Despite advances expanding context windows to\\nmillions of tokens, LLMs struggle with effective reasoning over extended contexts, particularly when relevant\\ninformation appears in middle positions [891, 685].\\nLong-Term Memory Implementations\\nLLMs face significant challenges maintaining long-term memory\\ndue to context window limitations and catastrophic forgetting [114]. External memory-based methods\\naddress these limitations by utilizing physical storage to cache historical information, allowing relevant\\nhistory retrieval without maintaining all information within constrained context windows [682, 1362]. These\\napproaches contrast with internal memory-based methods focusing on reducing self-attention computational\\ncosts to expand sequence length [682, 287].\\nLong-term memory implementations categorize into knowledge-organization methods (structuring mem-\\nory into interconnected semantic networks), retrieval mechanism-oriented approaches (integrating semantic\\nretrieval with forgetting curve mechanisms), and architecture-driven methods (implementing hierarchical\\nstructures with explicit read-write operations) [515, 1362, 444]. Memory storage representations can be\\nfurther divided into token-level memory (information stored as structured text for direct retrieval) and latent-\\nspace memory (utilizing high-dimensional vectors for abstract and compact information representation)\\n[1216, 1124]. Advanced approaches incorporate psychological principles, with MemoryBank implementing\\nEbbinghaus Forgetting Curve theory for selective memory preservation based on temporal factors [1362],\\nemotion-aware frameworks employing Mood-Dependent Memory theory [444], and memorization mecha-\\nnisms balancing performance advantages with privacy concerns through extraction vulnerability analysis\\n[1041, 122, 123].\\nMemory Access Patterns and Structures\\nLLMs exhibit characteristic memory access patterns with notable\\nsimilarities to human cognitive processes, demonstrating clear primacy and recency effects when recalling\\n32'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 32}, page_content='information lists [477]. Memory retrieval operates through sequential access (retrieving content in consecu-\\ntive order) and random access (accessing information from arbitrary points without processing preceding\\ncontent) [1387]. Memory persistence studies employ recognition experiments, recall experiments, and\\nretention experiments to quantify information accessibility duration and retrieval conditions [810], with\\ncognitive psychology concepts like semantic and episodic memory integration improving LLM information\\nsynthesis capabilities [240].\\nMemory organization encompasses diverse structural approaches including textual-form storage (complete\\nand recent agent-environment interactions, retrieved historical interactions, external knowledge), knowl-\\nedge representation structures (chunks, knowledge triples, atomic facts, summaries, mixed approaches),\\nhierarchical systems with library-enhanced reasoning components, and functional patterns organized by\\ntasks, temporal relevance, or semantic relationships [1329, 1290, 1027]. Core memory operations include\\nencoding (transforming textual information into latent space embeddings), retrieval (accessing relevant\\ninformation based on semantic relevance, importance, and recency), reflection (extracting higher-level\\ninsights), summarization (condensing texts while highlighting critical points), utilization (integrating mem-\\nory components for unified outputs), forgetting (selective information discarding), truncation (formatting\\nwithin token limitations), and judgment (assessing information importance for storage prioritization) [1331].\\nThese structures offer varying trade-offs between comprehensiveness, retrieval efficiency, and computational\\nrequirements.\\n5.2.2. Memory-Enhanced Agents\\nMemory systems fundamentally transform LLMs from stateless pattern processors into sophisticated agents\\ncapable of persistent learning and adaptation across extended interactions [1259]. Memory-enhanced agents\\nleverage both short-term memory (facilitating real-time responses and immediate context awareness) and\\nlong-term memory (supporting deeper understanding and knowledge application over extended periods) to\\nadapt to changing environments, learn from experiences, and make informed decisions requiring persistent\\ninformation access [1259].\\nAgent Architecture Integration\\nContemporary LLM agents employ memory systems analogous to computer\\nmemory hierarchies, with short-term memory functioning as primary storage for contextual understanding\\nwithin context windows, while long-term memory serves as persistent storage for extended information\\nretention [770]. From object-oriented perspectives, AI systems generate personal memories related to\\nindividual users and system memories containing intermediate task results [1167]. Structured frameworks\\nlike MemOS classify memory into Parametric Memory (knowledge encoded in model weights), Activation\\nMemory, and Plaintext Memory, with parametric memory representing long-term knowledge embedded\\nwithin feedforward and attention layers enabling zero-shot generation [637].\\nMemory integration frameworks have evolved to address LLM limitations through sophisticated archi-\\ntectures. The Self-Controlled Memory (SCM) framework enhances long-term memory through LLM-based\\nagent backbones, memory streams, and memory controllers managing updates and utilization [649]. The\\nREMEMBERER framework equips LLMs with experience memory exploiting past episodes across task goals,\\nenabling success/failure learning without parameter fine-tuning through verbal reinforcement and self-\\nreflective feedback mechanisms [1299]. Advanced systems like MemLLM implement structured read-write\\nmemory modules addressing challenges in memorizing rare events, updating information, and preventing\\nhallucinations [779]. Autonomous agents leveraging LLMs rely on four essential components—perception,\\nmemory, planning, and action—working together to enable environmental perception, interaction recall,\\n33'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 33}, page_content='Model\\nTextual Form\\nParametric Form\\nComplete\\nRecent\\nRetrieved\\nExternal\\nFine-tuning\\nEditing\\nCore Memory Systems\\nMemoryBank [1363]\\n×\\n×\\n✓\\n×\\n×\\n×\\nRET-LLM [778]\\n×\\n×\\n✓\\n×\\n×\\n×\\nChatDB [423]\\n×\\n×\\n✓\\n×\\n×\\n×\\nTiM [683]\\n×\\n×\\n✓\\n×\\n×\\n×\\nVoyager [1078]\\n×\\n×\\n✓\\n×\\n×\\n×\\nMemGPT [814]\\n×\\n✓\\n✓\\n×\\n×\\n×\\nRecMind [1115]\\n✓\\n×\\n×\\n×\\n×\\n×\\nRetroformer [1249]\\n✓\\n×\\n×\\n✓\\n✓\\n×\\nExpeL [1337]\\n✓\\n×\\n✓\\n✓\\n×\\n×\\nSynapse [1357]\\n×\\n×\\n✓\\n×\\n×\\n×\\nAgent-Based Systems\\nChatDev [855]\\n✓\\n×\\n×\\n×\\n×\\n×\\nInteRecAgent [450]\\n×\\n✓\\n✓\\n✓\\n×\\n×\\nTPTU [909, 554]\\n✓\\n×\\n×\\n✓\\n×\\n×\\nMetaGPT [409]\\n✓\\n×\\n×\\n×\\n×\\n×\\nS3 [301]\\n×\\n×\\n✓\\n×\\n×\\n×\\nMem0 [169]\\n×\\n×\\n✓\\n×\\n×\\n×\\nAdvanced Memory Architectures\\nLarimar [198]\\n×\\n✓\\n✓\\n×\\n×\\n✓\\nEM-LLM [286]\\n×\\n✓\\n✓\\n×\\n×\\n×\\nControllable Working Memory [597]\\n✓\\n✓\\n✓\\n×\\n✓\\n×\\nWorking Memory Hub [355]\\n✓\\n✓\\n✓\\n✓\\n×\\n×\\nRecent and Emerging Systems\\nLLM-based Opinion Dynamics [175]\\n×\\n×\\n✓\\n×\\n×\\n×\\nMemory Sandbox [456]\\n×\\n×\\n✓\\n×\\n×\\n✓\\nA-MEM [1203]\\n×\\n×\\n✓\\n×\\n×\\n✓\\nMemEngine [1331]\\n×\\n×\\n✓\\n✓\\n×\\n×\\nHIAGENT [429]\\n×\\n✓\\n✓\\n×\\n×\\n×\\nMemInsight [917]\\n×\\n×\\n✓\\n✓\\n×\\n×\\nMemory Sharing (MS) [302]\\n×\\n×\\n✓\\n✓\\n×\\n×\\nMemoRAG [860]\\n✓\\n×\\n✓\\n✓\\n✓\\n×\\nEcho [694]\\n✓\\n✓\\n✓\\n✓\\n✓\\n×\\nTable 6: Extended from [1329]: Memory implementation patterns. ✓= Adopted, ×= Not Adopted\\nand real-time planning and execution [614, 38].\\nReal-World Applications\\nMemory-enhanced LLM agents have demonstrated transformative impact across\\ndiverse application domains. In conversational AI, memory systems enable more natural, human-like\\ninteractions by recalling past experiences and user preferences to deliver personalized, context-aware\\nresponses. Commercial implementations include Charlie Mnemonic (combining Long-Term, Short-Term, and\\nepisodic memory using GPT-4), Google Gemini (leveraging long-term memory for personalized experiences\\nacross Google’s ecosystem), and ChatGPT Memory (remembering conversations across sessions) [578].\\nUser simulation applications employ LLM-powered conversational agents mimicking human behavior for\\ncost-effective dialogue system evaluation, adapting flexibly across open-domain dialogues, task-oriented\\ninteractions, and conversational recommendation [204], with systems like Memory Sandbox enabling user\\ncontrol over conversational memories through data object manipulation [455].\\n34\\n\\n\\n\\n\\n\\n\\n\\n\\n|Col1|Complete Recent Retrieved External|Fine-tuning Editing|\\n|---|---|---|\\n\\n\\n|Core Memory Systems|Col2|Col3|\\n|---|---|---|\\n|MemoryBank [1363]<br>RET-LLM [778]<br>ChatDB [423]<br>TiM [683]<br>Voyager [1078]<br>MemGPT [814]<br>RecMind [1115]<br>Retroformer [1249]<br>ExpeL [1337]<br>Synapse [1357]|_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>✓<br>✓<br>_×_<br>_×_<br>✓<br>_×_|_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_|\\n\\n\\n|Agent-Based Systems|Col2|Col3|\\n|---|---|---|\\n|ChatDev [855]<br>InteRecAgent [450]<br>TPTU [909, 554]<br>MetaGPT [409]<br>S3 [301]<br>Mem0 [169]|✓<br>_×_<br>_×_<br>_×_<br>_×_<br>✓<br>✓<br>✓<br>✓<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_|_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_|\\n\\n\\n|Advanced Memory Architectures|Col2|Col3|\\n|---|---|---|\\n|Larimar [198]<br>EM-LLM [286]<br>Controllable Working Memory [597]<br>Working Memory Hub [355]|_×_<br>✓<br>✓<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>✓<br>✓<br>✓<br>_×_<br>✓<br>✓<br>✓<br>✓|_×_<br>✓<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_|\\n\\n\\n|Recent and Emerging Systems|Col2|Col3|\\n|---|---|---|\\n|LLM-based Opinion Dynamics [175]<br>Memory Sandbox [456]<br>A-MEM [1203]<br>MemEngine [1331]<br>HIAGENT [429]<br>MemInsight [917]<br>Memory Sharing (MS) [302]<br>MemoRAG [860]<br>Echo [694]|_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>✓<br>✓<br>_×_<br>_×_<br>_×_<br>✓<br>✓<br>_×_<br>_×_<br>✓<br>✓<br>✓<br>_×_<br>✓<br>✓<br>✓<br>✓<br>✓<br>✓|_×_<br>_×_<br>_×_<br>✓<br>_×_<br>✓<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>_×_<br>✓<br>_×_<br>✓<br>_×_|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 34}, page_content='Task-oriented agents utilize memory to perform complex autonomous operations with minimal human\\nintervention, employing LLMs as controllers extended through multimodal perception, tool utilization, and\\nexternal memory [1160]. Applications span recommendation systems (RecMind providing personalized rec-\\nommendations through planning and external knowledge, InteRecAgent employing LLMs with recommender\\nmodels as tools), autonomous driving (DiLu instilling human-like knowledge through reasoning, reflection,\\nand memory), scientific research (ChemCrow automating chemical synthesis design and execution), and\\nsocial simulation (generative agents exhibiting believable behavior through memory storage and synthesis)\\n[1019, 647, 92, 825]. Proactive conversational agents address challenges in strategic dialogue scenarios\\nrequiring goal-oriented conversation steering through prompt-based policy planning methods and AI feedback\\ngeneration based on dialogue history [204, 203].\\nPersonalized assistant applications leverage memory to maintain coherent long-term relationships with\\nusers, with memory components serving as structured repositories storing contextually relevant information\\nincluding user preferences and historical interactions [438]. Domain-specific implementations include health-\\ncare assistants employing memory coordination for medical interactions [1316, 1307], recommendation\\nagents leveraging external knowledge bases [1316, 1293], educational agents providing context-aware\\nsupport through memory-enabled progress tracking [647], and specialized frameworks like MARK enhancing\\npersonalized AI assistants through user preference memory [299].\\nMemory Technologies and Integration Methods\\nMemory technology evolution addresses fundamental\\ncontext window limitations through RAG, which combines parametric and non-parametric memory for\\nlanguage generation using pre-trained seq2seq models and dense vector indices [1209, 591]. This approach\\nenables access to information beyond parameter storage without requiring retraining, significantly extending\\nknowledge capabilities. Advanced memory mechanisms including vector databases and retrieval-augmented\\ngeneration enable vast information storage with quick relevant data access, incorporating short-term contex-\\ntual memory and long-term external storage [38, 367, 1184, 507].\\nNon-parametric approaches maintain frozen LLM parameters while leveraging external resources like\\nRAG to enrich task contexts [934]. Systems like Reflexion implement verbal reinforcement through self-\\nreflective feedback in episodic memory buffers, while REMEMBERER incorporates persistent experience\\nmemory enabling learning from past successes and failures. Advanced architectures like MemoryBank enable\\nmemory retrieval, continuous evolution through updates, and personality adaptation by integrating previous\\ninteraction information [1202, 1362].\\nSpecialized memory architectures address particular agent requirements through sophisticated organi-\\nzation and retrieval mechanisms. While early systems required predefined storage structures and retrieval\\ntiming, newer systems like Mem0 incorporate graph databases following RAG principles for more effective\\nmemory organization and relevance-based retrieval [1202]. Commercial and open-source implementa-\\ntions including OpenAI ChatGPT Memory, Apple Personal Context, mem0, and MemoryScope demonstrate\\nwidespread adoption of memory systems for enhanced personalization capabilities [1167]. Tool-augmentation\\nparadigms validate effectiveness in complex task decomposition while leveraging world interaction tools,\\nwith memory-enhanced agents becoming central to modern AI systems performing complex tasks through\\nnatural language integration of planning, tool use, memory, and multi-step reasoning [247, 356, 1091, 34].\\n5.2.3. Evaluation and Challenges\\nMemory evaluation frameworks have emerged as critical components for systematically assessing LLM agent\\ncapabilities across multiple dimensions, reflecting the multifaceted nature of memory in intelligent systems.\\n35'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 35}, page_content='These comprehensive evaluation approaches reveal significant challenges while pointing toward promising\\nresearch directions that could unlock new capabilities for memory-enhanced agents.\\nEvaluation Frameworks and Metrics\\nContemporary memory evaluation employs specialized metrics\\nextending beyond traditional NLP performance indicators to capture nuanced memory functionality aspects\\n[1330]. Effectiveness metrics focus on factual information storage and utilization through accuracy measures\\n(correctness of responses based on historical messages) and recall@5 indicators (percentage of relevant\\nmessages retrieved within top-5 results). Efficiency metrics examine temporal aspects through response time\\n(duration for information retrieval and utilization) and adaptation time (period required for new information\\nstorage) [1330].\\nExtensive benchmarks such as LongMemEval assess five fundamental long-term memory capabilities:\\ninformation extraction, temporal reasoning, multi-session reasoning, knowledge updates, and abstention\\nthrough 500 carefully selected questions, demonstrating 30% accuracy degradation in commercial assistants\\nthroughout prolonged interactions, while automated memory evaluation frameworks facilitate thorough\\nassessment extending beyond passkey search methodologies [1171]. Dedicated frameworks target episodic\\nmemory via benchmarks assessing temporally-situated experiences, with research demonstrating that cutting-\\nedge models including GPT-4, Claude variants, and Llama 3.1 encounter difficulties with episodic memory\\nchallenges involving interconnected events or intricate spatio-temporal associations even in comparatively\\nbrief contexts [457]. Contemporary LLM benchmarks predominantly concentrate on assessing models’\\nretention of factual information and semantic relationships while substantially overlooking episodic memory\\nassessment—the capacity to contextualize memories with temporal and spatial occurrence details [841].\\nTask-specific evaluations encompass long-context passage retrieval (locating specific paragraphs within\\nextended contexts), long-context summarization (developing comprehensive understanding for concise\\nsummaries), NarrativeQA (answering questions based on lengthy narratives), and specialized benchmarks\\nlike MADail-Bench evaluating both passive and proactive memory recall in conversational contexts with\\nnovel dimensions including memory injection, emotional support proficiency, and intimacy assessment\\n[1329, 1380, 550, 386]. Additional task-specific frameworks include QMSum for meeting summarization,\\nQuALITY for reading comprehension, DialSim for dialogue-based QA requiring spatiotemporal memory,\\nand MEMENTO for personalized embodied agent evaluation using two-stage processes to assess memory\\nutilization in physical environment tasks [1380, 566].\\nCurrent Limitations and Challenges\\nMemory evaluation faces substantial challenges limiting effective\\nassessment of capabilities. Fundamental limitations include absence of consistent, rigorous methodologies\\nfor assessing memory performance, particularly regarding generalization beyond training data [284]. The\\nlack of standardized benchmarks specifically designed for long-term memory evaluation represents another\\nsignificant obstacle, with existing frameworks often failing to capture the full spectrum of memory capabilities\\nneeded for human-like intelligence [1071].\\nArchitectural constraints significantly complicate evaluation efforts, as most contemporary LLM-based\\nagents operate in fundamentally stateless manners, treating interactions independently without truly accu-\\nmulating knowledge incrementally over time [1355, 1354], despite advances in working memory through\\nattentional tagging mechanisms enabling flexible memory representation control [864]. This limitation pre-\\nvents genuine lifelong learning assessment—a cornerstone of human-level intelligence involving continuous\\nknowledge acquisition, retention, and reuse across diverse contexts and extended time horizons.\\n36'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 36}, page_content='Methodological issues arise when isolating memory-specific performance from other intelligence aspects,\\nchallenging determination of whether failures stem from inadequate memory mechanisms or reasoning limi-\\ntations [284]. Dynamic memory usage in real-world applications poses evaluation challenges, as controlled\\nlaboratory tests inadequately capture memory system performance in complex scenarios where information\\nrelevance changes unpredictably [1071].\\nOptimization Strategies and Future Research Directions\\nMemory optimization encompasses diverse\\ntechniques enhancing utilization while minimizing computational overhead and maximizing efficiency.\\nBiologically-inspired forgetting mechanisms provide effective optimization approaches, with frameworks like\\nMemoryBank implementing Ebbinghaus forgetting curves to selectively preserve and discard information\\nbased on temporal factors and significance [1362]. Reflection-based optimization through systems like\\nReflexion enables performance assessment through integrated evaluation and self-reflection, creating dual\\nfeedback systems refining memory and behavior through continuous learning [300].\\nHierarchical memory structures optimize information organization through multi-level formats enabling\\nefficient retrieval, demonstrated by Experience-based Hierarchical Control frameworks with rapid memory\\naccess modules [862], memory consolidation processes through bidirectional fast-slow variable interactions\\n[63], and Adaptive Cross-Attention Networks dynamically ranking memories based on query relevance [406].\\nFuture research directions encompass hybrid memory frameworks combining parametric precision\\nwith non-parametric efficiency [934], automated feedback mechanisms for scalable response evaluation\\n[885], multi-agent memory systems enabling collaborative learning through shared external memories\\n[302], enhanced metadata learning with knowledge graph integration [888, 382], domain-specific memory\\narchitectures for specialized applications [501], cognitive-inspired optimization incorporating memory\\nconsolidation during inactive periods [752], and parameter-efficient memory updates through techniques\\nlike Low-Rank Adaptation for efficient knowledge integration [424, 252]. These developments promise\\nadvancing memory-enhanced LLM agents toward sophisticated, human-like cognitive capabilities while\\naddressing computational and architectural limitations, with applications extending to long-term robotic\\nplanning, real-world decision-making systems, and collaborative AI assistants through streaming learning\\nscenarios and continuous feedback integration [1150, 1336, 1269].\\n5.3. Tool-Integrated Reasoning\\nTool-Integrated Reasoning transforms language models from passive text generators into active world\\ninteractors capable of dynamic tool utilization and environmental manipulation. This implementation\\nenables models to transcend their inherent limitations through function calling mechanisms, integrated\\nreasoning frameworks, and sophisticated environment interaction capabilities.\\n5.3.1. Function Calling Mechanisms\\nFunction calling transforms LLMs from generative models into interactive agents through structured output\\ngeneration leveraging functions’ abstraction mechanism, enabling external tool manipulation and access to\\ncurrent, domain-specific information for complex problem-solving [5, 663, 331, 874, 58, 517, 1104].\\nEvolution began with Toolformer’s self-supervised approach demonstrating autonomous API learning,\\ninspiring ReAct’s “thought-action-observation” cycle, progressing through specialized models like Gorilla\\nand comprehensive frameworks including ToolLLM, RestGPT, with OpenAI’s JSON standardization, while\\n37'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 37}, page_content='Tool-Augmented Systems\\nFunction Calling \\nMechanisms\\nTool-Integrated \\nReasoning\\nEnvironment \\nInteraction\\nFrom \"Text Generator\" to \"World Interactor\"\\nExternal Tools\\nDynamic\\nData\\nSearch\\nEngine\\nUser\\nInteraction\\nCalculation \\nEngine\\nFigure 6: Tool-Augmented Systems Framework: Evolution from text generators to world interactors through\\nfunction calling mechanisms, tool-integrated reasoning, and environment interaction capabilities.\\nadvanced systems like Chameleon enabled multimodal question answering and TaskMatrix.AI managed AI\\nmodels across domains [931, 248, 648, 541, 915, 866, 867, 709, 653, 945].\\nTechnical implementation involves fine-tuning (dominant method providing stable capabilities via exten-\\nsive API training but requiring significant resources) and prompt engineering (flexible, resource-efficient but\\nunstable), with approaches like “Reverse Chain” enabling API operation via prompts, addressing challenges\\nin large tool management [388, 5, 1323, 785, 144, 250].\\nCore process encompasses intent recognition, function selection, parameter-value-pair mapping, function\\nexecution, and response generation, with modern implementations utilizing structured LLM outputs for\\nexternal program interaction, while tools include diverse interfaces (digital systems, scratch pads, user inter-\\nactions, other LLMs, developer code), requiring complex navigation of tool selection, argument formulation,\\nand result parsing [1259, 663, 1132, 189, 952, 584, 902].\\nTraining Methodologies and Data Systems\\nTraining methodologies evolved from basic prompt-based\\napproaches to sophisticated multi-task learning frameworks, with fine-tuning on specialized datasets through\\nsystems like ToolLLM and Granite-20B-FunctionCalling, beginning with synthetic single-tool data followed\\nby human annotations [388, 5, 353, 771, 1226].\\nData generation strategies include Weaver’s GPT-4-based environment synthesis, APIGen’s hierarchical\\nverification pipelines (format checking, function execution, semantic verification), generating 60,000+\\nhigh-quality entries across thousands of APIs [1104, 1177, 1259, 1156, 65, 1393, 743].\\nTool selection enhancement involves irrelevance-aware data augmentation, with Hammer’s function\\nmasking techniques, oracle tool mixing for increased difficulty, tool intent detection synthesis for over-\\ntriggering mitigation, emphasizing high-quality data through stringent filtering and format verification\\n[664, 10, 353, 467, 1291, 214].\\nSelf-improvement paradigms reduce external supervision dependence through JOSH algorithm’s sparse\\nreward simulation environments and TTPA’s token-level optimization with error-oriented scoring, demon-\\nstrating improvements while preserving general capabilities [573, 440, 362, 1262].\\nSophisticated benchmarks include API-Bank (73 APIs, 314 dialogues), StableToolBench (API instability\\nsolutions), NesTools (nested tool evaluation), ToolHop (995 queries, 3,912 tools), addressing single-tool to\\n38'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 38}, page_content='multi-hop scenarios [615, 359, 373, 1255, 821, 987, 1248, 979].\\n5.3.2. Tool-Integrated Reasoning\\nTool-Integrated Reasoning (TIR) represents a paradigmatic advancement in Large Language Model capabili-\\nties, addressing fundamental limitations including outdated knowledge, calculation inaccuracy, and shallow\\nreasoning by enabling dynamic interaction with external resources during the reasoning process [858].\\nUnlike traditional reasoning approaches that rely exclusively on internal model knowledge, TIR establishes a\\nsynergistic relationship where reasoning guides complex problem decomposition into manageable subtasks\\nwhile specialized tools ensure accurate execution of each computational step [771]. This paradigm extends\\nbeyond conventional text-based reasoning by requiring models to autonomously select appropriate tools,\\ninterpret intermediate outputs, and adaptively refine their approach based on real-time feedback [858].\\nThe evolution of TIR methodologies encompasses three primary implementation categories addressing\\ndistinct aspects of tool utilization optimization. Prompting-based methods guide models through carefully\\ncrafted instructions without additional training, exemplified by approaches that decompose mathematical\\nproblems into executable code while delegating computation to Python interpreters [152, 595]. Supervised\\nfine-tuning approaches teach tool usage through imitation learning, with systems like ToRA focusing on\\nmathematical problem-solving by integrating natural language reasoning with computational libraries and\\nsymbolic solvers [341]. Reinforcement learning methods optimize tool-use behavior through outcome-driven\\nrewards, though current implementations often prioritize final correctness without considering efficiency,\\npotentially leading to cognitive offloading phenomena where models over-rely on external tools [223].\\nIn operational terms, TIR-based agents serve as intelligent orchestrators that systematically interweave\\ncognitive processing with external resource engagement to achieve targeted outcomes [1087]. This mech-\\nanism requires the harmonious integration of intrinsic reasoning capabilities and extrinsic tool utilization\\nfor progressive knowledge synthesis toward objective fulfillment, where the agent’s execution pathway is\\nformally characterized as a structured sequence of tool activations coupled with corresponding information\\nassimilation events [1087]. Emerging developments have established Agentic Reasoning architectures that\\namplify language model intelligence by incorporating autonomous tool-deploying agents, fluidly orchestrat-\\ning web-based information retrieval, computational processing, and layered reasoning-memory integration to\\ntackle sophisticated challenges necessitating comprehensive research and cascaded logical analysis [1153].\\nImplementation Frameworks and Paradigms\\nSingle-tool frameworks established foundational principles\\nof tool-integrated reasoning through specialized implementations targeting specific computational domains.\\nProgram-Aided Language Models (PAL) pioneered problem decomposition strategies by generating executable\\ncode while delegating mathematical computations to Python interpreters [305]. ToolFormer demonstrated\\nthat language models could learn external API usage with minimal demonstrations, incorporating calculators,\\nsearch engines, and diverse tools to enhance computational capabilities [931]. ToRA advanced mathematical\\nreasoning by integrating natural language processing with computational libraries and symbolic solvers, while\\nReTool applied reinforcement learning to optimize code interpreter usage, demonstrating improvements in\\nself-correction patterns [341, 1311, 965]. Self-Edit utilizes execution results of generated code to improve\\ncode quality for competitive programming tasks, employing a fault-aware code editor to correct errors based\\non test case results [1309].\\nMulti-tool coordination systems address the complexity of orchestrating heterogeneous tools within\\nintegrated reasoning architectures. ReAct pioneered the interleaving of reasoning traces with task-specific\\nactions, enabling models to think and act complementarily where reasoning supports plan tracking while\\n39'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 39}, page_content='actions interface with external information sources [1245]. Chameleon introduced plug-and-play composi-\\ntional reasoning by synthesizing programs combining vision models, search engines, and Python functions\\nwith an LLM-based planner core [709]. AutoTools established automated frameworks transforming raw tool\\ndocumentation into executable functions, reducing manual engineering requirements in tool integration\\n[419, 952]. Chain-of-Agents (CoA) trains models to decode reasoning chains with abstract placeholders,\\nsubsequently calling domain-specific tools to fill knowledge gaps [594, 1327].\\nAgent-based frameworks represent the most sophisticated evolution of TIR systems, moving beyond static\\nprompting approaches to create autonomous and adaptive AI systems. Unlike conventional tool-use that\\nfollows rigid patterns, agent models learn to couple Chain-of-Thought (CoT) and Chain-of-Action (CoA)\\npatterns into their core behavior, resulting in stronger logical coherence and natural transitions between\\nreasoning and action [1328]. These systems build upon foundational agent architectures including reactive\\nsystems that map perceptions directly to actions, deliberative systems implementing Belief-Desire-Intention\\n(BDI) models, and hybrid architectures combining multiple subsystems in hierarchical structures [728].\\nMethod\\nTool Categories\\nSearch &\\nRetrieval\\nComputation &\\nCode Execution\\nKnowledge Base\\n& QA\\nAPIs &\\nExternal Services\\nMultimodal\\nTools\\nLanguage\\nProcessing\\nInteractive\\nEnvironments\\nDomain-Specific\\nTools\\nReAct [1247]\\n✓\\n✓\\n✓\\nToolformer [931]\\n✓\\n✓\\n✓\\n✓\\n✓\\nToolkenGPT [378]\\n✓\\n✓\\n✓\\n✓\\n✓\\nToolLLM [867]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nToRA [341]\\n✓\\nPAL [303]\\n✓\\nHuggingGPT [945]\\n✓\\n✓\\nGPT4Tools [1225]\\n✓\\nCRITIC [340]\\n✓\\n✓\\n✓\\nChain of Code [595]\\n✓\\nTRICE [863]\\n✓\\n✓\\n✓\\n✓\\nTP-LLaMA [149]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nAlignToolLLaMA [161]\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nReTool [270]\\n✓\\nTool-Star [221]\\n✓\\n✓\\nARTIST [965]\\n✓\\nEgo-R1 [1038]\\n✓\\nVTool-R1 [1155]\\n✓\\nKG-Agent [487]\\n✓\\n✓\\nCACTUS [755]\\n✓\\nMuMath-Code [1265]\\n✓\\nToRL [621]\\n✓\\nMetaTool [452]\\n✓\\n✓\\n✓\\n✓\\nToolEyes [1253]\\n✓\\n✓\\nGraph-CoT [495]\\n✓\\n✓\\nToolRL [858]\\n✓\\n✓\\n✓\\n✓\\nLATS [1364]\\n✓\\n✓\\nTable 7: Tool-augmented language model architectures: Comparison of multiple methods across 8 tool\\ncategories including search, computation, knowledge bases, APIs, multimodal, language tools, interactive\\nenvironments, and domain-specific applications.\\n5.3.3. Agent-Environment Interaction\\nReinforcement learning approaches have emerged as superior alternatives to prompting-based methods and\\nsupervised fine-tuning for tool integration, enabling models to autonomously discover optimal tool usage\\nstrategies through exploration and outcome-driven rewards [223]. ReTool exemplifies this advancement\\n40\\n\\n\\n\\n\\n\\n\\n\\n\\n|Col1|Search & Computation & Knowledge Base APIs & Multimodal Language Interactive Domain-Specifci<br>Retrieval Code Execution & QA External Services Tools Processing Environments Tools|\\n|---|---|\\n\\n\\n|Method|Search & Retrieval ✓|Computation & Code Execution|Knowledge Base & QA ✓|Tool Categ APIs & External Services|gories Multimodal Tools|Language Processing|Interactive Environments ✓|Domain-Specific Tools|\\n|---|---|---|---|---|---|---|---|---|\\n|ReAct [1247]|✓<br>||✓<br>||||✓||\\n|Toolformer [931]|✓<br>|✓<br>|✓<br>|||✓||✓|\\n|ToolkenGPT [378]|✓<br>|✓<br>|✓<br>|✓<br>|||✓<br>||\\n|ToolLLM [867]|✓|✓<br>|✓|✓|✓|✓|✓|✓|\\n|ToRA [341]||✓<br>|||||||\\n|PAL [303]||✓|||||||\\n|HuggingGPT [945]||||✓|✓<br>||||\\n|GPT4Tools [1225]|||||✓||||\\n|CRITIC [340]|✓|✓<br>|✓||||||\\n|Chain of Code [595]||✓<br>|||||||\\n|TRICE [863]|✓<br>|✓<br>|✓<br>|||✓<br>|||\\n|TP-LLaMA [149]|✓<br>|✓<br>|✓<br>|✓<br>|✓<br>|✓<br>|✓<br>|✓<br>|\\n|AlignToolLLaMA [161]|✓|✓<br>|✓|✓|✓|✓|✓|✓|\\n|ReTool [270]||✓<br>|||||||\\n|Tool-Star [221]|✓|✓<br>|||||||\\n|ARTIST [965]||✓|||||||\\n|Ego-R1 [1038]|||||✓<br>||||\\n|VTool-R1 [1155]|||||✓||||\\n|KG-Agent [487]|||✓|||||✓<br>|\\n|CACTUS [755]||||||||✓|\\n|MuMath-Code [1265]||✓<br>|||||||\\n|ToRL [621]||✓<br>|||||||\\n|MetaTool [452]|✓|✓|✓|✓<br>|||||\\n|ToolEyes [1253]||||✓||||✓<br>|\\n|Graph-CoT [495]|||✓<br>|||||✓|\\n|ToolRL [858]|✓<br>|✓|✓|✓|||||\\n|LATS [1364]|✓|||||||✓|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 40}, page_content='by focusing on code interpreter optimization for mathematical reasoning, achieving 67.0% accuracy on\\nAIME2024 benchmarks after only 400 training steps, substantially outperforming text-based RL baselines\\nreaching 40.0% accuracy with extensive training [270]. This demonstrates that explicitly modeling tool use\\nwithin decision processes enhances both reasoning capabilities and training efficiency.\\nSearch-augmented reasoning systems represent innovative integrations of information retrieval directly\\ninto reasoning processes through specialized learning environments. The Search-R1 framework trains models\\nto make dynamic decisions about when to search and what queries to generate during multi-step reasoning\\ntasks, unlike traditional retrieval-augmented generation systems [976]. The architecture employs specialized\\ntoken systems structuring reasoning and search processes, where models learn to generate reasoning steps\\ninterspersed with explicit search actions triggered through tokens that encapsulate generated queries [648].\\nMulti-turn and customizable tool invocation frameworks address the complexity of coordinating multiple\\nheterogeneous tools during reasoning processes. Recent developments include frameworks like VisTA that\\nuse reinforcement learning to enable visual agents to dynamically explore, select, and combine tools from\\ndiverse libraries based on empirical performance [454]. ReVeal demonstrates self-evolving code agents via\\niterative generation-verification processes [506]. In multimodal domains, systems like VideoAgent employ\\nvision-language foundation models as tools for translating and retrieving visual information, achieving\\nimpressive performance on video understanding benchmarks [1108, 254].\\nEvaluation and Applications\\nComprehensive evaluation of tool-integrated reasoning systems requires\\nspecialized benchmarks that measure tool-integrated capabilities rather than general model performance.\\nMCP-RADAR provides a standardized evaluation framework employing strictly objective metrics derived\\nfrom quantifiable performance data, with extensible design spanning software engineering, mathematical\\nreasoning, and general problem-solving domains [310]. The framework visualizes performance through\\nradar charts highlighting model strengths and weaknesses across multiple dimensions, enabling systematic\\ncomparison of tool-integrated language models regardless of implementation mechanisms.\\nReal-world evaluation approaches reveal significant performance gaps between current systems and\\nhuman-level capabilities, providing crucial insights into practical limitations and optimization opportunities.\\nThe General Tool Agents (GTA) benchmark addresses limitations in existing evaluations by featuring real\\nhuman-written queries with implicit tool-use requirements, evaluation platforms with deployed tools across\\nperception, operation, logic, and creativity categories, and authentic multimodal inputs including images and\\ncode snippets [1090]. Results demonstrate substantial challenges for current LLMs, with GPT-4 completing\\nless than 50\\nFunction calling enabled sophisticated multi-agent systems where multiple LLM agents collaborate through\\ncoordinated tool use and task decomposition, with MAS leveraging collective intelligence through parallel\\nprocessing, information sharing, and adaptive role assignment, while LLM integration enhanced capabilities\\nin planning, specialization, and task decomposition through frameworks like DyLAN, MAD, and MetaGPT\\n[239, 903, 344, 140, 625]. Advanced multi-agent function calling employs sophisticated orchestration\\nmechanisms decomposing complex tasks into manageable subtasks, with fundamental approaches involving\\nsplitting reward machines into parallel execution units, each agent maintaining individual reward machines,\\nlocal state spaces, and propositions, while adaptive orchestration enables dynamic agent selection based on\\ncontext, responses, and status reports [39, 1048, 691, 117].\\n41'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 41}, page_content='5.4. Multi-Agent Systems\\nMulti-Agent Systems represent the pinnacle of collaborative intelligence, enabling multiple autonomous\\nagents to coordinate and communicate for solving complex problems beyond individual agent capabilities.\\nThis implementation focuses on sophisticated communication protocols, orchestration mechanisms, and\\ncoordination strategies that enable seamless collaboration across diverse agent architectures.\\nMulti-Agent Systems\\nOrchestration Mechanisms\\nCommunication Protocols \\nCoordination Strategies\\nConnection and \\nCollaboration\\nFigure 7: Multi-Agent Systems Framework: Overview of communication protocols, orchestration mechanisms,\\nand coordination strategies for collaborative AI agent systems.\\n5.4.1. Communication Protocols\\nAgent communication systems originate from the Knowledge Sharing Effort of the early 1990s, establishing\\nfoundational principles for autonomous entity coordination through standardized languages addressing\\ninteroperability challenges [369, 93]. KQML emerged as the pioneering Agent Communication Language,\\nintroducing multi-layered architecture separating content, message, and communication layers while employ-\\ning speech act theory [369, 82, 657, 280]. FIPA ACL enhanced this foundation through semantic frameworks\\nbased on modal logic, feasibility preconditions, and rational effects [1146, 369, 82].\\nInteroperability requirements necessitate semantic-level communication capabilities enabling cross-\\nplatform agent understanding without extensive pre-communication setup, addressing increasing hetero-\\ngeneity through ontology-based protocol formalization and Semantic Web technologies, while incorporating\\nsecurity mechanisms against communication vulnerabilities [480, 66, 443, 481, 786, 1055].\\nContemporary Protocol Ecosystem\\nContemporary standardized protocols address fragmentation chal-\\nlenges hindering LLM agent collaboration [1235, 1128, 408]. MCP functions as “USB-C for AI,” standardizing\\nagent-environment interactions through JSON-RPC client-server interfaces, enabling hundreds of servers\\nacross diverse domains while introducing security vulnerabilities [926, 246, 616, 266, 15, 257, 922, 1094,\\n370, 1185, 297, 1008, 713, 269].\\nA2A standardizes peer-to-peer communication through capability-based Agent Cards enabling task\\ndelegation and secure collaboration via JSON-based lifecycle models [616, 246, 926]. ACP provides general-\\npurpose RESTful HTTP communication supporting multipart messages and synchronous/asynchronous\\ninteractions with discovery, delegation, and orchestration features [277, 246].\\nANP extends interoperability to open internet through W3C decentralized identifiers and JSON-LD graphs,\\nwith emerging protocols AGNTCY and Agora diversifying standardization ecosystems [246, 679, 1128].\\n42'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 42}, page_content='Progressive layering strategy: MCP provides tool access, ACP enables message exchange, A2A supports peer\\ninteraction, ANP extends network interoperability [1007, 926].\\nLLM-Enhanced Communication Frameworks\\nLLMs transform agent communication through sophisticated\\nnatural language processing enabling unprecedented context sensitivity across academic and industrial\\napplications spanning social science, natural science, and engineering domains [486, 684, 498, 1091, 1170,\\n1127, 896, 1052, 871]. Enhanced systems demonstrate cognitive synergy through specialized knowledge\\nbases, planning, memory, and introspection capabilities, supporting cooperative, debate-oriented, and\\ncompetitive communication paradigms [486, 356].\\nCommunication structures encompass layered hierarchical organization, decentralized peer-to-peer\\nnetworks, centralized coordination, and shared message pool architectures, complemented by sequential\\nexchanges, universal language interfaces, and message-passing strategies [356, 1240, 1210, 167, 396, 485,\\n537, 659, 793, 941].\\nFramework implementations support comprehensive ecosystems: AutoGen enables dynamic response\\ngeneration, MetaGPT provides shared message pools, CAMEL offers integrated orchestration, CrewAI\\nfacilitates adaptation, with reinforcement learning integration enhancing reward redesign, action selection,\\nand policy interpretation [184, 38, 119, 996, 224, 865, 927, 950, 1264]. Human-agent communication\\nintroduces complex interaction landscapes through flexible participation and cognitive diversity, with agents\\ninferring communicator properties and mirroring human communicative intentions [1399, 34, 669].\\n5.4.2. Orchestration Mechanisms\\nOrchestration mechanisms constitute the critical coordination infrastructure for multi-agent systems, manag-\\ning agent selection, context distribution, and interaction flow control [894], enabling effective cooperation\\namong human and non-human actors through user input processing, contextual distribution, and optimal\\nagent selection based on capability assessment and response evaluation [53], while managing message flow,\\nensuring task progression, and addressing task deviations [171]. Advanced orchestration frameworks incor-\\nporate intent recognition, contextual memory maintenance, and task dispatching components for intelligent\\ncoordination across domain-specific agents, with the Swarm Agent framework utilizing real-time outputs\\nto direct tool invocations while addressing limitations in static tool registries and bespoke communication\\nframeworks [808, 263, 246].\\nContemporary orchestration strategies exhibit distinct operational paradigms: a priori orchestration\\ndetermines agent selection through pre-execution analysis of user input and agent capabilities, while\\nposterior orchestration distributes inputs to multiple agents simultaneously, utilizing confidence metrics\\nand response quality assessment as demonstrated by the 3S orchestrator framework [893]; function-based\\norchestration emphasizes agent selection from available pools, contextual information management, and\\nconversation flow control [54]; component-based orchestration employs dynamic planning processes where\\norchestrators arrange components in logical sequences based on user instructions, utilizing LLMs as component\\norchestration tools to generate workflows with embedded orchestration logic [675].\\nEmergent orchestration paradigms include puppeteer-style orchestration featuring centralized orchestra-\\ntors that dynamically direct agents in response to evolving task states through reinforcement learning-based\\nadaptive sequencing and prioritization, and serialized orchestration addressing collaboration topology com-\\nplexity by unfolding collaboration graphs into reasoning sequences guided by topological traversal, enabling\\norchestrators to select single agents at each step based on global system state and task specifications [194].\\n43'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 43}, page_content='Context Management and Environmental Adaptation\\nContext serves as the foundational element guiding\\nagent actions and interactions within orchestrated systems, supporting operational mode diversity while\\nmaintaining application individuality and task execution sequencing through global state maintenance that\\nenables orchestration systems to track task execution progress across distributed nodes, providing agents\\nwith contextual awareness necessary for effective subtask performance within broader workflow contexts\\n[26]. Session-based context refinement defines collaborative scope boundaries, facilitating event-driven\\norchestration where agents can enter and exit dynamically, create output streams, and contribute to shared\\nsession streams, with configurable sessions enabling agent inclusion based on user input or autonomous\\ndecision-making to create adaptable systems responsive to changing task requirements [513].\\nWell-designed interaction structures and task orchestration mechanisms underscore context’s critical role\\nin scalable multi-agent collaboration. Systems adapt communication patterns and agent roles to contextual\\nrequirements, supporting dynamic collaboration tailored to specific task demands through complex task\\ndecomposition and suitable agent assignment for subtask execution [1128]. This contextual adaptation\\nencompasses both organizational and operational dimensions, enabling systems to maintain coherence while\\naccommodating environmental variability and evolving user requirements.\\n5.4.3. Coordination Strategies\\nMulti-agent orchestration encounters significant challenges in maintaining transactional integrity across\\ncomplex workflows, with contemporary frameworks including LangGraph, AutoGen, and CAMEL demonstrat-\\ning insufficient transaction support: LangGraph provides basic state management while lacking atomicity\\nguarantees and systematic compensation mechanisms, AutoGen prioritizes flexible agent interactions without\\nadequate compensatory action management potentially resulting in inconsistent system states following par-\\ntial failures, and validation limitations emerge as many frameworks rely exclusively on large language models’\\ninherent self-validation capabilities without implementing independent validation procedures, exposing\\nsystems to reasoning errors, hallucinations, and inter-agent inconsistencies [128].\\nContext handling failures compound these challenges as agents struggle with long-term context main-\\ntenance encompassing both episodic and semantic information [210, 1113], while central orchestrator\\ntopologies introduce non-deterministic, runtime-dependent execution paths that enhance adaptability while\\ncomplicating anomaly detection, requiring dynamic graph reconstruction rather than simple path matching\\n[390], and environmental misconfigurations and LLM hallucinations can distract agentic systems, with poor\\nrecovery leading to goal deviation that becomes amplified in multi-agent setups with distributed subtasks\\n[210, 1091].\\nInter-agent dependency opacity presents additional concerns as agents may operate on inconsistent\\nassumptions or conflicting data without explicit constraints or validation layers, necessitating anomaly\\ndetection incorporating reasoning over orchestration intent and planning coherence [390], while addressing\\nthese challenges requires comprehensive solutions such as the SagaLLM framework providing transaction\\nsupport, independent validation procedures, and robust context preservation mechanisms [128], and\\napproaches like CodeAct integrating Python interpreters with LLM agents to enable code action execution\\nand dynamic revision capabilities through multi-turn interactions [1113].\\nApplications and Performance Implications\\nAgent and context orchestration demonstrates practical\\nutility across diverse application domains: healthcare applications employ context-switching mechanisms\\nwithin specialized agent-based architectures performing information retrieval, question answering, and\\ndecision support, utilizing supervisory agents to interpret input features and assign subtasks to specialized\\n44'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 44}, page_content='agents based on clinical query type, user background, and data modality requirements [613, 754, 1051];\\nnetwork management applications leverage context-aware orchestration to address complexity challenges by\\nequipping Points of Access with agents dedicated to unique contexts, enabling efficient network dynamics\\nmanagement through context-specific action sets including available service instances and network paths\\n[958].\\nBusiness process management and simulation represent significant application areas through platforms\\nlike AgentSimulator, enabling process behavior discovery and simulation in orchestrated and autonomous\\nsettings where orchestrated behavior follows global control-flow patterns with activity selection dependent\\non previous activities and agent assignment based on capabilities and availability, while autonomous behavior\\noperates through local control-flow and handover patterns acknowledging agent autonomy in collaborative\\nwork [543].\\nPerformance implications indicate that well-designed orchestration improves system effectiveness by\\nleveraging distinct agent capabilities, with research demonstrating that human users frequently struggle with\\neffective agent selection from available sets while automated orchestration enhances overall performance\\n[72], motivating frameworks that learn agent capabilities online and orchestrate multiple agents under\\nreal-world constraints including cost, capability requirements, and operational limitations, with autonomy\\nlevels varying across implementations where some systems exhibit pronounced autonomy within designated\\nphases, demonstrating adaptability in action management corresponding to task specificity and reaching\\nLevel 2 autonomy through contextual resource utilization [460].\\n6. Evaluation\\nThe evaluation of context-engineered systems presents unprecedented challenges that transcend traditional\\nlanguage model assessment paradigms. These systems exhibit complex, multi-component architectures\\nwith dynamic, context-dependent behaviors requiring comprehensive evaluation frameworks that assess\\ncomponent-level diagnostics, task-based performance, and overall system robustness [835, 1132].\\nThe heterogeneous nature of context engineering components—spanning retrieval mechanisms, memory\\nsystems, reasoning chains, and multi-agent coordination—demands evaluation methodologies that can\\ncapture both individual component effectiveness and emergent system-level behaviors [310, 931].\\n6.1. Evaluation Frameworks and Methodologies\\nThis subsection presents comprehensive approaches for evaluating both individual components and integrated\\nsystems in context engineering.\\n6.1.1. Component-Level Assessment\\nIntrinsic evaluation focuses on the performance of individual components in isolation, providing foundational\\ninsights into system capabilities and failure modes.\\nFor prompt engineering components, evaluation encompasses prompt effectiveness measurement\\nthrough semantic similarity metrics, response quality assessment, and robustness testing across diverse input\\nvariations. Current approaches reveal brittleness and robustness challenges in prompt design, necessitating\\nmore sophisticated evaluation frameworks that can assess contextual calibration and adaptive prompt\\noptimization [1132, 663].\\n45'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 45}, page_content='Long context processing evaluation requires specialized metrics addressing information retention,\\npositional bias, and reasoning coherence across extended sequences. The “needle in a haystack” evalua-\\ntion paradigm tests models’ ability to retrieve specific information embedded within long contexts, while\\nmulti-document reasoning tasks assess synthesis capabilities across multiple information sources. Position in-\\nterpolation techniques and ultra-long sequence processing methods face significant computational challenges\\nthat limit practical evaluation scenarios [731, 295].\\nSelf-contextualization mechanisms undergo evaluation through meta-learning assessments, adaptation\\nspeed measurements, and consistency analysis across multiple iterations. Self-refinement frameworks\\nincluding Self-Refine, Reflexion, and N-CRITICS demonstrate substantial performance improvements, with\\nGPT-4 achieving approximately 20% improvement through iterative self-refinement processes [735, 956, 789].\\nMulti-dimensional feedback mechanisms and ensemble-based evaluation approaches provide comprehensive\\nassessment of autonomous evolution capabilities [577, 704].\\nStructured and relational data integration evaluation examines accuracy in knowledge graph traver-\\nsal, table comprehension, and database query generation. However, current evaluation frameworks face\\nsignificant limitations in assessing structural reasoning capabilities, with high-quality structured training\\ndata development presenting ongoing challenges. LSTM-based models demonstrate increased errors when\\nsequential and structural information conflict, highlighting the need for more sophisticated benchmarks\\ntesting structural understanding [763, 668, 163].\\n6.1.2. System-Level Integration Assessment\\nExtrinsic evaluation measures end-to-end performance on downstream tasks, providing holistic assessments\\nof system utility through comprehensive benchmarks spanning question answering, reasoning, and real-world\\napplications.\\nSystem-level evaluation must capture emergent behaviors arising from component interactions, including\\nsynergistic effects where combined components exceed individual performance and potential interference\\npatterns where component integration degrades overall effectiveness [835, 1132].\\nRetrieval-Augmented Generation evaluation encompasses both retrieval quality and generation effec-\\ntiveness through comprehensive metrics addressing precision, recall, relevance, and factual accuracy. Agentic\\nRAG systems introduce additional complexity requiring evaluation of task decomposition accuracy, multi-plan\\nselection effectiveness, and memory-augmented planning capabilities. Self-reflection mechanisms demon-\\nstrate iterative improvement through feedback loops, with MemoryBank implementations incorporating\\nEbbinghaus Forgetting Curve principles for enhanced memory evaluation [438, 162, 1362, 1183, 41].\\nMemory systems evaluation encounters substantial difficulties stemming from the absence of standard-\\nized assessment frameworks and the inherently stateless characteristics of contemporary LLMs. LongMemEval\\noffers 500 carefully curated questions that evaluate fundamental capabilities encompassing information\\nextraction, temporal reasoning, multi-session reasoning, and knowledge updates. Commercial AI assistants\\nexhibit 30% accuracy degradation throughout extended interactions, underscoring significant deficiencies in\\nmemory persistence and retrieval effectiveness [1330, 1171, 457, 841, 386]. Dedicated benchmarks such as\\nNarrativeQA, QMSum, QuALITY, and MEMENTO tackle episodic memory evaluation challenges [550, 566].\\nTool-integrated reasoning systems require comprehensive evaluation covering the entire interaction\\ntrajectory, including tool selection accuracy, parameter extraction precision, execution success rates, and error\\nrecovery capabilities. The MCP-RADAR framework provides standardized evaluation employing objective\\nmetrics for software engineering and mathematical reasoning domains. Real-world evaluation reveals\\n46'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 46}, page_content='significant performance gaps, with GPT-4 completing less than 50% of tasks in the GTA benchmark, compared\\nto human performance of 92% [310, 1090, 126, 931]. Advanced benchmarks including BFCL (2,000 testing\\ncases), T-Eval (553 tool-use cases), API-Bank (73 APIs, 314 dialogues), and ToolHop (995 queries, 3,912\\ntools) address multi-turn interactions and nested tool calling scenarios [259, 359, 373, 1255, 157, 829].\\nMulti-agent systems evaluation captures communication effectiveness, coordination efficiency, and\\ncollective outcome quality through specialized metrics addressing protocol adherence, task decomposition ac-\\ncuracy, and emergent collaborative behaviors. Contemporary orchestration frameworks including LangGraph,\\nAutoGen, and CAMEL demonstrate insufficient transaction support, with validation limitations emerging\\nas systems rely exclusively on LLM self-validation capabilities without independent validation procedures.\\nContext handling failures compound challenges as agents struggle with long-term context maintenance\\nencompassing both episodic and semantic information [128, 390, 893].\\n6.2. Benchmark Datasets and Evaluation Paradigms\\nThis subsection reviews specialized benchmarks and evaluation paradigms designed for assessing context\\nengineering system performance.\\n6.2.1. Foundational Component Benchmarks\\nLong context processing evaluation employs specialized benchmark suites designed to test information reten-\\ntion, reasoning, and synthesis across extended sequences. Current benchmarks face significant computational\\ncomplexity challenges, with O(n2) scaling limitations in attention mechanisms creating substantial memory\\nconstraints for ultra-long sequences. Position interpolation and extension techniques require sophisticated\\nevaluation frameworks that can assess both computational efficiency and reasoning quality across varying\\nsequence lengths [731, 295, 1227].\\nAdvanced architectures including LongMamba and specialized position encoding methods demonstrate\\npromising directions for long context processing, though evaluation reveals persistent challenges in main-\\ntaining coherence across extended sequences. The development of sliding attention mechanisms and\\nmemory-efficient implementations requires comprehensive benchmarks that can assess both computational\\ntractability and task performance [1258, 347].\\nStructured and relational data integration benchmarks encompass diverse knowledge representation\\nformats and reasoning patterns. However, current evaluation frameworks face limitations in assessing\\nstructural reasoning capabilities, with the development of high-quality structured training data presenting\\nongoing challenges. Evaluation must address the fundamental tension between sequential and structural\\ninformation processing, particularly in scenarios where these information types conflict [763, 668, 163].\\n6.2.2. System Implementation Benchmarks\\nRetrieval-Augmented Generation evaluation leverages comprehensive benchmark suites addressing diverse\\nretrieval and generation challenges. Modular RAG architectures demonstrate enhanced flexibility through\\nspecialized modules for retrieval, augmentation, and generation, enabling fine-grained evaluation of individual\\ncomponents and their interactions. Graph-enhanced RAG systems incorporating GraphRAG and LightRAG\\ndemonstrate improved performance in complex reasoning scenarios, though evaluation frameworks must\\naddress the additional complexity of graph traversal and multi-hop reasoning assessment [312, 965, 360].\\nAgentic RAG systems introduce sophisticated planning and reflection mechanisms requiring evaluation\\n47'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 47}, page_content='of task decomposition accuracy, multi-plan selection effectiveness, and iterative refinement capabilities.\\nReal-time and streaming RAG applications present unique evaluation challenges in assessing both latency\\nand accuracy under dynamic information conditions [438, 162, 1183].\\nTool-integrated reasoning system evaluation employs comprehensive benchmarks spanning diverse tool\\nusage scenarios and complexity levels. The Berkeley Function Calling Leaderboard (BFCL) provides 2,000\\ntesting cases with step-by-step and end-to-end assessments measuring call accuracy, pass rates, and win rates\\nacross increasingly complex scenarios. T-Eval contributes 553 tool-use cases testing multi-turn interactions\\nand nested tool calling capabilities [259, 1380, 829]. Advanced benchmarks including StableToolBench\\naddress API instability challenges, while NesTools evaluates nested tool scenarios and ToolHop assesses\\nmulti-hop tool usage across 995 queries and 3,912 tools [359, 373, 1255].\\nWeb agent evaluation frameworks including WebArena and Mind2Web provide comprehensive assessment\\nacross thousands of tasks spanning 137 websites, revealing significant performance gaps in current LLM\\ncapabilities for complex web interactions. VideoWebArena extends evaluation to multimodal agents, while\\nDeep Research Bench and DeepShop address specialized evaluation for research and shopping agents\\nrespectively [1368, 202, 87, 476].\\nMulti-agent system evaluation employs specialized frameworks addressing coordination, communication,\\nand collective intelligence. However, current frameworks face significant challenges in transactional integrity\\nacross complex workflows, with many systems lacking adequate compensation mechanisms for partial\\nfailures. Orchestration evaluation must address context management, coordination strategy effectiveness,\\nand the ability to maintain system coherence under varying operational conditions [128, 893].\\nRelease Date\\nOpen Source\\nMethod / Model\\nSuccess Rate (%)\\nSource\\n2025-02\\n×\\nIBM CUGA\\n61.7\\n[747]\\n2025-01\\n×\\nOpenAI Operator\\n58.1\\n[807]\\n2024-08\\n×\\nJace.AI\\n57.1\\n[470]\\n2024-12\\n×\\nScribeAgent + GPT-4o\\n53.0\\n[942]\\n2025-01\\n✓\\nAgentSymbiotic\\n52.1\\n[1314]\\n2025-01\\n✓\\nLearn-by-Interact\\n48.0\\n[990]\\n2024-10\\n✓\\nAgentOccam-Judge\\n45.7\\n[1222]\\n2024-08\\n×\\nWebPilot\\n37.2\\n[1322]\\n2024-10\\n✓\\nGUI-API Hybrid Agent\\n35.8\\n[980]\\n2024-09\\n✓\\nAgent Workflow Memory\\n35.5\\n[1135]\\n2024-04\\n✓\\nSteP\\n33.5\\n[971]\\n2025-06\\n✓\\nTTI\\n26.1\\n[943]\\n2024-04\\n✓\\nBrowserGym + GPT-4\\n23.5\\n[234]\\nTable 8: WebArena [1368] Leaderboard: Top performing models with their success rates and availability\\nstatus.\\n6.3. Evaluation Challenges and Emerging Paradigms\\nThis subsection identifies current limitations in evaluation methodologies and explores emerging approaches\\nfor more effective assessment.\\n48\\n\\n\\n\\n\\n\\n\\n\\n\\n|Release Date|Open Source|Method / Model|Success Rate (%)|Source|\\n|---|---|---|---|---|\\n\\n\\n|Release Date|Open Source|Method / Model|Success Rate (%)|Source|\\n|---|---|---|---|---|\\n|2025-02|_×_|IBM CUGA|61.7|[747]|\\n|2025-01|_×_|OpenAI Operator|58.1|[807]|\\n|2024-08|_×_|Jace.AI|57.1|[470]|\\n|2024-12|_×_<br>|ScribeAgent + GPT-4o|53.0|[942]|\\n|2025-01|✓<br>|AgentSymbiotic|52.1|[1314]|\\n|2025-01|✓<br>|Learn-by-Interact|48.0|[990]|\\n|2024-10|✓|AgentOccam-Judge|45.7|[1222]|\\n|2024-08|_×_<br>|WebPilot|37.2|[1322]|\\n|2024-10|✓<br>|GUI-API Hybrid Agent|35.8|[980]|\\n|2024-09|✓<br>|Agent Workfow Memory|35.5|[1135]|\\n|2024-04|✓<br>|SteP|33.5|[971]|\\n|2025-06|✓<br>|TTI|26.1|[943]|\\n|2024-04|✓|BrowserGym + GPT-4|23.5|[234]|'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 48}, page_content='6.3.1. Methodological Limitations and Biases\\nTraditional evaluation metrics prove fundamentally inadequate for capturing the nuanced, dynamic behaviors\\nexhibited by context-engineered systems. Static metrics like BLEU, ROUGE, and perplexity, originally\\ndesigned for simpler text generation tasks, fail to assess complex reasoning chains, multi-step interactions,\\nand emergent system behaviors. The inherent complexity and interdependencies of multi-component systems\\ncreate attribution challenges where isolating failures and identifying root causes becomes computationally\\nand methodologically intractable. Future metrics must evolve to capture not just task success, but the\\nquality and robustness of the underlying reasoning process, especially in scenarios requiring compositional\\ngeneralization and creative problem-solving [835, 1132].\\nMemory system evaluation faces particular challenges due to the lack of standardized benchmarks and\\nthe stateless nature of current LLMs. Automated memory testing frameworks must address the isolation\\nproblem where different memory testing stages cannot be effectively separated, leading to unreliable\\nassessment results. Commercial AI assistants demonstrate significant performance degradation during\\nsustained interactions, with accuracy drops of up to 30% highlighting critical gaps in current evaluation\\nmethodologies and pointing to the need for longitudinal evaluation frameworks that track memory fidelity\\nover time [1330, 1171, 457].\\nTool-integrated reasoning system evaluation reveals substantial performance gaps between current\\nsystems and human-level capabilities. The GAIA benchmark demonstrates that while humans achieve 92%\\naccuracy on general assistant tasks, advanced models like GPT-4 achieve only 15% accuracy, indicating\\nfundamental limitations in current evaluation frameworks and system capabilities [772, 1090, 126]. Evalua-\\ntion frameworks must address the complexity of multi-tool coordination, error recovery, and adaptive tool\\nselection across diverse operational contexts [310, 931].\\n6.3.2. Emerging Evaluation Paradigms\\nSelf-refinement evaluation paradigms leverage iterative improvement mechanisms to assess system capa-\\nbilities across multiple refinement cycles. Frameworks including Self-Refine, Reflexion, and N-CRITICS\\ndemonstrate substantial performance improvements through multi-dimensional feedback and ensemble-\\nbased evaluation approaches. GPT-4 achieves approximately 20% improvement through self-refinement\\nprocesses, highlighting the importance of evaluating systems across multiple iteration cycles rather than\\nsingle-shot assessments. However, a key future challenge lies in evaluating the meta-learning capability\\nitself—not just whether the system improves, but how efficiently and robustly it learns to refine its strategies\\nover time [735, 956, 789, 577].\\nMulti-aspect feedback evaluation incorporates diverse feedback dimensions including correctness, rel-\\nevance, clarity, and robustness, providing comprehensive assessment of system outputs. Self-rewarding\\nmechanisms enable autonomous evolution and meta-learning assessment, allowing systems to develop\\nincreasingly sophisticated evaluation criteria through iterative refinement [704].\\nCriticism-guided evaluation employs specialized critic models to provide detailed feedback on system\\noutputs, enabling fine-grained assessment of reasoning quality, factual accuracy, and logical consistency.\\nThese approaches address the limitations of traditional metrics by providing contextual, content-aware\\nevaluation that can adapt to diverse task requirements and output formats [789, 577].\\nOrchestration evaluation frameworks address the unique challenges of multi-agent coordination by\\nincorporating transactional integrity assessment, context management evaluation, and coordination strategy\\neffectiveness measurement. Advanced frameworks including SagaLLM provide transaction support and\\n49'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 49}, page_content='independent validation procedures to address the limitations of systems that rely exclusively on LLM self-\\nvalidation capabilities [128, 390].\\n6.3.3. Safety and Robustness Assessment\\nSafety-oriented evaluation incorporates comprehensive robustness testing, adversarial attack resistance, and\\nalignment assessment to ensure responsible development of context-engineered systems. Particular attention\\nmust be paid to the evaluation of agentic systems that can operate autonomously across extended periods, as\\nthese systems present unique safety challenges that traditional evaluation frameworks cannot adequately\\naddress [965, 360].\\nRobustness evaluation must assess system performance under distribution shifts, input perturbations, and\\nadversarial conditions through comprehensive stress testing protocols. Multi-agent systems face additional\\nchallenges in coordination failure scenarios, where partial system failures can cascade through the entire agent\\nnetwork. Evaluation frameworks must address graceful degradation strategies, error recovery protocols,\\nand the ability to maintain system functionality under adverse conditions. Beyond predefined failure\\nmodes, future evaluation must grapple with assessing resilience to “unknown unknowns”—emergent and\\nunpredictable failure cascades in highly complex, autonomous multi-agent systems [128, 390].\\nAlignment evaluation measures system adherence to intended behaviors, value consistency, and beneficial\\noutcome optimization through specialized assessment frameworks. Context engineering systems present\\nunique alignment challenges due to their dynamic adaptation capabilities and complex interaction patterns\\nacross multiple components. Long-term evaluation must assess whether systems maintain beneficial behaviors\\nas they adapt and evolve through extended operational periods [893].\\nLooking ahead, the evaluation of context-engineered systems requires a paradigm shift from static\\nbenchmarks to dynamic, holistic assessments. Future frameworks must move beyond measuring task success\\nto evaluating compositional generalization for novel problems and tracking long-term autonomy in interactive\\nenvironments. The development of ’living’ benchmarks that co-evolve with AI capabilities, alongside the\\nintegration of socio-technical and economic metrics, will be critical for ensuring these advanced systems\\nare not only powerful but also reliable, efficient, and aligned with human values in real-world applications\\n[310, 1368, 1330].\\nThe evaluation landscape for context-engineered systems continues evolving rapidly as new architectures,\\ncapabilities, and applications emerge. Future evaluation paradigms must address increasing system complexity\\nwhile providing reliable, comprehensive, and actionable insights for system improvement and deployment\\ndecisions. The integration of multiple evaluation approaches—from component-level assessment to system-\\nwide robustness testing—represents a critical research priority for ensuring the reliable deployment of\\ncontext-engineered systems in real-world applications [835, 1132].\\n7. Future Directions and Open Challenges\\nContext Engineering stands at a critical inflection point where foundational advances converge with emerging\\napplication demands, creating unprecedented opportunities for innovation while revealing fundamental\\nchallenges that require sustained research efforts across multiple dimensions [835, 1132].\\nAs the field transitions from isolated component development toward integrated system architectures,\\nthe complexity of research challenges grows exponentially, demanding interdisciplinary approaches that\\nbridge theoretical computer science, practical system engineering, and domain-specific expertise [310, 931].\\n50'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 50}, page_content='This section systematically examines key research directions and open challenges that will define the\\nevolution of Context Engineering over the coming decade.\\n7.1. Foundational Research Challenges\\nThis subsection examines core theoretical and computational challenges that must be addressed to advance\\ncontext engineering systems beyond current limitations.\\n7.1.1. Theoretical Foundations and Unified Frameworks\\nContext Engineering currently operates without unified theoretical foundations that connect disparate\\ntechniques and provide principled design guidelines, representing a critical research gap that limits systematic\\nprogress and optimal system development.\\nThe absence of mathematical frameworks characterizing context engineering capabilities, limitations,\\nand optimal design principles across different architectural configurations impedes both fundamental\\nunderstanding and practical optimization [1132, 663, 835, 310].\\nInformation-theoretic analysis of context engineering systems requires comprehensive investigation into\\noptimal context allocation strategies, information redundancy quantification, and fundamental compression\\nlimits within context windows. Current approaches lack principled methods for determining optimal context\\ncomposition, leading to suboptimal resource utilization and performance degradation. Research must\\nestablish mathematical bounds on context efficiency, develop optimization algorithms for context selection,\\nand create theoretical frameworks for predicting system behavior across varying context configurations\\n[731, 295].\\nCompositional understanding of context engineering systems demands formal models describing how\\nindividual components interact, interfere, and synergize within integrated architectures. The emergence of\\ncomplex behaviors from component interactions requires systematic investigation through both empirical\\nstudies and theoretical modeling approaches. Multi-agent orchestration presents particular challenges in\\ndeveloping mathematical frameworks for predicting coordination effectiveness and emergent collaborative\\nbehaviors [128, 893].\\n7.1.2. Scaling Laws and Computational Efficiency\\nThe fundamental asymmetry between LLMs’ remarkable comprehension capabilities and their pronounced\\ngeneration limitations represents one of the most critical challenges in Context Engineering research.\\nThis comprehension-generation gap manifests across multiple dimensions including long-form output\\ncoherence, factual consistency maintenance, and planning sophistication, requiring investigation into whether\\nlimitations stem from architectural constraints, training methodologies, or fundamental computational\\nboundaries [835, 1132].\\nLong-form generation capabilities demand systematic investigation into planning mechanisms that can\\nmaintain coherence across thousands of tokens while preserving factual accuracy and logical consistency.\\nCurrent systems exhibit significant performance degradation in extended generation tasks, highlighting the\\nneed for architectural innovations beyond traditional transformer paradigms. State space models including\\nMamba demonstrate potential for more efficient long sequence processing through linear scaling properties,\\nthough current implementations require substantial development to match transformer performance across\\ndiverse tasks [731, 1258, 347, 216].\\n51'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 51}, page_content='Context scaling efficiency faces fundamental computational challenges, with current attention mecha-\\nnisms scaling quadratically (O(n2)) with sequence length, creating prohibitive memory and computational\\nrequirements for ultra-long sequences. Sliding attention mechanisms and memory-efficient implementations\\nrepresent promising directions, though significant research is needed to address both computational tractabil-\\nity and reasoning quality preservation [295, 1227, 347]. Position interpolation and extension techniques\\nrequire advancement to handle sequences exceeding current architectural limitations while maintaining\\npositional understanding and coherence.\\n7.1.3. Multi-Modal Integration and Representation\\nThe integration of diverse modalities within context engineering systems presents fundamental challenges\\nin representation learning, cross-modal reasoning, and unified architectural design. Current approaches\\ntypically employ modality-specific encoders with limited cross-modal interaction, failing to capture the rich\\ninterdependencies that characterize sophisticated multi-modal understanding. VideoWebArena demonstrates\\nthe complexity of multimodal agent evaluation, revealing substantial performance gaps in current systems\\nwhen processing video, audio, and text simultaneously [476].\\nBeyond these sensory modalities, context engineering must also handle more abstract forms of information\\nsuch as graphs, whose structural semantics are not directly interpretable by language models. Capturing\\nthe high-level meaning encoded in graph structures introduces unique challenges, including aligning graph\\nrepresentations with language model embeddings and expressing graph topology efficiently. Recent efforts\\nlike GraphGPT [1024] and GraphRAG [244] attempt to bridge this gap through cross-modal alignment\\nstrategies, while others explore converting graphs into natural language descriptions to facilitate model under-\\nstanding [262, 319]. Bi et al. [75] further propose a divide-and-conquer approach to encode text-attributed\\nheterogeneous networks, addressing context length limitations and enabling effective link prediction. Graph\\nreasoning thus emerges as a core difficulty in context engineering, requiring models to navigate complex\\nrelational structures beyond raw modalities.\\nTemporal reasoning across multi-modal contexts requires sophisticated architectures capable of tracking\\nobject persistence, causal relationships, and temporal dynamics across extended sequences. Web agent\\nframeworks including WebArena showcase the challenges of maintaining coherent understanding across\\ncomplex multi-step interactions involving diverse modalities and dynamic content. Current systems demon-\\nstrate significant limitations in coordinating multi-modal information processing with action planning and\\nexecution [1368, 202].\\nCross-modal alignment and consistency present ongoing challenges in ensuring that information extracted\\nfrom different modalities remains factually consistent and semantically coherent. Deep Research Bench\\nevaluation reveals that current multi-modal agents struggle with complex research tasks requiring synthesis\\nacross textual, visual, and structured data sources, highlighting the need for more sophisticated alignment\\nmechanisms [87].\\n7.2. Technical Innovation Opportunities\\nThis subsection explores emerging technical approaches and architectural innovations that promise to\\nenhance context engineering capabilities.\\n52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 52}, page_content='7.2.1. Next-Generation Architectures\\nArchitectural innovations beyond traditional transformer paradigms offer promising directions for addressing\\ncurrent limitations in context engineering systems. State space models including LongMamba demonstrate\\npotential for more efficient long sequence processing through linear scaling properties and improved memory\\nutilization, though current implementations require substantial development to match transformer perfor-\\nmance across diverse tasks [1258, 731]. Specialized position encoding methods and parameter-efficient\\narchitectures present opportunities for scaling to ultra-long sequences while maintaining computational\\ntractability [347, 295].\\nMemory-augmented architectures require advancement beyond current external memory mechanisms to\\nenable more sophisticated long-term memory organization, hierarchical memory structures, and adaptive\\nmemory management strategies. MemoryBank implementations incorporating Ebbinghaus Forgetting Curve\\nprinciples demonstrate promising approaches to memory persistence, though significant research is needed to\\naddress the fundamental stateless nature of current LLMs [1362, 1330, 1171, 813, 1202]. The development\\nof episodic memory systems capable of maintaining coherent long-term context across extended interactions\\nrepresents a critical architectural challenge [457, 841, 393].\\nModular and compositional architectures enable flexible system construction through specialized com-\\nponent integration while maintaining overall system coherence. Modular RAG architectures demonstrate\\nenhanced flexibility through specialized modules for retrieval, augmentation, and generation, enabling\\nfine-grained optimization of individual components. Graph-enhanced approaches including GraphRAG and\\nLightRAG showcase the potential for integrating structured knowledge representation with neural processing\\n[312, 965, 360].\\n7.2.2. Advanced Reasoning and Planning\\nContext engineering systems require enhanced reasoning capabilities spanning causal reasoning, counter-\\nfactual thinking, temporal reasoning, and analogical reasoning across extended contexts. Current systems\\ndemonstrate limited capacity for sophisticated reasoning patterns that require integration of multiple evi-\\ndence sources, consideration of alternative scenarios, and maintenance of logical consistency across complex\\ninference chains [1132, 835].\\nMulti-step planning and execution capabilities represent critical advancement areas enabling systems\\nto decompose complex tasks, formulate execution strategies, monitor progress, and adapt plans based on\\nintermediate results. Agentic RAG systems demonstrate sophisticated planning and reflection mechanisms\\nrequiring integration of task decomposition, multi-plan selection, and iterative refinement capabilities.\\nHowever, current implementations face significant challenges in maintaining coherence across extended\\nplanning horizons and adapting to dynamic information conditions [438, 162, 1183].\\nTool-integrated reasoning represents a paradigmatic advancement requiring dynamic interaction with\\nexternal resources during reasoning processes. The GAIA benchmark demonstrates substantial performance\\ngaps, with human achievement of 92% accuracy compared to advanced models achieving only 15%, high-\\nlighting fundamental limitations in current reasoning and planning capabilities [772, 1090, 126]. Advanced\\ntool integration must address autonomous tool selection, parameter extraction, multi-tool coordination, and\\nerror recovery across diverse operational contexts [310, 931].\\n53'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 53}, page_content='7.2.3. Complex Context Organization and Solving Graph Problems\\nGraph reasoning represents a fundamental challenge in context engineering, requiring systems to navigate\\ncomplex structural relationships while maintaining semantic understanding across interconnected elements.\\nRecent advances in graph-language model integration demonstrate multiple paradigms: specialized archi-\\ntectural approaches that incorporate graph-specific components and text-based encoding strategies that\\ntransform graph structures into natural language representations [1085, 1023].\\nArchitectural integration approaches include GraphGPT, which employs dual-stage instruction tuning\\naligning graph structural information with language tokens via self-supervised graph matching [1023, 741].\\nThis framework introduces specialized GraphTokens refined through Graph Instruction Tuning and utilizes\\na lightweight graph-text alignment projector for transitioning between textual and structural processing\\nmodalities [1270, 274]. Building upon instruction-tuning paradigms, GraphWiz extends this approach by\\nincorporating DPO to enhance reasoning reliability, achieving 65% average accuracy across diverse graph tasks\\nand significantly outperforming GPT-4’s 43.8% [145]. Chain-of-thought distillation mechanisms enhance\\nstep-by-step reasoning performance [1138, 1391]. RL presents another promising direction, as demonstrated\\nby G1, which trains LLMs on synthetic graph-theoretic tasks using the Erdős dataset comprising 50 diverse\\ntasks, achieving strong zero-shot generalization with a 3B parameter model outperforming significantly\\nlarger models [357].\\nText-based encoding approaches transform graph structures into natural language descriptions using\\nfew-shot prompting and chain-of-thought reasoning without architectural modifications [262, 192]. These\\nmethods introduce diverse graph description templates contextualizing structural elements through multiple\\nsemantic interpretations [936, 716]. Recent work investigates the impact of graph description ordering\\non LLM performance, revealing that sequential presentation significantly influences model comprehension\\nand reasoning accuracy [319]. Benchmark evaluations have expanded with GraphArena, offering both\\npolynomial-time tasks and NP-complete challenges with a rigorous evaluation framework that classifies\\noutputs as correct, suboptimal, hallucinatory, or missing [1025]. Combined with existing benchmarks\\nlike NLGraph and GraphDO, these evaluations reveal substantial performance disparities between simple\\nconnectivity problems and complex tasks like maximum flow computation [1085, 895, 319].\\nCurrent implementations face challenges in scaling to large structures, maintaining consistency across\\nmulti-hop relationships, and generalizing to novel topologies, with text-based approaches offering inter-\\npretability at reduced structural precision while specialized architectures provide enhanced performance\\nthrough increased complexity [889, 1100]. Emerging hybrid approaches including InstructGraph and\\nGraphAdapter attempt to bridge these paradigms through structured format verbalizers and GNN-based\\nadapters, though limitations persist in handling dynamic structures and temporal evolution of relationships\\n[261]. Looking forward, broad connection paradigms that organize information through associative networks\\nrather than fragmented searches, spreading outward from central nodes to discover potential connections\\nbetween entities, may represent the next generation of RAG systems for complex context organization [131].\\n7.2.4. Intelligent Context Assembly and Optimization\\nAutomated context engineering systems capable of intelligently assembling contexts from available com-\\nponents represent a critical research frontier requiring development of context optimization algorithms,\\nadaptive selection strategies, and learned assembly functions. Current approaches rely heavily on heuristic\\nmethods and domain-specific engineering, limiting scalability and optimality across diverse applications\\n[1132, 663].\\n54'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 54}, page_content='Self-refinement mechanisms demonstrate substantial potential for intelligent context optimization through\\niterative improvement processes. Self-Refine, Reflexion, and N-CRITICS frameworks achieve significant\\nperformance improvements, with GPT-4 demonstrating approximately 20% improvement through iterative\\nrefinement. However, these approaches require advancement in optimization strategies for autonomous\\nevolution and meta-learning across diverse contexts [735, 956, 789, 577].\\nMulti-dimensional feedback mechanisms incorporating diverse feedback dimensions including correctness,\\nrelevance, clarity, and robustness provide promising directions for context optimization. Self-rewarding\\nmechanisms enable autonomous evolution capabilities, though research must address fundamental questions\\nabout optimal adaptation rates, stability-plasticity trade-offs, and preservation of beneficial adaptations\\nacross varying operational conditions [704].\\n7.3. Application-Driven Research Directions\\nThis subsection addresses research challenges arising from real-world deployment requirements and domain-\\nspecific applications.\\n7.3.1. Domain Specialization and Adaptation\\nContext engineering systems require sophisticated specialization mechanisms for diverse domains including\\nhealthcare, legal analysis, scientific research, education, and engineering applications, each presenting\\nunique requirements for knowledge integration, reasoning patterns, safety considerations, and regulatory\\ncompliance. Domain-specific optimization demands investigation into transfer learning strategies, domain\\nadaptation techniques, and specialized training paradigms that preserve general capabilities while enhancing\\ndomain-specific performance [1132, 663].\\nScientific research applications require sophisticated reasoning capabilities over complex technical content,\\nmathematical expressions, experimental data, and theoretical frameworks while maintaining rigorous\\naccuracy standards. Deep Research Bench evaluation reveals significant challenges in current systems’ ability\\nto conduct complex research tasks requiring synthesis across multiple information sources and reasoning\\nover technical content. Research must address integration of symbolic reasoning with neural approaches and\\nincorporation of domain-specific knowledge bases [87].\\nHealthcare applications demand comprehensive safety evaluation frameworks, regulatory compliance\\nmechanisms, privacy protection protocols, and integration with existing clinical workflows while maintaining\\ninterpretability and auditability requirements. Medical context engineering must address challenges in\\nhandling sensitive information, ensuring clinical accuracy, supporting diagnostic reasoning, and maintaining\\npatient privacy across complex healthcare ecosystems. Current evaluation frameworks reveal substantial\\ngaps in medical reasoning capabilities and safety assessment methodologies [386].\\n7.3.2. Large-Scale Multi-Agent Coordination\\nScaling multi-agent context engineering systems to hundreds or thousands of participating agents requires\\ndevelopment of distributed coordination mechanisms, efficient communication protocols, and hierarchical\\nmanagement structures that maintain system coherence while enabling local autonomy. Research must\\naddress fundamental challenges in distributed consensus, fault tolerance, and emergent behavior prediction\\nin large-scale agent populations [239, 140].\\nCommunication protocol standardization represents a critical research frontier, with emerging protocols\\n55'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 55}, page_content='including MCP (“USB-C for AI”), A2A (Agent-to-Agent), ACP (Agent Communication Protocol), and ANP\\n(Agent Network Protocol) demonstrating the need for unified frameworks enabling interoperability across\\ndiverse agent ecosystems. However, current implementations face security vulnerabilities and scalability\\nlimitations that must be addressed for large-scale deployment [37, 1007, 462, 1, 246, 926, 616].\\nOrchestration challenges including transactional integrity, context management, and coordination strategy\\neffectiveness represent significant obstacles to large-scale multi-agent deployment. Contemporary frameworks\\nincluding LangGraph, AutoGen, and CAMEL demonstrate insufficient transaction support and validation\\nlimitations, requiring systems that rely exclusively on LLM self-validation capabilities. Advanced coordination\\nframeworks must address compensation mechanisms for partial failures and maintain system coherence\\nunder varying operational conditions [128, 390, 893].\\n7.3.3. Human-AI Collaboration and Integration\\nSophisticated human-AI collaboration frameworks require deep understanding of human cognitive pro-\\ncesses, communication preferences, trust dynamics, and collaboration patterns to enable effective hybrid\\nteams that leverage complementary strengths. Research must investigate optimal task allocation strate-\\ngies, communication protocols, and shared mental model development between humans and AI systems\\n[1132, 835].\\nWeb agent evaluation frameworks reveal significant challenges in human-AI collaboration, particularly\\nin complex task scenarios requiring sustained interaction and coordination. WebArena and Mind2Web\\ndemonstrate that current systems struggle with multi-step interactions across diverse websites, highlighting\\nfundamental gaps in collaborative task execution. Advanced interfaces require investigation into context-\\naware adaptation and personalization mechanisms that enhance human-AI team performance [1368, 202].\\nTrust calibration and transparency mechanisms represent critical research areas for ensuring appropriate\\nhuman reliance on AI systems while maintaining human agency and decision-making authority. Evaluation\\nframeworks must address explanation generation, uncertainty communication, and confidence calibration\\nto support informed human decision-making in collaborative scenarios. The substantial performance gaps\\nrevealed by benchmarks like GAIA underscore the importance of developing systems that can effectively\\ncommunicate their limitations and capabilities [772, 1090].\\n7.4. Deployment and Societal Impact Considerations\\nThis subsection examines critical considerations for deploying context engineering systems at scale while\\nensuring responsible and beneficial outcomes.\\n7.4.1. Scalability and Production Deployment\\nProduction deployment of context engineering systems requires addressing scalability challenges across\\nmultiple dimensions including computational resource management, latency optimization, throughput\\nmaximization, and cost efficiency while maintaining consistent performance across diverse operational\\nconditions. The O(n2) scaling limitation of current attention mechanisms creates substantial barriers to\\ndeploying ultra-long context systems in production environments, necessitating advancement in memory-\\nefficient architectures and sliding attention mechanisms [295, 1227].\\nReliability and fault tolerance mechanisms become critical as context engineering systems assume increas-\\ningly important roles in decision-making processes across domains. Multi-agent orchestration frameworks\\n56'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 56}, page_content='face particular challenges in maintaining transactional integrity across complex workflows, with current\\nsystems lacking adequate compensation mechanisms for partial failures. Research must address grace-\\nful degradation strategies, error recovery protocols, and redundancy mechanisms that maintain system\\nfunctionality under adverse conditions [128, 390].\\nMaintainability and evolution challenges require investigation into system versioning, backward compati-\\nbility, continuous integration protocols, and automated testing frameworks that support ongoing system\\nimprovement without disrupting deployed services. Memory system implementations face additional chal-\\nlenges due to the stateless nature of current LLMs and the lack of standardized benchmarks for long-term\\nmemory persistence and retrieval efficiency [1330, 1171].\\n7.4.2. Safety, Security, and Robustness\\nComprehensive safety evaluation requires development of assessment frameworks that can identify potential\\nfailure modes, safety violations, and unintended behaviors across the full spectrum of context engineering\\nsystem capabilities. Agentic systems present particular safety challenges due to their autonomous operation\\ncapabilities and complex interaction patterns across extended operational periods [965, 360].\\nSecurity considerations encompass protection against adversarial attacks, data poisoning, prompt in-\\njection, model extraction, and privacy violations while maintaining system functionality and usability.\\nMulti-agent communication protocols including MCP, A2A, and ACP introduce security vulnerabilities that\\nmust be addressed while preserving interoperability and functionality. Research must develop defense\\nmechanisms and detection systems that address evolving threat landscapes across distributed agent networks\\n[246, 926].\\nAlignment and value specification challenges require investigation into methods for ensuring context\\nengineering systems behave according to intended objectives while avoiding specification gaming, reward\\nhacking, and goal misalignment. Context engineering systems present unique alignment challenges due to\\ntheir dynamic adaptation capabilities and complex interaction patterns across multiple components. The\\nsubstantial performance gaps revealed by evaluation frameworks underscore the importance of developing\\nrobust alignment mechanisms that can maintain beneficial behaviors as systems evolve and adapt [772, 128].\\n7.4.3. Ethical Considerations and Responsible Development\\nBias mitigation and fairness evaluation require comprehensive assessment frameworks that can identify and\\naddress systematic biases across different demographic groups, application domains, and use cases while\\nmaintaining system performance and utility. Research must investigate bias sources in training data, model\\narchitectures, and deployment contexts while developing mitigation strategies that address root causes\\nrather than symptoms [1132, 835].\\nPrivacy protection mechanisms must address challenges in handling sensitive information, preventing data\\nleakage, and maintaining user privacy while enabling beneficial system capabilities. Memory systems face\\nparticular privacy challenges due to their persistent information storage and retrieval capabilities, requiring\\nadvanced frameworks for secure memory management and selective forgetting mechanisms [1330, 457].\\nTransparency and accountability frameworks require development of explanation systems, audit mecha-\\nnisms, and governance structures that enable responsible oversight of context engineering systems while\\nsupporting innovation and beneficial applications. The substantial performance gaps revealed by evalua-\\ntion frameworks like GAIA (human 92% vs AI 15%) highlight the importance of transparent capability\\ncommunication and appropriate expectation setting for deployed systems [772, 1090].\\n57'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 57}, page_content='The future of Context Engineering will be shaped by our ability to address these interconnected challenges\\nthrough sustained, collaborative research efforts that bridge technical innovation with societal considerations.\\nSuccess will require continued investment in fundamental research, interdisciplinary collaboration,\\nand responsible development practices that ensure context engineering systems remain beneficial, reliable,\\nand aligned with human values as they become increasingly integrated into critical societal functions\\n[835, 1132, 310].\\n8. Conclusion\\nThis survey has presented the first comprehensive examination of Context Engineering as a formal discipline\\nthat systematically designs, optimizes, and manages information payloads for LLMs. Through our analysis of\\nover 1400 research papers, we have established Context Engineering as a critical foundation for developing\\nsophisticated AI systems that effectively integrate external knowledge, maintain persistent memory, and\\ninteract dynamically with complex environments.\\nOur primary contribution lies in introducing a unified taxonomic framework that organizes context\\nengineering techniques into Foundational Components (Context Retrieval and Generation, Context Process-\\ning, and Context Management) and System Implementations (Retrieval-Augmented Generation, Memory\\nSystems, Tool-Integrated Reasoning, and Multi-Agent Systems). This framework demonstrates how core\\ntechnical capabilities integrate into sophisticated architectures addressing real-world requirements.\\nThrough this systematic examination, we have identified several key insights. First, we observe a\\nfundamental asymmetry between LLMs’ remarkable capabilities in understanding complex contexts and\\ntheir limitations in generating equally sophisticated outputs. This comprehension-generation gap represents\\none of the most critical challenges facing the field. Second, our analysis reveals increasingly sophisticated\\nintegration patterns where multiple techniques combine synergistically, creating capabilities that exceed\\ntheir individual components. Third, we observe a clear trend toward modularity and compositionality,\\nenabling flexible architectures adaptable to diverse applications while maintaining system coherence. The\\nevaluation challenges we identified underscore the need for comprehensive assessment frameworks that\\ncapture the complex, dynamic behaviors exhibited by context-engineered systems. Traditional evaluation\\nmethodologies prove insufficient for systems that integrate multiple components, exhibit adaptive behaviors,\\nand operate across extended time horizons. Our examination of future research directions reveals significant\\nopportunities including developing next-generation architectures for efficient long context handling, creating\\nintelligent context assembly systems, and advancing multi-agent coordination mechanisms. Key challenges\\nspan theoretical foundations, technical implementation, and practical deployment, including the lack of\\nunified theoretical frameworks, scaling limitations, and safety considerations.\\nLooking toward the future, Context Engineering stands poised to play an increasingly central role in AI\\ndevelopment as the field moves toward complex, multi-component systems. The interdisciplinary nature of\\nContext Engineering necessitates collaborative research approaches spanning computer science, cognitive\\nscience, linguistics, and domain-specific expertise.\\nAs LLMs continue to evolve, the fundamental insight underlying Context Engineering—that AI system\\nperformance is fundamentally determined by contextual information—will remain central to artificial\\nintelligence development. This survey provides both a comprehensive snapshot of the current state and a\\nroadmap for future research, establishing Context Engineering as a distinct discipline with its own principles,\\nmethodologies, and challenges to foster innovation and support responsible development of context-aware\\nAI systems.\\n58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 58}, page_content='Acknowledgments\\nThis survey represents an ongoing effort to comprehensively map the rapidly evolving landscape of Context\\nEngineering for Large Language Models. Given the dynamic nature of this field, with new developments\\nemerging continuously, we acknowledge that despite our best efforts, some recent works or emerging trends\\nmay have been inadvertently overlooked or underrepresented. We welcome feedback from the research\\ncommunity to help improve future iterations of this work. We are grateful to the broader research community\\nwhose foundational contributions have made this survey possible. This work would not have been achievable\\nwithout the invaluable support of both the research community and the open-source community, whose\\ncollaborative efforts in developing frameworks, tools, and resources have significantly advanced the field of\\nContext Engineering.\\nReferences\\n[1] Anp-agent\\ncommunication\\nmeta-protocol\\nspecification(draft).\\nhttps://\\nagent-network-protocol.com/specs/communication.html.\\n[Online; accessed 17-\\nJuly-2025].\\n[2] S. A. Automating human evaluation of dialogue systems. North American Chapter of the Association\\nfor Computational Linguistics, 2022.\\n[3] Samir Abdaljalil, Hasan Kurban, Khalid A. Qaraqe, and E. Serpedin. Theorem-of-thought: A multi-\\nagent framework for abductive, deductive, and inductive reasoning in language models. arXiv\\npreprint, 2025.\\n[4] Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, and Adam Jatowt.\\nRankify: A comprehensive python toolkit for retrieval, re-ranking, and retrieval-augmented genera-\\ntion, arXiv preprint arXiv:2502.02464, 2025. URL https://arxiv.org/abs/2502.02464v3.\\n[5] Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matt Stallone, Rameswar\\nPanda, Yara Rizk, G. Bhargav, M. Crouse, Chulaka Gunasekara, S. Ikbal, Sachin Joshi, Hima P.\\nKaranam, Vineet Kumar, Asim Munawar, S. Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza\\nSoria, Dheeraj Sreedhar, P. Venkateswaran, Merve Unuvar, David Cox, S. Roukos, Luis A. Lastras, and\\nP. Kapanipathi. Granite-function calling model: Introducing function calling abilities via multi-task\\nlearning of granular tasks. Conference on Empirical Methods in Natural Language Processing, 2024.\\n[6] D. Acharya, Karthigeyan Kuppan, and Divya Bhaskaracharya. Agentic ai: Autonomous intelligence\\nfor complex goals—a comprehensive survey. IEEE Access, 2025.\\n[7] Manoj Acharya, Kushal Kafle, and Christopher Kanan.\\nTallyqa: Answering complex counting\\nquestions. AAAI Conference on Artificial Intelligence, 2018.\\n[8] Shantanu Acharya, Fei Jia, and Boris Ginsburg.\\nStar attention: Efficient llm inference over\\nlong sequences, arXiv preprint arXiv:2411.17116, 2024. URL https://arxiv.org/abs/2411.\\n17116v3.\\n[9] Emre Can Acikgoz, Jeremy Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil\\nKoukoumidis, Dilek Hakkani-Tur, and Gokhan Tur. Can a single model master both multi-turn\\nconversations and tool use? coalm: A unified conversational agentic language model, arXiv preprint\\narXiv:2502.08820, 2025. URL https://arxiv.org/abs/2502.08820v3.\\n59'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 59}, page_content='[10] Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek Hakkani-\\nTur, and Gokhan Tur. A desideratum for conversational agents: Capabilities, challenges, and\\nfuture directions, arXiv preprint arXiv:2504.16939, 2025. URL https://arxiv.org/abs/2504.\\n16939v1.\\n[11] Anum Afzal, Juraj Vladika, Gentrit Fazlija, Andrei Staradubets, and Florian Matthes. Towards opti-\\nmizing a retrieval augmented generation using large language model on academic data. International\\nConference on Natural Language Processing and Information Retrieval, 2024.\\n[12] Ankush Agarwal, Sakharam Gawade, A. Azad, and P. Bhattacharyya.\\nKitlm: Domain-specific\\nknowledge integration into language models for question answering. ICON, 2023.\\n[13] Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Large scale knowledge graph based\\nsynthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint,\\n2020.\\n[14] Monica Agrawal, S. Hegselmann, Hunter Lang, Yoon Kim, and D. Sontag. Large language models\\nare few-shot clinical information extractors. Conference on Empirical Methods in Natural Language\\nProcessing, 2022.\\n[15] Arash Ahmadi, S. Sharif, and Yaser Mohammadi Banadaki. Mcp bridge: A lightweight, llm-agnostic\\nrestful proxy for model context protocol servers, arXiv preprint arXiv:2504.08999, 2025. URL\\nhttps://arxiv.org/abs/2504.08999v1.\\n[16] J. Ainslie, J. Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr’on, and Sumit K. Sanghai.\\nGqa: Training generalized multi-query transformer models from multi-head checkpoints. Conference\\non Empirical Methods in Natural Language Processing, 2023.\\n[17] Adel Al-Jumaily. Multi-agent system concepts theory and application phases. arXiv preprint, 2006.\\n[18] Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, and Joel Hestness. Position interpolation improves\\nalibi extrapolation. arXiv preprint, 2023.\\n[19] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\\nA. Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy\\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, O. Vinyals,\\nAndrew Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning.\\nNeural Information Processing Systems, 2022.\\n[20] Stefano V. Albrecht and P. Stone. Autonomous agents modelling other agents: A comprehensive\\nsurvey and open problems. Artificial Intelligence, 2017.\\n[21] Buthayna AlMulla, Maram Assi, and Safwat Hassan. Understanding the challenges and promises of\\ndeveloping generative ai apps: An empirical study, arXiv preprint arXiv:2506.16453, 2025. URL\\nhttps://arxiv.org/abs/2506.16453v2.\\n[22] Reem S. Alsuhaibani, Christian D. Newman, M. J. Decker, Michael L. Collard, and Jonathan I. Maletic.\\nOn the naming of methods: A survey of professional developers. International Conference on Software\\nEngineering, 2021.\\n60'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 60}, page_content='[23] Francesco Alzetta, P. Giorgini, A. Najjar, M. Schumacher, and Davide Calvaresi. In-time explainability\\nin multi-agent systems: Challenges, opportunities, and roadmap. EXTRAAMAS@AAMAS, 2020.\\n[24] Kenza Amara, Lukas Klein, Carsten T. Lüth, Paul F. Jäger, Hendrik Strobelt, and Mennatallah El-\\nAssady. Why context matters in vqa and reasoning: Semantic interventions for vlm input modalities,\\narXiv preprint arXiv:2410.01690v1, 2024. URL https://arxiv.org/abs/2410.01690v1.\\n[25] Xavier Amatriain. Prompt design and engineering: Introduction and advanced methods, arXiv\\npreprint arXiv:2401.14423, 2024. URL https://arxiv.org/abs/2401.14423v4.\\n[26] Zahra Aminiranjbar, Jianan Tang, Qiudan Wang, Shubha Pant, and Mahesh Viswanathan. Dawn:\\nDesigning distributed agents in a worldwide network, arXiv preprint arXiv:2410.22339, 2024. URL\\nhttps://arxiv.org/abs/2410.22339v3.\\n[27] Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng\\nKong. Why does the effective context length of llms fall short? International Conference on Learning\\nRepresentations, 2024.\\n[28] Kaikai An, Fangkai Yang, Liqun Li, Junting Lu, Sitao Cheng, Shuzheng Si, Lu Wang, Pu Zhao, Lele\\nCao, Qingwei Lin, et al. Thread: A logic-based data organization paradigm for how-to question\\nanswering with retrieval augmented generation. arXiv preprint arXiv:2406.13372, 2024.\\n[29] Kaikai An, Fangkai Yang, Junting Lu, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang,\\nHua Ding, et al. Nissist: An incident mitigation copilot based on troubleshooting guides.\\nIn\\nProceedings of the 27th European Conference on Artificial Intelligence (ECAI 2024), pages 4471–4474,\\n2024.\\n[30] Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. Ultraif:\\nAdvancing instruction following from the wild. pages 7930–7957, 2025.\\n[31] Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, and Paul Hongsuck Seo. Lcirc: A recurrent\\ncompression approach for efficient long-form context and query dependent modeling in llms. North\\nAmerican Chapter of the Association for Computational Linguistics, 2025.\\n[32] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurélien Lucchi, and Thomas Hof-\\nmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Neural\\nInformation Processing Systems, 2023.\\n[33] John R. Anderson, M. Matessa, and C. Lebiere. Act-r: A theory of higher level cognition and its\\nrelation to visual attention. Hum. Comput. Interact., 1997.\\n[34] Jacob Andreas. Language models as agent models. Conference on Empirical Methods in Natural\\nLanguage Processing, 2022.\\n[35] Leonardo Aniello, R. Baldoni, and Leonardo Querzoni. Adaptive online scheduling in storm. Dis-\\ntributed Event-Based Systems, 2013.\\n[36] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, M. Burtsev, and Evgeny Burnaev.\\nArigraph: Learning knowledge graph world models with episodic memory for llm agents, arXiv\\npreprint arXiv:2407.04363, 2024. URL https://arxiv.org/abs/2407.04363v3.\\n61'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 61}, page_content='[37] Anthropic.\\nIntroducing the model context protocol, November 2024.\\nURL https://www.\\nanthropic.com/news/model-context-protocol. [Online; accessed 17-July-2025].\\n[38] RM Aratchige and Dr. Wmks Ilmini. Llms working in harmony: A survey on the technological aspects\\nof building effective llm-based multi agent systems, arXiv preprint arXiv:2504.01963, 2025. URL\\nhttps://arxiv.org/abs/2504.01963v1.\\n[39] Leo Ardon, Daniel Furelos-Blanco, and A. Russo. Learning reward machines in cooperative multi-\\nagent tasks. AAMAS Workshops, 2023.\\n[40] K. Armeni, C. Honey, and Tal Linzen. Characterizing verbatim short-term memory in neural language\\nmodels. Conference on Computational Natural Language Learning, 2022.\\n[41] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning\\nto retrieve, generate, and critique through self-reflection. International Conference on Learning\\nRepresentations, 2023.\\n[42] Hikaru Asano, Tadashi Kozuno, and Yukino Baba. Self iterative label refinement via robust unla-\\nbeled learning, arXiv preprint arXiv:2502.12565, 2025. URL https://arxiv.org/abs/2502.\\n12565v1.\\n[43] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding,\\nQing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta\\nSengupta, and Bing Xiang. Bifurcated attention: Accelerating massively parallel decoding with\\nshared prefixes in llms, arXiv preprint arXiv:2403.08845, 2024. URL https://arxiv.org/abs/\\n2403.08845v2.\\n[44] Avinash Ayalasomayajula, Rui Guo, Jingbo Zhou, Sujan Kumar Saha, and Farimah Farahmandi. Lasp:\\nLlm assisted security property generation for soc verification. Workshop on Machine Learning for\\nCAD, 2024.\\n[45] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with\\nadaptive cognitive-inspired sketching. arXiv preprint, 2025.\\n[46] Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, A. Kazerouni, I. Rekik, and D. Merhof.\\nFoundational models in medical imaging: A comprehensive survey and future vision, arXiv preprint\\narXiv:2310.18689, 2023. URL https://arxiv.org/abs/2310.18689v1.\\n[47] Gilbert Badaro, Mohammed Saeed, and Paolo Papotti. Transformers for tabular data representation:\\nA survey of models and applications. Transactions of the Association for Computational Linguistics,\\n2023.\\n[48] Jinheon Baek, N. Chandrasekaran, Silviu Cucerzan, Allen Herring, and S. Jauhar. Knowledge-\\naugmented large language models for personalized contextual query suggestion. The Web Conference,\\n2023.\\n[49] Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He,\\nBinhang Yuan, and Wentao Zhang. A survey of multimodal large language model from a data-centric\\nperspective, arXiv preprint arXiv:2405.16640v2, 2024. URL https://arxiv.org/abs/2405.\\n16640v2.\\n62'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 62}, page_content='[50] Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, and Jackie\\nChi Kit Cheung. Citrus: Chunked instruction-aware state eviction for long sequence modeling.\\nConference on Empirical Methods in Natural Language Processing, 2024.\\n[51] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\\nSam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback,\\narXiv preprint arXiv:2212.08073, 2022. URL https://arxiv.org/abs/2212.08073.\\n[52] Souhail Bakkali, Sanket Biswas, Zuheng Ming, Mickaël Coustaty, Marccal Rusinol, O. R. Terrades,\\nand Josep Llad’os. Globaldoc: A cross-modal vision-language framework for real-world document\\nimage retrieval and classification. IEEE Workshop/Winter Conference on Applications of Computer\\nVision, 2023.\\n[53] Jayachandu Bandlamudi, K. Mukherjee, Prerna Agarwal, Sampath Dechu, Siyu Huo, Vatche Isahagian,\\nVinod Muthusamy, N. Purushothaman, and Renuka Sindhgatta. Towards hybrid automation by\\nbootstrapping conversational interfaces for it operation tasks. AAAI Conference on Artificial Intelligence,\\n2023.\\n[54] Jayachandu Bandlamudi, Kushal Mukherjee, Prerna Agarwal, Ritwik Chaudhuri, R. Pimplikar,\\nSampath Dechu, Alex Straley, Anbumunee Ponniah, and Renuka Sindhgatta. Building conversational\\nartifacts to enable digital assistant for apis and rpas. AAAI Conference on Artificial Intelligence, 2024.\\n[55] Keqin Bao, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, and Fuli Feng. Large language models\\nfor recommendation: Past, present, and future. Annual International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 2024.\\n[56] Sara Di Bartolomeo, Giorgio Severi, V. Schetinger, and Cody Dunne. Ask and you shall receive\\n(a graph drawing): Testing chatgpt’s potential to apply graph layout algorithms. Eurographics\\nConference on Visualization, 2023.\\n[57] Saikat Barua. Exploring autonomous agents through the lens of large language models: A review,\\narXiv preprint arXiv:2404.04442, 2024. URL https://arxiv.org/abs/2404.04442v1.\\n[58] Kinjal Basu, Ibrahim Abdelaziz, Kelsey Bradford, M. Crouse, Kiran Kate, Sadhana Kumaravel, Saurabh\\nGoyal, Asim Munawar, Yara Rizk, Xin Wang, Luis A. Lastras, and P. Kapanipathi. Nestful: A benchmark\\nfor evaluating llms on nested sequences of api calls, arXiv preprint arXiv:2409.03797, 2024. URL\\nhttps://arxiv.org/abs/2409.03797v3.\\n[59] Amin Beheshti. Natural language-oriented programming (nlop): Towards democratizing software\\ncreation. 2024 IEEE International Conference on Software Services Engineering (SSE), 2024.\\n[60] Azadeh Beiranvand and S. M. Vahidipour. Integrating structural and semantic signals in text-\\nattributed graphs with bigtex, arXiv preprint arXiv:2504.12474, 2025. URL https://arxiv.org/\\nabs/2504.12474v2.\\n63'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 63}, page_content='[61] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf,\\nand Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. International\\nConference on Learning Representations, 2024.\\n[62] Assaf Ben-Kish, Itamar Zimerman, M. J. Mirza, James R. Glass, Leonid Karlinsky, and Raja Giryes.\\nOverflow prevention enhances long-context recurrent llms. arXiv preprint, 2025.\\n[63] M. Benna and Stefano Fusi. Complex synapses as efficient memory systems. BMC Neuroscience,\\n2015.\\n[64] M. Benna and Stefano Fusi.\\nComputational principles of biological memory, arXiv preprint\\narXiv:1507.07580, 2015. URL https://arxiv.org/abs/1507.07580v1.\\n[65] Shelly Bensal, Umar Jamil, Christopher Bryant, M. Russak, Kiran Kamble, Dmytro Mozolevskyi,\\nMuayad Ali, and Waseem Alshikh. Reflect, retry, reward: Self-improving llms via reinforcement learn-\\ning, arXiv preprint arXiv:2505.24726, 2025. URL https://arxiv.org/abs/2505.24726v1.\\n[66] Idoia Berges, J. Bermúdez, A. Goñi, and A. Illarramendi. Semantic web technology for agent\\ncommunication protocols. Extended Semantic Web Conference, 2008.\\n[67] Gaurav Beri and Vaishnavi Srivastava. Advanced techniques in prompt engineering for large language\\nmodels: A comprehensive study. 2024 IEEE 4th International Conference on ICT in Business Industry\\n& Government (ICTBIG), 2024.\\n[68] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range\\ntransformers with unlimited length input. Neural Information Processing Systems, 2023.\\n[69] Maciej Besta, Nils Blach, Aleš Kubíček, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda,\\nTomasz Lehmann, Michal Podstawski, H. Niewiadomski, P. Nyczyk, and Torsten Hoefler. Graph of\\nthoughts: Solving elaborate problems with large language models. AAAI Conference on Artificial\\nIntelligence, 2023.\\n[70] Gregor Betz and Kyle Richardson. Judgment aggregation, discursive dilemma and reflective equilib-\\nrium: Neural language models as self-improving doxastic agents. Frontiers in Artificial Intelligence,\\n2022.\\n[71] L. Bezalel, Eyal Orgad, and Amir Globerson. Teaching models to improve on tape. AAAI Conference\\non Artificial Intelligence, 2024.\\n[72] Umang Bhatt, Sanyam Kapoor, Mihir Upadhyay, Ilia Sucholutsky, Francesco Quinzan, Katherine M.\\nCollins, Adrian Weller, Andrew Gordon Wilson, and Muhammad Bilal Zafar. When should we\\norchestrate multiple agents?, arXiv preprint arXiv:2503.13577, 2025. URL https://arxiv.org/\\nabs/2503.13577v1.\\n[73] Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui\\nMei, Junfeng Fang, Zehao Li, Furu Wei, et al. Context-dpo: Aligning language models for context-\\nfaithfulness. ACL 2025, 2024.\\n[74] Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, and Xueqi Cheng. Decoding by\\ncontrasting knowledge: Enhancing llms’ confidence on edited facts. ACL 2025, 2024.\\n64'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 64}, page_content='[75] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, and Xueqi Cheng. Lpnl: Scalable link prediction\\nwith large language models. ACL 2024, 2024.\\n[76] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, and Xueqi\\nCheng. Struedit: Structured outputs enable the fast and accurate knowledge editing for large\\nlanguage models. 2024.\\n[77] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, and Xueqi Cheng.\\nAdaptive token biaser: Knowledge editing via biasing key entities. EMNLP 2024, 2024.\\n[78] Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei,\\nJunfeng Fang, Jiafeng Guo, and Xueqi Cheng. Refinex: Learning to refine pre-training data at scale\\nfrom expert-guided programs. 2025.\\n[79] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, Shiyu Ni, and\\nXueqi Cheng. Is factuality enhancement a free lunch for llms? better factuality can lead to worse\\ncontext-faithfulness. ICLR 2025, 2025.\\n[80] Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, and Xueqi Cheng.\\nParameters vs. context: Fine-grained control of knowledge reliance in language models. 2025.\\n[81] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-\\ning&autoregressive language model for context-conditioned generation. Conference on Empirical\\nMethods in Natural Language Processing, 2020.\\n[82] Dinh Doan Van Bien, David Lillis, and Rem W. Collier. Call graph profiling for multi agent systems.\\nMulti-Agent Logics, Languages, and Organisations Federated Workshops, 2009.\\n[83] Jonas Bode, Bastian Pätzold, Raphael Memmesheimer, and Sven Behnke. A comparison of prompt\\nengineering techniques for task planning and execution in service robotics. IEEE-RAS International\\nConference on Humanoid Robots, 2024.\\n[84] P. Bonzon. Grounding mental representations in a virtual multi-level functional framework. Journal\\nof Cognition, 2023.\\n[85] Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones,\\nAlbin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan,\\nJack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of\\ntokens. International Conference on Machine Learning, 2021.\\n[86] Zalán Borsos, Raphaël Marinier, Damien Vincent, E. Kharitonov, O. Pietquin, Matthew Sharifi,\\nDominik Roblek, O. Teboul, David Grangier, M. Tagliasacchi, and Neil Zeghidour. Audiolm: A\\nlanguage modeling approach to audio generation. IEEE/ACM Transactions on Audio Speech and\\nLanguage Processing, 2022.\\n[87] FutureSearch Nikos I. Bosse, Jon Evans, Robert G. Gambee, Daniel Hnyk, Peter Muhlbacher, Lawrence\\nPhillips, Dan Schwarz, and Jack Wildman. Deep research bench: Evaluating ai web research agents,\\narXiv preprint arXiv:2506.06287, 2025. URL https://arxiv.org/abs/2506.06287v1.\\n65'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 65}, page_content='[88] Vicent Botti.\\nAgentic ai and multiagentic: Are we reinventing the wheel?, arXiv preprint\\narXiv:2506.01463, 2025. URL https://arxiv.org/abs/2506.01463v1.\\n[89] William Brach, Kristián Kostál, and Michal Ries. The effectiveness of large language models in\\ntransforming unstructured text to standardized formats. IEEE Access, 2025.\\n[90] C. Brainerd, C. F. Gomes, and K. Nakamura. Dual recollection in episodic memory. Journal of\\nexperimental psychology. General, 2015.\\n[91] Inês Bramão, Jiefeng Jiang, A. Wagner, and M. Johansson. Encoding contexts are incidentally\\nreinstated during competitive retrieval and track the temporal dynamics of memory interference.\\nCerebral Cortex, 2022.\\n[92] Andrés M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and P. Schwaller.\\nAugmenting large language models with chemistry tools. Nat. Mac. Intell., 2023.\\n[93] Maricela Claudia Bravo and Martha Coronel. Aligning agent communication protocols - a pragmatic\\napproach. International Conference on Software and Data Technologies, 2008.\\n[94] F. Brazier, B. Dunin-Keplicz, N. Jennings, and Jan Treur. Desire: Modelling multi-agent systems in a\\ncompositional formal framework. International Journal of Cooperative Information Systems, 1997.\\n[95] Lorenz Brehme, Thomas Ströhle, and Ruth Breu. Can llms be trusted for evaluating rag systems? a\\nsurvey of methods and datasets, arXiv preprint arXiv:2504.20119, 2025. URL https://arxiv.\\norg/abs/2504.20119v2.\\n[96] R. Breil, D. Delahaye, Laurent Lapasset, and E. Feron. Multi-agent systems to help managing air\\ntraffic structure. arXiv preprint, 2017.\\n[97] Alexander Brinkmann and Christian Bizer. Self-refinement strategies for llm-based product attribute\\nvalue extraction. Datenbanksysteme für Business, Technologie und Web, 2025.\\n[98] D. Britz, M. Guan, and Minh-Thang Luong. Efficient attention using a fixed-size memory representa-\\ntion. Conference on Empirical Methods in Natural Language Processing, 2017.\\n[99] Adam W. Broitman and M. J. Kahana. Neural context reinstatement of recurring events. bioRxiv,\\n2024.\\n[100] Ethan A. Brooks, Logan Walls, Richard L. Lewis, and Satinder Singh. Large language models can\\nimplement policy iteration. Neural Information Processing Systems, 2022.\\n[101] Rodney A. Brooks. A robust layered control system for a mobile robot. IEEE J. Robotics Autom., 1986.\\n[102] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image\\nediting instructions. Computer Vision and Pattern Recognition, 2022.\\n[103] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\\nGretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter,\\nChristopher Hesse, Mark Chen, Eric Sigler, Ma teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\\nChristopher Berner, Sam McCandlish, Alec Radford, I. Sutskever, and Dario Amodei. Language\\nmodels are few-shot learners. Neural Information Processing Systems, 2020.\\n66'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 66}, page_content='[104] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\\nVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. URL\\nhttps://arxiv.org/abs/2005.14165.\\n[105] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, L. Baraldi, and\\nR. Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. 2024\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2024.\\n[106] Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller,\\nFotis Psallidas, and Carlo Curino. Optimizing open-domain question answering with graph-based\\nretrieval augmented generation, arXiv preprint arXiv:2503.02922, 2025. URL https://arxiv.\\norg/abs/2503.02922v1.\\n[107] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua.\\nLarge language models empowered personalized web agents. The Web Conference, 2024.\\n[108] Yujun Cai, Liuhao Ge, Jianfei Cai, and Junsong Yuan. Weakly-supervised 3d hand pose estimation\\nfrom monocular rgb images. In Proceedings of the European conference on computer vision (ECCV),\\npages 666–682, 2018.\\n[109] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat\\nThalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional\\nnetworks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2272–\\n2281, 2019.\\n[110] Yujun Cai, Liuhao Ge, Jianfei Cai, Nadia Magnenat Thalmann, and Junsong Yuan. 3d hand pose\\nestimation using synthetic data and weakly labeled rgb images. IEEE transactions on pattern analysis\\nand machine intelligence, 43(11):3739–3753, 2020.\\n[111] Yujun Cai, Lin Huang, Yiwei Wang, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Xu Yang, Yiheng\\nZhu, Xiaohui Shen, et al. Learning progressive joint propagation for human motion prediction.\\nIn Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\\nProceedings, Part VII 16, pages 226–242. Springer International Publishing, 2020.\\n[112] Yujun Cai, Yiwei Wang, Yiheng Zhu, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Chuanxia\\nZheng, Sijie Yan, Henghui Ding, et al. A unified 3d human motion synthesis model via conditional\\nvariational auto-encoder. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\\npages 11645–11655, 2021.\\n[113] V. Camos and P. Barrouillet. Attentional and non-attentional systems in the maintenance of ver-\\nbal information in working memory: the executive and phonological loops. Frontiers in Human\\nNeuroscience, 2014.\\n[114] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun. Retentive\\nor forgetful? diving into the knowledge memorizing mechanism of language models. International\\nConference on Language Resources and Evaluation, 2023.\\n67'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 67}, page_content='[115] He Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, and Dongyan Zhao. A step closer to com-\\nprehensive answers: Constrained multi-stage question decomposition with large language models,\\narXiv preprint arXiv:2311.07491, 2023. URL https://arxiv.org/abs/2311.07491v1.\\n[116] Nicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents\\nwith graph convolutional networks. North American Chapter of the Association for Computational\\nLinguistics, 2018.\\n[117] Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao,\\nKang Liu, and Jun Zhao. Large language models for planning: A comprehensive and systematic survey,\\narXiv preprint arXiv:2505.19683, 2025. URL https://arxiv.org/abs/2505.19683v1.\\n[118] Yongcan Cao, Wenwu Yu, W. Ren, and Guanrong Chen. An overview of recent progress in the study\\nof distributed multi-agent coordination. IEEE Transactions on Industrial Informatics, 2012.\\n[119] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua\\nZhao, Jinyue Yan, and Yunjie Li. Survey on large language model-enhanced reinforcement learning:\\nConcept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems, 2024.\\n[120] Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, and S. K. Zhou. Lego-graphrag: Modularizing\\ngraph-based retrieval-augmented generation for design space exploration. arXiv preprint, 2024.\\n[121] R. C. Cardoso and Angelo Ferrando. A review of agent-based programming for multi-agent systems.\\nDe Computis, 2021.\\n[122] Nicholas Carlini, Chang Liu, Ú. Erlingsson, Jernej Kos, and D. Song. The secret sharer: Evaluating\\nand testing unintended memorization in neural networks. USENIX Security Symposium, 2018.\\n[123] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee,\\nAdam Roberts, Tom B. Brown, D. Song, Ú. Erlingsson, Alina Oprea, and Colin Raffel. Extracting\\ntraining data from large language models. USENIX Security Symposium, 2020.\\n[124] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and\\nG. Jiménez-Moreno. A bio-inspired implementation of a sparse-learning spike-based hippocampus\\nmemory model. IEEE Transactions on Emerging Topics in Computing, 2022.\\n[125] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and\\nG. Jiménez-Moreno. Bio-inspired computational memory model of the hippocampus: an approach\\nto a neuromorphic spike-based content-addressable memory. Neural Networks, 2023.\\n[126] Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang,\\nSambit Sahu, M. Naphade, and Genta Indra Winata. T1: A tool-oriented conversational dataset for\\nmulti-turn agentic planning, arXiv preprint arXiv:2505.16986, 2025. URL https://arxiv.org/\\nabs/2505.16986v1.\\n[127] Kranti Chalamalasetti, Jana Gotze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David\\nSchlangen. clembench: Using game play to evaluate chat-optimized language models as conversa-\\ntional agents. Conference on Empirical Methods in Natural Language Processing, 2023.\\n[128] Edward Y. Chang and Longling Geng. Sagallm: Context management, validation, and transaction\\nguarantees for multi-agent llm planning, arXiv preprint arXiv:2503.11951, 2025. URL https:\\n//arxiv.org/abs/2503.11951v2.\\n68'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 68}, page_content='[129] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan\\nYi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and\\nXingxu Xie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems\\nand Technology, 2023.\\n[130] Subhajit Chaudhury, Payel Das, Sarathkrishna Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu\\nPahwa, Tejaswini Pedapati, Igor Melnyk, and Matthew Riemer. Epman: Episodic memory attention\\nfor generalizing to longer contexts, arXiv preprint arXiv:2502.14280, 2025. URL https://arxiv.\\norg/abs/2502.14280v1.\\n[131] Xueqi CHEGN, Shenghua Liu, and Ruqing ZHANG. Thinking on new system for big data technology.\\nBulletin of Chinese Academy of Sciences (Chinese Version), 37(1):60–67, 2022.\\n[132] Viktoriia Chekalina, Anton Razzigaev, Elizaveta Goncharova, and Andrey Kuznetsov. Addressing\\nhallucinations in language models with knowledge graph embeddings as an additional modality,\\narXiv preprint arXiv:2411.11531, 2024. URL https://arxiv.org/abs/2411.11531v2.\\n[133] Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced sparse attention\\nacceleration, arXiv preprint arXiv:2410.10165, 2024. URL https://arxiv.org/abs/2410.\\n10165v2.\\n[134] Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, and Cheng\\nYang. Pathrag: Pruning graph-based retrieval augmented generation with relational paths, arXiv\\npreprint arXiv:2502.14902, 2025. URL https://arxiv.org/abs/2502.14902v1.\\n[135] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp:\\nA survey on vision-language pre-training. Machine Intelligence Research, 20(1):38–56, 2023.\\n[136] Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong\\nXue, Zhi Yang, Mao Yang, and Haibo Chen. Attentionengine: A versatile framework for efficient\\nattention mechanisms on diverse hardware platforms, arXiv preprint arXiv:2502.15349, 2025. URL\\nhttps://arxiv.org/abs/2502.15349v1.\\n[137] Huajun Chen. Large knowledge model: Perspectives and challenges. Data Intelligence, 2023.\\n[138] Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong\\nZhao, Tianlu Mao, and Yucheng Zhang. Haif-gs: Hierarchical and induced flow-guided gaussian\\nsplatting for dynamic scene. 2025.\\n[139] Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei\\nKe, Joseph Liu, Yiwen Yuan, Julian McAuley, and Li jia Li. Symbolic representation for any-to-any\\ngenerative tasks, arXiv preprint arXiv:2504.17261v1, 2025. URL https://arxiv.org/abs/\\n2504.17261v1.\\n[140] Jiayi Chen, J. Ye, and Guiling Wang. From standalone llms to integrated intelligence: A survey of\\ncompound al systems, arXiv preprint arXiv:2506.04565, 2025. URL https://arxiv.org/abs/\\n2506.04565v1.\\n[141] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei,\\nXiaolong Chen, Xingmei Wang, Defu Lian, and Enhong Chen. When large language models meet\\npersonalization: Perspectives of challenges and opportunities. World wide web (Bussum), 2023.\\n69'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 69}, page_content='[142] Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, and Xiaoxin Chen.\\nEdgeinfinite: A memory-efficient infinite-context transformer for edge devices, arXiv preprint\\narXiv:2503.22196, 2025. URL https://arxiv.org/abs/2503.22196v1.\\n[143] Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magi-\\ncore: Multi-agent, iterative, coarse-to-fine refinement for reasoning, arXiv preprint arXiv:2409.12147,\\n2024. URL https://arxiv.org/abs/2409.12147v1.\\n[144] Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang,\\nZenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional\\ninstruction tuning. International Conference on Learning Representations, 2024.\\n[145] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model\\nfor graph computational problems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge\\nDiscovery and Data Mining, pages 353–364, 2024.\\n[146] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He.\\nJudgelrm: Large reasoning models as a judge, arXiv preprint arXiv:2504.00050, 2025. URL https:\\n//arxiv.org/abs/2504.00050v1.\\n[147] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,\\nYuhang Zhou, Te Gao, and Wangxiang Che.\\nTowards reasoning era: A survey of long chain-\\nof-thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL\\nhttps://arxiv.org/abs/2503.09567v3.\\n[148] Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan\\nJi, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and\\nWanxiang Che. Ai4research: A survey of artificial intelligence for scientific research, arXiv preprint\\narXiv:2507.01903, 2025. URL https://arxiv.org/abs/2507.01903.\\n[149] S\\nChen,\\nY\\nWang,\\nYF\\nWu,\\nand\\nQ\\nChen....\\nAdvancing\\ntool-augmented\\nlarge\\nlanguage\\nmodels:\\nIntegrating\\ninsights\\nfrom\\nerrors\\nin\\ninference\\ntrees.\\n2024.\\nURL\\nhttps://proceedings.neurips.cc/paper_files/paper/2024/hash/\\nc0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html.\\n[150] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\\nof large language models via positional interpolation, arXiv preprint arXiv:2306.15595, 2023. URL\\nhttps://arxiv.org/abs/2306.15595v2.\\n[151] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\\ncontrastive learning of visual representations. International Conference on Machine Learning, 2020.\\n[152] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:\\nDisentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res.,\\n2022.\\n[153] Yanda Chen, Ruiqi Zhong, Sheng Zha, G. Karypis, and He He. Meta-learning via language model\\nin-context tuning. Annual Meeting of the Association for Computational Linguistics, 2021.\\n[154] Yi Chen, JiaHao Zhao, and HaoHao Han. A survey on collaborative mechanisms between large and\\nsmall language models, arXiv preprint arXiv:2505.07460, 2025. URL https://arxiv.org/abs/\\n2505.07460v1.\\n70'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 70}, page_content='[155] Yixin Chen, Shuai Zhang, Boran Han, Tong He, and Bo Li. Camml: Context-aware multimodal\\nlearner for large models. Annual Meeting of the Association for Computational Linguistics, 2024.\\n[156] Z Chen, K Zhou, B Zhang, Z Gong, and WX Zhao.... Chatcot: Tool-augmented chain-of-thought\\nreasoning on chat-based large language models. 2023. URL https://arxiv.org/abs/2305.\\n14323.\\n[157] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo,\\nSongyang Zhang, Dahua Lin, Kai Chen, et al. T-eval: Evaluating the tool utilization capability step\\nby step. arXiv preprint arXiv:2312.14033, 2023.\\n[158] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao.\\nMindsearch: Mimicking human minds elicits deep ai searcher, arXiv preprint arXiv:2407.20183,\\n2024. URL https://arxiv.org/abs/2407.20183v1.\\n[159] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Haifang Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei\\nYin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms)in\\nlearning on graphs. SIGKDD Explorations, 2023.\\n[160] Zihan Chen, Song Wang, Zhen Tan, Xingbo Fu, Zhenyu Lei, Peng Wang, Huan Liu, Cong Shen, and\\nJundong Li. A survey of scaling in large language model reasoning, arXiv preprint arXiv:2504.02181,\\n2025. URL https://arxiv.org/abs/2504.02181v1.\\n[161] ZY Chen, S Shen, G Shen, and G Zhi.... Towards tool use alignment of large language models.\\n2024. URL https://aclanthology.org/2024.emnlp-main.82/.\\n[162] Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao,\\nJie Ma, Daoyu Wang, and Enhong Chen. A survey on knowledge-oriented retrieval-augmented gener-\\nation, arXiv preprint arXiv:2503.10677, 2025. URL https://arxiv.org/abs/2503.10677v2.\\n[163] Ning Cheng, Zhaohui Yan, Ziming Wang, Zhijie Li, Jiaming Yu, Zilong Zheng, Kewei Tu, Jinan Xu,\\nand Wenjuan Han. Potential and limitations of llms in capturing structured semantics: A case study\\non srl. International Conference on Intelligent Computing, 2024.\\n[164] Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang,\\nLing Chen, Qingwei Lin, Dongmei Zhang, et al. Call me when necessary: Llms can efficiently and\\nfaithfully reason over structured environments. In Association for Computational Linguistics 2024,\\npages 4275–4295, 2024.\\n[165] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.\\nLift yourself up:\\nRetrieval-augmented text generation with self memory. Neural Information Processing Systems, 2023.\\n[166] Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, and Xiang Li. Human cognition inspired rag\\nwith knowledge graph for complex problem solving, arXiv preprint arXiv:2503.06567, 2025. URL\\nhttps://arxiv.org/abs/2503.06567v1.\\n[167] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao\\nWang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based\\nintelligent agents: Definitions, methods, and prospects, arXiv preprint arXiv:2401.03428, 2024. URL\\nhttps://arxiv.org/abs/2401.03428v1.\\n71'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 71}, page_content='[168] Egor Cherepanov, Nikita Kachaev, A. Kovalev, and Aleksandr I. Panov. Memory, benchmark & robots: A\\nbenchmark for solving complex tasks with reinforcement learning, arXiv preprint arXiv:2502.10550,\\n2025. URL https://arxiv.org/abs/2502.10550v2.\\n[169] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building\\nproduction-ready ai agents with scalable long-term memory, arXiv preprint arXiv:2504.19413, 2025.\\nURL https://arxiv.org/abs/2504.19413.\\n[170] Yew Ken Chia, Lidong Bing, Soujanya Poria, and Luo Si. Relationprompt: Leveraging prompts to\\ngenerate synthetic data for zero-shot relation triplet extraction. Findings, 2022.\\n[171] Jihye Choi, Nils Palumbo, P. Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, and\\nDavid Page. Malade: Orchestration of llm-powered agents with retrieval augmented generation\\nfor pharmacovigilance, arXiv preprint arXiv:2408.01869, 2024. URL https://arxiv.org/abs/\\n2408.01869v1.\\n[172] K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós,\\nPeter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell,\\nand Adrian Weller. Rethinking attention with performers. International Conference on Learning\\nRepresentations, 2020.\\n[173] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu,\\nJing Liang, Philip S. Yu, and Qingsong Wen. Llm agents for education: Advances and applications,\\narXiv preprint arXiv:2503.11733, 2025. URL https://arxiv.org/abs/2503.11733v1.\\n[174] Zhixuan Chu, Huaiyu Guo, Xinyuan Zhou, Yijia Wang, Fei Yu, Hong Chen, Wanqing Xu, Xin Lu, Qing\\nCui, Longfei Li, Junqing Zhou, and Sheng Li. Data-centric financial large language models, arXiv\\npreprint arXiv:2310.17784, 2023. URL https://arxiv.org/abs/2310.17784v2.\\n[175] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang,\\nDhavan Shah, Junjie Hu, and Timothy T. Rogers. Simulating opinion dynamics with networks of\\nllm-based agents, arXiv preprint arXiv:2311.09618, 2024. URL https://arxiv.org/abs/2311.\\n09618.\\n[176] Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria\\nLin, James Glass, Shang-Wen Li, and Wen tau Yih. Selfcite: Self-supervised alignment for context\\nattribution in large language models, arXiv preprint arXiv:2502.09604, 2025. URL https://\\narxiv.org/abs/2502.09604v3.\\n[177] Julian Coda-Forno, Marcel Binz, Zeynep Akata, M. Botvinick, Jane X. Wang, and Eric Schulz.\\nMeta-in-context learning in large language models. Neural Information Processing Systems, 2023.\\n[178] Joao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, A. Paladugu, Pranav Setlur, Jiahe Jin, James P.\\nCallan, João Magalhães, Bruno Martins, and Chenyan Xiong. Deepresearchgym: A free, transparent,\\nand reproducible evaluation sandbox for deep research, arXiv preprint arXiv:2505.19253, 2025.\\nURL https://arxiv.org/abs/2505.19253v2.\\n[179] Emile Contal and Garrin McGoldrick. Ragsys: Item-cold-start recommender as rag system. IR-\\nRAG@SIGIR, 2024.\\n[180] Erica Coppolillo.\\nInjecting knowledge graphs into large language models, arXiv preprint\\narXiv:2505.07554, 2025. URL https://arxiv.org/abs/2505.07554v1.\\n72'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 72}, page_content='[181] R. P. Costa, R. Froemke, P. J. Sjöström, and Mark C. W. van Rossum. Unified pre- and postsynaptic\\nlong-term plasticity enables reliable and flexible learning. eLife, 2015.\\n[182] Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve:\\nScaling reasoning without scaling models, arXiv preprint arXiv:2504.18116, 2025. URL https:\\n//arxiv.org/abs/2504.18116v1.\\n[183] Michael Craig, Karla Butterworth, Jonna Nilsson, Colin J Hamilton, P. Gallagher, and T. Smulders.\\nHow does intentionality of encoding affect memory for episodic information? Learning & memory\\n(Cold Spring Harbor, N.Y.), 2016.\\n[184] crewAI Inc. crewai: Framework for orchestrating role-playing, autonomous ai agents. https:\\n//github.com/crewAIInc/crewAI, 2024. [Online; accessed 17-July-2025].\\n[185] A. Cruz, André V. dos Santos, R. Santiago, and B. Bedregal. A fuzzy semantic for bdi logic. Fuzzy\\nInformation and Engineering, 2021.\\n[186] Florin Cuconasu, Giovanni Trappolini, F. Siciliano, Simone Filice, Cesare Campagnano, Y. Maarek,\\nNicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems.\\nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval,\\n2024.\\n[187] Kai Cui, Anam Tahir, Gizem Ekinci, Ahmed Elshamanhory, Yannick Eich, Mengguang Li, and\\nH. Koeppl. A survey on large-population systems and scalable multi-agent reinforcement learning,\\narXiv preprint arXiv:2209.03859, 2022. URL https://arxiv.org/abs/2209.03859v1.\\n[188] Yuanning Cui, Zequn Sun, and Wei Hu. A prompt-based knowledge graph foundation model for\\nuniversal in-context reasoning. In Advances in Neural Information Processing Systems, 2024.\\n[189] Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, and Xiaofang Zhou. En-\\nhancing tool learning in large language models with hierarchical error checklists, arXiv preprint\\narXiv:2506.00042, 2025. URL https://arxiv.org/abs/2506.00042v1.\\n[190] C. Curto, A. Degeratu, and V. Itskov. Flexible memory networks. Bulletin of Mathematical Biology,\\n2010.\\n[191] Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, and Yao Cheng. G-sap:\\nGraph-based structure-aware prompt learning over heterogeneous knowledge for commonsense\\nreasoning. International Conference on Multimedia Retrieval, 2024.\\n[192] Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang\\nTang, and Caihua Shan. How do large language models understand graph patterns? a benchmark for\\ngraph pattern comprehension, arXiv preprint arXiv:2410.05298v2, 2024. URL https://arxiv.\\norg/abs/2410.05298v2.\\n[193] Fatemeh Daneshfar and H. Bevrani. Multi-agent systems in control engineering: a survey. arXiv\\npreprint, 2009.\\n[194] Yufan Dang, Cheng Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng\\nYang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, and Maosong Sun. Multi-agent\\ncollaboration via evolving orchestration, arXiv preprint arXiv:2505.19591, 2025. URL https:\\n//arxiv.org/abs/2505.19591v1.\\n73'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 73}, page_content='[195] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. International\\nConference on Learning Representations, 2023.\\n[196] Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, and Christopher R’e. Flashattention: Fast and\\nmemory-efficient exact attention with io-awareness. Neural Information Processing Systems, 2022.\\n[197] D Das, D Banerjee, S Aditya, and A Kulkarni. Mathsensei: a tool-augmented large language model\\nfor mathematical reasoning. 2024. URL https://arxiv.org/abs/2402.17231.\\n[198] Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie\\nLozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, and Pin-Yu Chen.\\nLarimar: Large language models with episodic memory control, arXiv preprint arXiv:2403.11901,\\n2024. URL https://arxiv.org/abs/2403.11901.\\n[199] Adrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen. On meta-prompting, arXiv preprint\\narXiv:2312.06562, 2023. URL https://arxiv.org/abs/2312.06562v3.\\n[200] Ramandeep Singh Dehal, Mehak Sharma, and Enayat Rajabi. Knowledge graphs and their reciprocal\\nrelationship with large language models. Machine Learning and Knowledge Extraction, 2025.\\n[201] Mauricio R. Delgado, V. Stenger, and J. Fiez. Motivation-dependent responses in the human caudate\\nnucleus. Cerebral Cortex, 2004.\\n[202] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\\nMind2web: Towards a generalist agent for the web. Neural Information Processing Systems, 2023.\\n[203] Yang Deng, Wenqiang Lei, Hongru Wang, and Tat seng Chua. Prompting and evaluating large lan-\\nguage models for proactive dialogues: Clarification, target-guided, and non-collaboration. Conference\\non Empirical Methods in Natural Language Processing, 2023.\\n[204] Yang Deng, An Zhang, Yankai Lin, Xu Chen, Ji-Rong Wen, and Tat-Seng Chua. Large language\\nmodel powered agents in the web. The Web Conference, 2024.\\n[205] Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. On the\\nmulti-turn instruction following for conversational web agents. Annual Meeting of the Association for\\nComputational Linguistics, 2024.\\n[206] Brouillet Denis and Versace Rémy. The nature of the traces and the dynamics of memory. Psychology\\nand Behavioral Sciences, 2019.\\n[207] Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, M. Worring, and Yuki\\nAsano. Self-supervised open-ended classification with small visual language models, arXiv preprint\\narXiv:2310.00500, 2023. URL https://arxiv.org/abs/2310.00500v2.\\n[208] Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, and Sutanay Choudhury.\\nGlam: Fine-tuning large language models for domain knowledge graph alignment via neighborhood\\npartitioning and generative subgraph encoding. AAAI Spring Symposia, 2024.\\n[209] Rushali Deshmukh, Rutuj Raut, Mayur Bhavsar, Sanika Gurav, and Y. Patil. Optimizing human-ai\\ninteraction: Innovations in prompt engineering. 2025 3rd International Conference on Intelligent\\nData Communication Technologies and Internet of Things (IDCIoT), 2025.\\n74'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 74}, page_content='[210] Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, and Rebecca\\nQian. Trail: Trace reasoning and agentic issue localization, arXiv preprint arXiv:2505.08638, 2025.\\nURL https://arxiv.org/abs/2505.08638v3.\\n[211] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding. North American Chapter of the Association\\nfor Computational Linguistics, 2019.\\n[212] Dhruv Dhamani and Mary Lou Maher. The tyranny of possibilities in the design of task-oriented llm\\nsystems: A scoping survey, arXiv preprint arXiv:2312.17601, 2023. URL https://arxiv.org/\\nabs/2312.17601v1.\\n[213] Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, and Gareth Vanderpool.\\nContextual memory reweaving in large language models using layered latent state reconstruction,\\narXiv preprint arXiv:2502.02046, 2025. URL https://arxiv.org/abs/2502.02046v2.\\n[214] Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen,\\nand Xueqi Chen. Toolcoder: A systematic code-empowered tool learning framework for large\\nlanguage models, arXiv preprint arXiv:2502.11404, 2025. URL https://arxiv.org/abs/2502.\\n11404v2.\\n[215] Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng\\nZhao, and Yasha Wang. 3ds: Decomposed difficulty data selection’s case study on llm medical\\ndomain adaptation. 2024.\\n[216] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.\\nLongnet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv preprint, 2023.\\n[217] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang,\\nZhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models:\\nAn algorithmic survey, arXiv preprint arXiv:2312.00678, 2023. URL https://arxiv.org/abs/\\n2312.00678v2.\\n[218] Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,\\nand Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. International\\nConference on Machine Learning, 2024.\\n[219] Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui,\\nQi Zhang, and Xuanjing Huang. Mitigating tail narrowing in llm self-improvement via socratic-guided\\nsampling. North American Chapter of the Association for Computational Linguistics, 2024.\\n[220] Christian Djeffal. Reflexive prompt engineering: A framework for responsible prompt engineering\\nand ai interaction design. Conference on Fairness, Accountability and Transparency, 2025.\\n[221] G Dong, Y Chen, X Li, J Jin, H Qian, and Y Zhu.... Tool-star: Empowering llm-brained multi-tool\\nreasoner via reinforcement learning. 2025. URL https://arxiv.org/abs/2505.16410.\\n[222] Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu,\\nZhuoma Gongque, Keqing He, Zechen Wang, and Weiran Xu. Revisit input perturbation problems\\nfor llms: A unified robustness evaluation framework for noisy slot filling task. Natural Language\\nProcessing and Chinese Computing, 2023.\\n75'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 75}, page_content='[223] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui\\nZhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via\\nreinforcement learning. arXiv preprint, 2025.\\n[224] Kaiwen Dong. Large language model applied in multi-agent systema survey. Applied and Computa-\\ntional Engineering, 2024.\\n[225] Peijie Dong, Zhenheng Tang, Xiang-Hong Liu, Lujun Li, Xiaowen Chu, and Bo Li. Can compressed\\nllms truly act? an empirical evaluation of agentic capabilities in llm compression, arXiv preprint\\narXiv:2505.19433, 2025. URL https://arxiv.org/abs/2505.19433v2.\\n[226] Vicky Dong, Hao Yu, and Yao Chen. Graph-augmented relation extraction model with llms-generated\\nsupport document, arXiv preprint arXiv:2410.23452, 2024. URL https://arxiv.org/abs/\\n2410.23452v1.\\n[227] Xiangjue Dong, Maria Teleki, and James Caverlee. A survey on llm inference-time self-improvement,\\narXiv preprint arXiv:2412.14352, 2024. URL https://arxiv.org/abs/2412.14352v1.\\n[228] Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang.\\nAdvanced rag models with graph structures: Optimizing complex knowledge reasoning and text gen-\\neration. 2024 5th International Symposium on Computer Engineering and Intelligent Communications\\n(ISCEIC), 2024.\\n[229] Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, and\\nJi-Rong Wen. Exploring context window of large language models via decomposed positional vectors.\\nNeural Information Processing Systems, 2024.\\n[230] Ehsan Doostmohammadi and Marco Kuhlmann. Studying the role of input-neighbor overlap in\\nretrieval-augmented language models training efficiency, arXiv preprint arXiv:2505.14309, 2025.\\nURL https://arxiv.org/abs/2505.14309v1.\\n[231] Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan Nekouei, H. Zarrabi,\\nReza Keypour, Apostolos I. Rikos, and K. H. Johansson. Survey of distributed algorithms for resource\\nallocation over multi-agent systems, arXiv preprint arXiv:2401.15607, 2024. URL https://arxiv.\\norg/abs/2401.15607v1.\\n[232] A. Dorri, S. Kanhere, and R. Jurdak. Multi-agent systems: A survey. IEEE Access, 2018.\\n[233] Mauro Dragone. Component & service-based agent systems: Self-osgi. International Conference on\\nAgents and Artificial Intelligence, 2012.\\n[234] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom\\nMarty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web\\nagents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine\\nHeller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings\\nof the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine\\nLearning Research, pages 11642–11662. PMLR, 21–27 Jul 2024. URL https://proceedings.\\nmlr.press/v235/drouin24a.html.\\n[235] Hung Du, Srikanth Thudumu, Rajesh Vasa, and K. Mouzakis. A survey on context-aware multi-agent\\nsystems: Techniques, challenges and future directions, arXiv preprint arXiv:2402.01968, 2024. URL\\nhttps://arxiv.org/abs/2402.01968v2.\\n76'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 76}, page_content='[236] Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Çelebi, Michael Auli, Ves Stoyanov,\\nand Alexis Conneau. Self-training improves pre-training for natural language understanding. North\\nAmerican Chapter of the Association for Computational Linguistics, 2020.\\n[237] Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with\\nmixture-of-memories, arXiv preprint arXiv:2502.13685, 2025. URL https://arxiv.org/abs/\\n2502.13685v2.\\n[238] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: A\\ncomprehensive benchmark for deep research agents, arXiv preprint arXiv:2506.11763, 2025. URL\\nhttps://arxiv.org/abs/2506.11763v1.\\n[239] Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, and Liang He. A\\nsurvey on the optimization of large language model-based agents, arXiv preprint arXiv:2503.12434,\\n2025. URL https://arxiv.org/abs/2503.12434v1.\\n[240] Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang,\\nand Kam-Fai Wong. Perltqa: A personal long-term memory dataset for memory classification,\\nretrieval, and synthesis in question answering, arXiv preprint arXiv:2402.16288, 2024. URL https:\\n//arxiv.org/abs/2402.16288v1.\\n[241] Hanqi Duan, Yao Cheng, Jianxiang Yu, and Xiang Li. Can large language models act as ensembler\\nfor multi-gnns?, arXiv preprint arXiv:2410.16822, 2024. URL https://arxiv.org/abs/2410.\\n16822v2.\\n[242] Peitong Duan, Chin yi Chen, Bjoern Hartmann, and Yang Li. Visual prompting with iterative\\nrefinement for design critique generation, arXiv preprint arXiv:2412.16829, 2024. URL https:\\n//arxiv.org/abs/2412.16829v2.\\n[243] Brown Ebouky, A. Bartezzaghi, and Mattia Rigotti. Eliciting reasoning in language models with\\ncognitive tools, arXiv preprint arXiv:2506.12115, 2025. URL https://arxiv.org/abs/2506.\\n12115v1.\\n[244] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\\nDasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A graph\\nrag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\n[245] Candace Edwards.\\nHybrid context retrieval augmented generation pipeline: Llm-augmented\\nknowledge graphs and vector database for accreditation reporting assistance, arXiv preprint\\narXiv:2405.15436, 2024. URL https://arxiv.org/abs/2405.15436v1.\\n[246] Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. A survey of agent interoperabil-\\nity protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent\\nprotocol (a2a), and agent network protocol (anp), arXiv preprint arXiv:2505.02279, 2025. URL\\nhttps://arxiv.org/abs/2505.02279v2.\\n[247] Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema\\nAmershi. Interactive debugging and steering of multi-agent ai systems. International Conference on\\nHuman Factors in Computing Systems, 2025.\\n77'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 77}, page_content='[248] Lutfi Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman\\nHooper, G. Anumanchipalli, Kurt Keutzer, and A. Gholami. Tinyagent: Function calling at the edge.\\nConference on Empirical Methods in Natural Language Processing, 2024.\\n[249] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. An empirical categorization of\\nprompting techniques for large language models: A practitioner’s guide. arXiv preprint, 2024.\\n[250] Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, S. Balasub-\\nramanian, Parsa Hosseini, and S. Feizi. Gaming tool preferences in agentic llms, arXiv preprint\\narXiv:2505.18135, 2025. URL https://arxiv.org/abs/2505.18135v1.\\n[251] Linxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\\nagents with internet-scale knowledge. Neural Information Processing Systems, 2022.\\n[252] Siqi Fan, Xiusheng Huang, Yiqun Yao, Xuezhi Fang, Kang Liu, Peng Han, Shuo Shang, Aixin Sun, and\\nYequan Wang. If an llm were a character, would it know its own story? evaluating lifelong learning in\\nllms, arXiv preprint arXiv:2503.23514, 2025. URL https://arxiv.org/abs/2503.23514v1.\\n[253] Wenqi Fan, Yujuan Ding, Liang bo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua,\\nand Qing Li. A survey on rag meeting llms: Towards retrieval-augmented large language models.\\nKnowledge Discovery and Data Mining, 2024.\\n[254] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-\\naugmented multimodal agent for video understanding, arXiv preprint arXiv:2403.11481, 2024. URL\\nhttps://arxiv.org/abs/2403.11481v2.\\n[255] Hongchao Fang and Pengtao Xie. An end-to-end contrastive self-supervised learning framework for\\nlanguage understanding. Transactions of the Association for Computational Linguistics, 2022.\\n[256] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive\\nself-supervised learning for language understanding, arXiv preprint arXiv:2005.12766, 2020. URL\\nhttps://arxiv.org/abs/2005.12766v2.\\n[257] Junfeng Fang, Zijun Yao, Ruipeng Wang, Haokai Ma, Xiang Wang, and Tat-Seng Chua.\\nWe\\nshould identify and mitigate third-party safety risks in mcp-powered agent systems, arXiv preprint\\narXiv:2506.13666, 2025. URL https://arxiv.org/abs/2506.13666v1.\\n[258] Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xinrun Du, Ningxuan Lu, Ge Zhang, and Qingkun Tang.\\nKarpa: A training-free method of adapting knowledge graph as references for large language model’s\\nreasoning path aggregation. arXiv preprint, 2024.\\n[259] Wei-Wen Fang, Yang Zhang, Kaizhi Qian, James Glass, and Yada Zhu. Play2prompt: Zero-shot tool\\ninstruction optimization for llm agents via tool play, arXiv preprint arXiv:2503.14432, 2025. URL\\nhttps://arxiv.org/abs/2503.14432v2.\\n[260] Yi Fang, Dongzhe Fan, D. Zha, and Qiaoyu Tan. Gaugllm: Improving graph contrastive learning for\\ntext-attributed graphs with large language models. Knowledge Discovery and Data Mining, 2024.\\n[261] Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, and Jiawei Han. Graphgpt-o: Synergistic\\nmultimodal comprehension and generation on graphs. arXiv preprint, 2025.\\n78'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 78}, page_content='[262] Bahare Fatemi, Jonathan J. Halcrow, and Bryan Perozzi. Talk like a graph: Encoding graphs for large\\nlanguage models. International Conference on Learning Representations, 2023.\\n[263] George Fatouros, Georgios Makridis, George Kousiouris, John Soldatos, A. Tsadimas, and D. Kyriazis.\\nTowards conversational ai for human-machine collaborative mlops, arXiv preprint arXiv:2504.12477,\\n2025. URL https://arxiv.org/abs/2504.12477v1.\\n[264] M. Fauth, F. Wörgötter, and Christian Tetzlaff. Formation and maintenance of robust long-term\\ninformation storage in the presence of synaptic turnover. bioRxiv, 2015.\\n[265] Zahra Fayyaz, Aya Altamimi, Sen Cheng, and Laurenz Wiskott. A model of semantic completion in\\ngenerative episodic memory. Neural Computation, 2021.\\n[266] Xiang Fei, Xiawu Zheng, and Hao Feng. Mcp-zero: Proactive toolchain construction for llm agents\\nfrom scratch. arXiv preprint, 2025.\\n[267] Philip Feldman, James R. Foulds, and Shimei Pan. Ragged edges: The double-edged sword of\\nretrieval-augmented chatbots, arXiv preprint arXiv:2403.01193, 2024. URL https://arxiv.\\norg/abs/2403.01193v3.\\n[268] Aosong Feng, Rex Ying, and L. Tassiulas. Long sequence modeling with attention tensorization:\\nFrom sequence to tensor learning. Conference on Empirical Methods in Natural Language Processing,\\n2024.\\n[269] Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du,\\nZhi-Hua Zhou, Yubin Xia, and Haibo Chen. Get experience from practice: Llm agents with record &\\nreplay, arXiv preprint arXiv:2505.17716, 2025. URL https://arxiv.org/abs/2505.17716v1.\\n[270] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,\\nJinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, arXiv\\npreprint arXiv:2504.11536, 2025. URL https://arxiv.org/abs/2504.11536v2.\\n[271] Kaituo Feng, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, and Guoren Wang. Keypoint-based\\nprogressive chain-of-thought distillation for llms. International Conference on Machine Learning,\\n2024.\\n[272] Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, and M. O. Ahmed. Constant memory\\nattention block, arXiv preprint arXiv:2306.12599, 2023. URL https://arxiv.org/abs/2306.\\n12599v1.\\n[273] Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-hop\\nrelational reasoning for knowledge-aware question answering. Conference on Empirical Methods in\\nNatural Language Processing, 2020.\\n[274] Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, and Yue Gao. Hypergraph\\nfoundation model, arXiv preprint arXiv:2503.01203v1, 2025. URL https://arxiv.org/abs/\\n2503.01203v1.\\n[275] Chrisantha Fernando, Dylan Banarse, H. Michalewski, Simon Osindero, and Tim Rocktäschel. Prompt-\\nbreeder: Self-referential self-improvement via prompt evolution. International Conference on Machine\\nLearning, 2023.\\n79'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 79}, page_content='[276] Tharindu Fernando, Simon Denman, A. Mcfadyen, S. Sridharan, and C. Fookes. Tree memory\\nnetworks for modelling long-term temporal dependencies. Neurocomputing, 2017.\\n[277] M. Ferrag, Norbert Tihanyi, and M. Debbah. From llm reasoning to autonomous ai agents: A\\ncomprehensive review, arXiv preprint arXiv:2504.19678, 2025. URL https://arxiv.org/abs/\\n2504.19678v1.\\n[278] M. Ferrag, Norbert Tihanyi, and M. Debbah. Reasoning beyond limits: Advances and open prob-\\nlems for llms, arXiv preprint arXiv:2503.22732, 2025. URL https://arxiv.org/abs/2503.\\n22732v1.\\n[279] Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jurij Leskovec, Christopher R’e, and\\nSebastian Thrun. Context-aware meta-learning. International Conference on Learning Representations,\\n2023.\\n[280] Tim Finin, Richard Fritzson, Donald P McKay, Robin McEntire, et al. Kqml-a language and protocol\\nfor knowledge and information exchange. In 13th Int. Distributed Artificial Intelligence Workshop,\\npages 93–103, 1994.\\n[281] Chelsea Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep\\nnetworks. International Conference on Machine Learning, 2017.\\n[282] Paolo Finotelli and Francis Eustache. Mathematical modeling of human memory. Frontiers in\\nPsychology, 2023.\\n[283] Ferdinando Fioretto, Enrico Pontelli, and W. Yeoh. Distributed constraint optimization problems and\\napplications: A survey. Journal of Artificial Intelligence Research, 2016.\\n[284] Meire Fortunato, Melissa Tan, Ryan Faulkner, S. Hansen, Adrià Puigdomènech Badia, Gavin Buttimore,\\nCharlie Deck, Joel Z. Leibo, and C. Blundell. Generalization of reinforcement learners with working\\nand episodic memory. Neural Information Processing Systems, 2019.\\n[285] Samy Foudil, Claire Pleche, and E. Macaluso. Memory for spatio-temporal contextual details during\\nthe retrieval of naturalistic episodes. Scientific Reports, 2021.\\n[286] Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lam-\\npouras, Haitham Bou-Ammar, and Jun Wang. Human-like episodic memory for infinite context llms,\\narXiv preprint arXiv:2407.09450, 2024. URL https://arxiv.org/abs/2407.09450.\\n[287] Quentin Fournier, G. Caron, and D. Aloise. A practical survey on faster and lighter transformers.\\nACM Computing Surveys, 2021.\\n[288] Luca Franceschi, P. Frasconi, Saverio Salzo, Riccardo Grazzi, and M. Pontil. Bilevel programming\\nfor hyperparameter optimization and meta-learning. International Conference on Machine Learning,\\n2018.\\n[289] Eduard Frankford, Daniel Crazzolara, Clemens Sauerwein, Michael Vierhauser, and Ruth Breu.\\nRequirements for an online integrated development environment for automated programming\\nassessment systems. International Conference on Computer Supported Education, 2024.\\n80'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 80}, page_content='[290] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu\\nZhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li,\\nTong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever\\ncomprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint, 2024.\\n[291] Honghao Fu, Yilang Shen, Yuxuan Liu, Jingzhong Li, and Xiang Zhang. Sgcn: a multi-order neigh-\\nborhood feature fusion landform classification method based on superpixel and graph convolutional\\nnetwork. International Journal of Applied Earth Observation and Geoinformation, 122:103441, 2023.\\n[292] Honghao Fu, Yufei Wang, Wenhan Yang, Alex C Kot, and Bihan Wen. Dp-iqa: Utilizing diffusion\\nprior for blind image quality assessment in the wild. 2024.\\n[293] Honghao Fu, Hao Wang, Jing Jih Chin, and Zhiqi Shen. Brainvis: Exploring the bridge between brain\\nand visual signals via image reconstruction. In ICASSP 2025-2025 IEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), pages 1–5. IEEE, 2025.\\n[294] Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang. Ras-eval: A comprehensive benchmark for security\\nevaluation of llm agents in real-world environments. arXiv preprint, 2025.\\n[295] Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu,\\nXuetao Wei, Tong Xu, and Xiangyu Zhao. Sliding window attention training for efficient large\\nlanguage models, arXiv preprint arXiv:2502.18845, 2025. URL https://arxiv.org/abs/2502.\\n18845v2.\\n[296] Stefano Fusi. Memory capacity of neural network models, arXiv preprint arXiv:2108.07839, 2021.\\nURL https://arxiv.org/abs/2108.07839v2.\\n[297] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrieval-\\naugmented generation. arXiv preprint, 2025.\\n[298] Kanishk Gandhi, Gala Stojnic, B. Lake, and M. Dillon. Baby intuitions benchmark (bib): Discerning\\nthe goals, preferences, and actions of others. Neural Information Processing Systems, 2021.\\n[299] Anish Ganguli, Prabal Deb, and Debleena Banerjee. Mark: Memory augmented refinement of knowl-\\nedge, arXiv preprint arXiv:2505.05177, 2025. URL https://arxiv.org/abs/2505.05177v1.\\n[300] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li.\\nLarge language models empowered agent-based modeling and simulation: A survey and perspectives.\\nHumanities and Social Sciences Communications, 2023.\\n[301] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin,\\nand Yong Li. S3: Social-network simulation system with large language model-empowered agents,\\narXiv preprint arXiv:2307.14984, 2025. URL https://arxiv.org/abs/2307.14984.\\n[302] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents, arXiv\\npreprint arXiv:2404.09982, 2024. URL https://arxiv.org/abs/2404.09982v2.\\n[303] L Gao, A Madaan, S Zhou, and U Alon.... Pal: Program-aided language models. 2023. URL\\nhttps://proceedings.mlr.press/v202/gao23f.\\n[304] Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. Precise zero-shot dense retrieval without\\nrelevance labels. Annual Meeting of the Association for Computational Linguistics, 2022.\\n81'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 81}, page_content='[305] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\\nGraham Neubig. Pal: Program-aided language models. International Conference on Machine Learning,\\n2022.\\n[306] Shuzheng Gao, Xinjie Wen, Cuiyun Gao, Wenxuan Wang, and Michael R. Lyu. What makes good in-\\ncontext demonstrations for code intelligence tasks with llms? International Conference on Automated\\nSoftware Engineering, 2023.\\n[307] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\\nlearners. Annual Meeting of the Association for Computational Linguistics, 2021.\\n[308] Weiguo Gao. Mep: Multiple kernel learning enhancing relative positional encoding length extrapola-\\ntion, arXiv preprint arXiv:2403.17698, 2024. URL https://arxiv.org/abs/2403.17698v1.\\n[309] Xian Gao, Zongyun Zhang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Graph of ai ideas: Leveraging\\nknowledge graphs and llms for ai research idea generation, arXiv preprint arXiv:2503.08549, 2025.\\nURL https://arxiv.org/abs/2503.08549v1.\\n[310] Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: A multi-dimensional\\nbenchmark for evaluating tool use capabilities in large language models. arXiv preprint, 2025.\\n[311] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\\nMeng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey,\\narXiv preprint arXiv:2312.10997, 2023. URL https://arxiv.org/abs/2312.10997v5.\\n[312] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. Modular rag: Transforming rag systems\\ninto lego-like reconfigurable frameworks, arXiv preprint arXiv:2407.21059, 2024. URL https:\\n//arxiv.org/abs/2407.21059v1.\\n[313] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing rag and\\nreasoning: A systematic review, arXiv preprint arXiv:2504.15909, 2025. URL https://arxiv.\\norg/abs/2504.15909v2.\\n[314] Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, and Stan Z. Li. A graph is worth k\\nwords: Euclideanizing graph using pure transformer. International Conference on Machine Learning,\\n2024.\\n[315] Itai Gat, Idan Schwartz, and A. Schwing. Perceptual score: What data modalities does your model\\nperceive? Neural Information Processing Systems, 2021.\\n[316] Itai Gat, Felix Kreuk, Tu Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux, and\\nYossi Adi. Augmentation invariant discrete representation for generative spoken language modeling.\\nInternational Workshop on Spoken Language Translation, 2022.\\n[317] Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. International Conference on Learning Representations, 2023.\\n[318] Yuyao Ge, Zhongguo Yang, Lizhe Chen, Yiming Wang, and Chengyang Li. Attack based on data: a\\nnovel perspective to attack sensitive points directly. Cybersecurity, 6(1):43, 2023.\\n[319] Yuyao Ge, Shenghua Liu, Baolong Bi, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, and Xueqi\\nCheng. Can graph descriptive order affect solving graph problems with llms? ACL 2025, 2024.\\n82'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 82}, page_content='[320] Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, Baolong Bi, and Xueqi Cheng. Innate\\nreasoning is not enough: In-context learning enhances reasoning large language models with less\\noverthinking. 2025.\\n[321] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, and\\nLinjian Mo. Breaking the length barrier: Llm-enhanced ctr prediction in long textual user behaviors.\\nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval,\\n2024.\\n[322] Hejia Geng, Boxun Xu, and Peng Li. Upar: A kantian-inspired prompting framework for enhancing\\nlarge language model capabilities, arXiv preprint arXiv:2310.01441, 2023. URL https://arxiv.\\norg/abs/2310.01441v2.\\n[323] Antonios Georgiou, M. Katkov, and M. Tsodyks. Retroactive interference model of forgetting. Journal\\nof Mathematical Neuroscience, 2021.\\n[324] S. Gershman, A. Schapiro, A. Hupbach, and K. Norman. Neural context reinstatement predicts\\nmemory misattribution. Journal of Neuroscience, 2013.\\n[325] Mor Geva, R. Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are\\nkey-value memories. Conference on Empirical Methods in Natural Language Processing, 2020.\\n[326] Mor Geva, Avi Caciularu, Ke Wang, and Yoav Goldberg. Transformer feed-forward layers build\\npredictions by promoting concepts in the vocabulary space. Conference on Empirical Methods in\\nNatural Language Processing, 2022.\\n[327] Arda Gezdur and J. Bhattacharjya. Innovators and transformers: enhancing supply chain employee\\ntraining with an innovative application of a large language model. International Journal of Physical\\nDistribution & Logistics Management, 2025.\\n[328] Alireza Ghafarollahi and Markus J. Buehler. Protagents: protein discovery via large language model\\nmulti-agent collaborations combining physics and machine learning. Digital Discovery, 2024.\\n[329] Abdellah Ghassel, Ian Robinson, Gabriel Tanase, Hal Cooper, Bryan Thompson, Zhen Han, V. Ioanni-\\ndis, Soji Adeshina, and H. Rangwala. Hierarchical lexical graph for enhanced multi-hop retrieval,\\narXiv preprint arXiv:2506.08074, 2025. URL https://arxiv.org/abs/2506.08074v1.\\n[330] S. Ghetti and S. Bunge. Neural changes underlying the development of episodic memory during\\nmiddle childhood. Developmental Cognitive Neuroscience, 2012.\\n[331] D. Ghica. Function interface models for hardware compilation: Types, signatures, protocols, arXiv\\npreprint arXiv:0907.0749, 2009. URL https://arxiv.org/abs/0907.0749v1.\\n[332] Tyler Giallanza, Declan Campbell, and Jonathan D. Cohen. Toward the emergence of intelligent\\ncontrol: Episodic generalization and optimization. Open Mind, 2024.\\n[333] In Gim, Seung seob Lee, and Lin Zhong.\\nAsynchronous llm function calling, arXiv preprint\\narXiv:2412.07017, 2024. URL https://arxiv.org/abs/2412.07017v1.\\n[334] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\\nRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan\\nUesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory\\n83'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 83}, page_content='Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas\\nFernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis\\nHassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of\\ndialogue agents via targeted human judgements, arXiv preprint arXiv:2209.14375, 2022. URL\\nhttps://arxiv.org/abs/2209.14375.\\n[335] D. Godden and A. Baddeley. Context-dependent memory in two natural environments: on land and\\nunderwater. arXiv preprint, 1975.\\n[336] Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, and Hui Song. Privacy policy\\nanalysis through prompt engineering for llms, arXiv preprint arXiv:2409.14879, 2024. URL https:\\n//arxiv.org/abs/2409.14879v1.\\n[337] Yaroslav Golubev, Zarina Kurbatova, E. Alomar, T. Bryksin, and Mohamed Wiem Mkaouer. One\\nthousand and one stories: a large-scale survey of software refactoring. ESEC/SIGSOFT FSE, 2021.\\n[338] Alan M Gordon, Jesse Rissman, Roozbeh Kiani, and Anthony D Wagner. Cortical reinstatement\\nmediates the relationship between content-specific encoding activity and subsequent recollection\\ndecisions. Cerebral Cortex, 2014.\\n[339] E. Gordon and B. Logan. Managing goals and resources in dynamic environments. arXiv preprint,\\n2005.\\n[340] Z Gou, Z Shao, Y Gong, Y Shen, and Y Yang.... Critic: Large language models can self-correct with\\ntool-interactive critiquing. 2023. URL https://arxiv.org/abs/2305.11738.\\n[341] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan,\\nand Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving.\\nInternational Conference on Learning Representations, 2023.\\n[342] Alex Graves, Abdel rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\\nrecurrent neural networks. IEEE International Conference on Acoustics, Speech, and Signal Processing,\\n2013.\\n[343] Ekaterina Grishina, Mikhail Gorbunov, and Maxim Rakhuba. Procrustesgpt: Compressing llms with\\nstructured matrices and orthogonal transformations, arXiv preprint arXiv:2506.02818, 2025. URL\\nhttps://arxiv.org/abs/2506.02818v1.\\n[344] Sven Gronauer and K. Diepold. Multi-agent deep reinforcement learning: a survey. Artificial\\nIntelligence Review, 2021.\\n[345] C. Gros. Complex and adaptive dynamical systems, arXiv preprint arXiv:0807.4838, 2008. URL\\nhttps://arxiv.org/abs/0807.4838v3.\\n[346] Albert Gu, Karan Goel, and Christopher R’e. Efficiently modeling long sequences with structured\\nstate spaces. International Conference on Learning Representations, 2021.\\n[347] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization\\nof diagonal state space models. Neural Information Processing Systems, 2022.\\n[348] Jian Gu, Chunyang Chen, and A. Aleti. Vocabulary-defined semantics: Latent space clustering for\\nimproving in-context learning, arXiv preprint arXiv:2401.16184, 2024. URL https://arxiv.\\norg/abs/2401.16184v6.\\n84'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 84}, page_content='[349] Yongli Gu, Xiang Yan, Hanlin Qin, Naveed Akhtar, Shuai Yuan, Honghao Fu, Shuowen Yang, and\\nAjmal Mian. Hdtcnet: A hybrid-dimensional convolutional network for multivariate time series\\nclassification. Pattern Recognition, page 111837, 2025.\\n[350] Zhuohan Gu, Jiayi Yao, Kuntai Du, and Junchen Jiang. Llmsteer: Improving long-context llm\\ninference by steering attention on reused contexts, arXiv preprint arXiv:2411.13009, 2024. URL\\nhttps://arxiv.org/abs/2411.13009v2.\\n[351] Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian guang Lou. Evaluating\\nllm-based agents for multi-turn conversations: A survey, arXiv preprint arXiv:2503.22458, 2025.\\nURL https://arxiv.org/abs/2503.22458v1.\\n[352] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language\\ndescriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024.\\nURL https://arxiv.org/abs/2406.13250v1.\\n[353] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin,\\nFeng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei\\nYe, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek\\nRathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang\\nXu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby,\\nAndrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf,\\nChinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\\nFang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P.\\nBigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet\\nSingh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell,\\nMeng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi,\\nRamsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen\\nMa, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek\\nKumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun\\nMeng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin,\\nArsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang\\nYang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen\\nYang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho,\\nNikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam\\nXu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui,\\nVivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu,\\nYang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models,\\narXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1.\\n[354] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph\\nstructured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066,\\n2023. URL https://arxiv.org/abs/2305.15066v2.\\n[355] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu.\\nEmpowering working memory for large language model agents, arXiv preprint arXiv:2312.17259,\\n2024. URL https://arxiv.org/abs/2312.17259.\\n85'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 85}, page_content='[356] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges.\\nInternational Joint Conference on Artificial Intelligence, 2024.\\n[357] Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, and Yisen Wang. G1: Teaching llms to reason on\\ngraphs with reinforcement learning. arXiv preprint arXiv:2505.18499, 2025.\\n[358] Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, and Zhuosheng Zhang. Atomic-\\nto-compositional generalization for mobile agents with a new benchmark and scheduling system,\\narXiv preprint arXiv:2506.08972, 2025. URL https://arxiv.org/abs/2506.08972v1.\\n[359] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun,\\nand Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large\\nlanguage models. Annual Meeting of the Association for Computational Linguistics, 2024.\\n[360] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-\\naugmented generation, arXiv preprint arXiv:2410.05779, 2024. URL https://arxiv.org/abs/\\n2410.05779v3.\\n[361] Sharut Gupta, Chenyu Wang, Yifei Wang, T. Jaakkola, and Stefanie Jegelka. In-context symmetries:\\nSelf-supervised learning through contextual world models. Neural Information Processing Systems,\\n2024.\\n[362] Tanmay Gupta, Luca Weihs, and Aniruddha Kembhavi. Codenav: Beyond tool-use to using real-world\\ncodebases with llm agents, arXiv preprint arXiv:2406.12276, 2024. URL https://arxiv.org/\\nabs/2406.12276v1.\\n[363] I Gur, H Furuta, A Huang, M Safdari, and Y Matsuo.... A real-world webagent with planning, long\\ncontext understanding, and program synthesis. 2023. URL https://arxiv.org/abs/2307.\\n12856.\\n[364] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, D. Eck, and Aleksandra\\nFaust. A real-world webagent with planning, long context understanding, and program synthesis.\\nInternational Conference on Learning Representations, 2023.\\n[365] Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrián Tormos, Daniel Hinjos,\\nPablo Bernabeu Perez, Anna Arias-Duart, Pablo A. Martin-Torres, Lucia Urcelay-Ganzabal, Marta\\nGonzalez-Mallo, S. Álvarez Napagao, Eduard Ayguad’e-Parra, and Ulises Cortés Dario Garcia-Gasulla.\\nAloe: A family of fine-tuned open healthcare llms, arXiv preprint arXiv:2405.01886, 2024. URL\\nhttps://arxiv.org/abs/2405.01886v1.\\n[366] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro-\\nbiologically inspired long-term memory for large language models. Neural Information Processing\\nSystems, 2024.\\n[367] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\\naugmented language model pre-training. International Conference on Machine Learning, 2020.\\n[368] K. Gödel, B. Meltzer, and R. Schlegel. On formally undecidable propositions of principia mathematica\\nand related systems. arXiv preprint, 1966.\\n86'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 86}, page_content='[369] Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni. Building a secure agentic\\nai application leveraging a2a protocol, arXiv preprint arXiv:2504.16902, 2025. URL https://\\narxiv.org/abs/2504.16902v2.\\n[370] John Halloran. Mcp safety training: Learning to refuse falsely benign mcp exploits using improved\\npreference alignment, arXiv preprint arXiv:2505.23634, 2025. URL https://arxiv.org/abs/\\n2505.23634v1.\\n[371] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soo-Uck Kim, Hyunji Choi, Sungjun Jung, and Jae W.\\nLee. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural\\nnetworks. International Symposium on Computer Architecture, 2021.\\n[372] Feijiang Han, Licheng Guo, Hengtao Cui, and Zhiyuan Lyu. Question tokens deserve more attention:\\nEnhancing large language models without training through step-by-step reading and question\\nattention recalibration, arXiv preprint arXiv:2504.09402, 2025. URL https://arxiv.org/abs/\\n2504.09402v1.\\n[373] Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, and Wenliang Chen. Nestools: A\\ndataset for evaluating nested tool learning abilities of large language models. International Conference\\non Computational Linguistics, 2024.\\n[374] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar,\\nRyan A. Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi He, Zhigang Hua, Bo Long, Tong Zhao,\\nNeil Shah, Amin Javari, Yinglong Xia, and Jiliang Tang. Retrieval-augmented generation with\\ngraphs (graphrag), arXiv preprint arXiv:2501.00309, 2025. URL https://arxiv.org/abs/\\n2501.00309.\\n[375] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyun Zhao, Shiqing Ma, and Zhenyu Chen. Token-\\nbudget-aware llm reasoning, arXiv preprint arXiv:2412.18547, 2024. URL https://arxiv.org/\\nabs/2412.18547v5.\\n[376] Yuanning Han, Ziyi Qiu, Jiale Cheng, and Ray Lc. When teams embrace ai: Human collaboration\\nstrategies in generative prompting in a creative design task. International Conference on Human\\nFactors in Computing Systems, 2024.\\n[377] R. Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, and Greig A.\\nCowan. Evaluating the sensitivity of llms to prior context, arXiv preprint arXiv:2506.00069, 2025.\\nURL https://arxiv.org/abs/2506.00069v1.\\n[378] S Hao, T Liu, Z Wang, and Z Hu. Toolkengpt: Augmenting frozen language models with massive\\ntools via tool embeddings. 2023. URL https://proceedings.neurips.cc/paper_files/\\npaper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference.\\nhtml.\\n[379] Mohanakrishnan Hariharan. Semantic mastery: Enhancing llms with advanced natural language\\nunderstanding, arXiv preprint arXiv:2504.00409, 2025. URL https://arxiv.org/abs/2504.\\n00409v1.\\n[380] Mareike Hartmann and Alexander Koller. A survey on complex tasks for goal-directed interac-\\ntive agents, arXiv preprint arXiv:2409.18538, 2024.\\nURL https://arxiv.org/abs/2409.\\n18538v1.\\n87'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 87}, page_content='[381] A. Hassani, A. Medvedev, P. D. Haghighi, Sea Ling, A. Zaslavsky, and P. Jayaraman. Context definition\\nand query language: Conceptual specification, implementation, and evaluation. Italian National\\nConference on Sensors, 2019.\\n[382] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks,\\nZohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term\\nmemory in llm-agents. Proceedings of the AAAI Symposium Series, 2024.\\n[383] Kostas Hatalis, Despina Christou, and Vyshnavi Kondapalli. Review of case-based reasoning for llm\\nagents: Theoretical foundations, architectural components, and cognitive integration, arXiv preprint\\narXiv:2504.06943, 2025. URL https://arxiv.org/abs/2504.06943v2.\\n[384] Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, and Xiaokai Wang. Context-guided\\ndynamic retrieval for improving generation quality in rag models, arXiv preprint arXiv:2504.19436,\\n2025. URL https://arxiv.org/abs/2504.19436v1.\\n[385] Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu. Poem: Interactive\\nprompt optimization for enhancing multimodal reasoning of large language models. IEEE Pacific\\nVisualization Symposium, 2024.\\n[386] Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, and Jiaxing Zhang. Madial-bench:\\nTowards real-world evaluation of memory-augmented dialogue generation. North American Chapter\\nof the Association for Computational Linguistics, 2024.\\n[387] Shawn He, Surangika Ranathunga, Stephen Cranefield, and B. Savarimuthu.\\nNorm violation\\ndetection in multi-agent systems using large language models: A pilot study. COINE, 2024.\\n[388] Shengtao He. Achieving tool calling functionality in llms using only prompt engineering with-\\nout fine-tuning, arXiv preprint arXiv:2407.04997, 2024. URL https://arxiv.org/abs/2407.\\n04997v1.\\n[389] Wenchong He, Liqian Peng, Zhe Jiang, and Alex Go. You only fine-tune once: Many-shot in-\\ncontext fine-tuning for large language model, arXiv preprint arXiv:2506.11103, 2025. URL https:\\n//arxiv.org/abs/2506.11103v1.\\n[390] Xu He, Di Wu, Yan Zhai, and Kun Sun. Sentinelagent: Graph-based anomaly detection in multi-\\nagent systems, arXiv preprint arXiv:2505.24201, 2025. URL https://arxiv.org/abs/2505.\\n24201v1.\\n[391] Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, and Ting Liu. Self-\\nroute: Automatic mode switching via capability estimation for efficient reasoning. arXiv preprint,\\n2025.\\n[392] Yu He, Yingxi Li, Colin White, and Ellen Vitercik. Dsr-bench: Evaluating the structural reasoning\\nabilities of llms via data structures. arXiv preprint, 2025.\\n[393] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogério Feris.\\nCamelot: Towards large language models with training-free consolidated associative memory, arXiv\\npreprint arXiv:2402.13449, 2024. URL https://arxiv.org/abs/2402.13449v1.\\n[394] James B. Heald, M. Lengyel, and D. Wolpert. Contextual inference in learning and memory. Trends\\nin Cognitive Sciences, 2022.\\n88'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 88}, page_content='[395] Shekoofeh Hedayati, Ryan E. O’Donnell, and Brad Wyble. A model of working memory for latent\\nrepresentations. Nature Human Behaviour, 2021.\\n[396] Tooraj Helmi. Modeling response consistency in multi-agent llm systems: A comparative analysis\\nof shared and separate context approaches, arXiv preprint arXiv:2504.07303, 2025. URL https:\\n//arxiv.org/abs/2504.07303v1.\\n[397] Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, and Afsaneh Fatemi. Leveraging\\nretrieval-augmented generation for persian university knowledge retrieval. Conference on Information\\nand Knowledge Technology, 2024.\\n[398] M. Herrera, Marco Pérez-Hernández, A. Kumar Parlikad, and J. Izquierdo. Multi-agent systems and\\ncomplex networks: Review and applications in systems engineering. Processes, 2020.\\n[399] Nora A. Herweg, A. Sharan, M. Sperling, A. Brandt, A. Schulze-Bonhage, and M. Kahana. Reactivated\\nspatial context guides episodic recall. Journal of Neuroscience, 2018.\\n[400] Thomas F. Heston and Charya Khun. Prompt engineering in medical education. International Medical\\nEducation, 2023.\\n[401] Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, K. Koedinger, and Vincent Aleven. Com-\\nparative analysis of gpt-4 and human graders in evaluating human tutors giving praise to students.\\nHuman-AI Math Tutoring@AIED, 2023.\\n[402] Thomas Hoang. Gnn: Graph neural network and large language model for data discovery, arXiv\\npreprint arXiv:2408.13609, 2024. URL https://arxiv.org/abs/2408.13609v2.\\n[403] W. Hoek and M. Wooldridge. Towards a logic of rational agency. Logic Journal of the IGPL, 2003.\\n[404] Aidan Hogan, E. Blomqvist, Michael Cochez, C. d’Amato, Gerard de Melo, C. Gutierrez, J. E. L. Gayo,\\nS. Kirrane, S. Neumaier, A. Polleres, Roberto Navigli, A. Ngomo, S. M. Rashid, Anisa Rula, Lukas\\nSchmelzeisen, Juan Sequeda, Steffen Staab, and Antoine Zimmermann. Knowledge graphs. ACM\\nComputing Surveys, 2020.\\n[405] Nithin Holla, Pushkar Mishra, H. Yannakoudakis, and Ekaterina Shutova. Meta-learning with sparse\\nexperience replay for lifelong language learning, arXiv preprint arXiv:2009.04891, 2020. URL\\nhttps://arxiv.org/abs/2009.04891v2.\\n[406] Chuanyang Hong and Qingyun He. Enhancing memory retrieval in generative agents through\\nllm-trained cross attention networks. Frontiers in Psychology, 2025.\\n[407] M. Hong, Sean M. Polyn, and Lisa K. Fazio. Examining the episodic context account: does retrieval\\npractice enhance memory for context? Cognitive Research, 2019.\\n[408] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\\nKa Shing Yau, Z. Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta\\nprogramming for multi-agent collaborative framework. arXiv preprint, 2023.\\n[409] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang,\\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\\nand Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent collaborative framework,\\narXiv preprint arXiv:2308.00352, 2024. URL https://arxiv.org/abs/2308.00352.\\n89'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 89}, page_content='[410] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan\\nWang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents.\\nComputer Vision and Pattern Recognition, 2023.\\n[411] Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. On the\\ntoken distance modeling ability of higher rope attention dimension. Conference on Empirical Methods\\nin Natural Language Processing, 2024.\\n[412] Yubin Hong, Chaofan Li, Jingyi Zhang, and Yingxia Shao. Fg-rag: Enhancing query-focused summa-\\nrization with context-aware fine-grained graph rag. arXiv preprint, 2025.\\n[413] Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Kasidis Kanwatchara, B. Kijsirikul, and P. Va-\\nteekul. Meta lifelong-learning with selective and task-aware adaptation. IEEE Access, 2024.\\n[414] A. N. Hoskin, A. Bornstein, K. Norman, and J. Cohen. Refresh my memory: Episodic memory rein-\\nstatements intrude on working memory maintenance. Cognitive, Affective, & Behavioral Neuroscience,\\n2017.\\n[415] Timothy M. Hospedales, Antreas Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural\\nnetworks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\\n[416] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an\\nintriguing failure of llms: Long context window does not mean llms can analyze long sequences\\nflawlessly. International Conference on Computational Linguistics, 2024.\\n[417] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language\\nmodels via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL\\nhttps://arxiv.org/abs/2408.15491v1.\\n[418] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented\\nmultimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024.\\nURL https://arxiv.org/abs/2411.10937v1.\\n[419] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape,\\nsecurity threats, and future research directions, arXiv preprint arXiv:2503.23278, 2025. URL\\nhttps://arxiv.org/abs/2503.23278v2.\\n[420] Zejiang Hou, Julian Salazar, and George Polovets. Meta-learning the difference: Preparing large\\nlanguage models for efficient adaptation. Transactions of the Association for Computational Linguistics,\\n2022.\\n[421] N. Houlsby, A. Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\\nGesmundo, Mona Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. International\\nConference on Machine Learning, 2019.\\n[422] Marc W Howard and M. Kahana. A distributed representation of temporal context. arXiv preprint,\\n2002.\\n[423] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, J. Zhao, and Hang Zhao. Chatdb: Augmenting\\nllms with databases as their symbolic memory, arXiv preprint arXiv:2306.03901, 2023. URL https:\\n//arxiv.org/abs/2306.03901v2.\\n90'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 90}, page_content='[424] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. International Conference on Learning\\nRepresentations, 2021.\\n[425] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen,\\nTao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning,\\narXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2.\\n[426] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: A\\nnano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL\\nhttps://arxiv.org/abs/2504.15989v2.\\n[427] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F. Arvin. Voronoi-based multi-robot autonomous\\nexploration in unknown environments via deep reinforcement learning. IEEE Transactions on\\nVehicular Technology, 2020.\\n[428] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. A survey of knowledge\\nenhanced pre-trained language models. IEEE Transactions on Knowledge and Data Engineering, 2022.\\n[429] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent:\\nHierarchical working memory management for solving long-horizon agent tasks with large language\\nmodel, arXiv preprint arXiv:2408.09559, 2024. URL https://arxiv.org/abs/2408.09559.\\n[430] Nathan J. Hu, E. Mitchell, Christopher D. Manning, and Chelsea Finn.\\nMeta-learning online\\nadaptation of language models. Conference on Empirical Methods in Natural Language Processing,\\n2023.\\n[431] Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, and Yixin Chen. Large\\nlanguage model meets graph neural network in knowledge distillation. AAAI Conference on Artificial\\nIntelligence, 2024.\\n[432] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: A preliminary\\ncase study with claude 3.5 computer use. arXiv preprint, 2024.\\n[433] Ting Hu, Christoph Meinel, and Haojin Yang. Scaled prompt-tuning for few-shot natural lan-\\nguage generation, arXiv preprint arXiv:2309.06759, 2023. URL https://arxiv.org/abs/\\n2309.06759v1.\\n[434] Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan,\\nand Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and\\ndiversity of llm generated ideas, arXiv preprint arXiv:2410.14255, 2024. URL https://arxiv.\\norg/abs/2410.14255v2.\\n[435] Yuntong Hu, Zhengwu Zhang, and Liang Zhao. Beyond text: A deep dive into large language\\nmodels’ ability on understanding graph data, arXiv preprint arXiv:2310.04944, 2023. URL https:\\n//arxiv.org/abs/2310.04944v1.\\n[436] Yilun Hua and Yoav Artzi. Talk less, interact better: Evaluating in-context conversational adaptation\\nin multimodal llms, arXiv preprint arXiv:2408.01417v1, 2024. URL https://arxiv.org/abs/\\n2408.01417v1.\\n91'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 91}, page_content='[437] Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, and Roei Herzig.\\nMultimodal task vectors enable many-shot multimodal in-context learning. Neural Information\\nProcessing Systems, 2024.\\n[438] Chengkai Huang, Hongtao Huang, Tong Yu, Kaige Xie, Junda Wu, Shuai Zhang, Julian J. McAuley,\\nDietmar Jannach, and Lina Yao. A survey of foundation model-powered recommender systems:\\nFrom feature-based, generative to agentic paradigms, arXiv preprint arXiv:2504.16420, 2025. URL\\nhttps://arxiv.org/abs/2504.16420v1.\\n[439] Chengkai Huang, Junda Wu, Yu Xia, Zixu Yu, Ruhan Wang, Tong Yu, Ruiyi Zhang, Ryan A. Rossi,\\nB. Kveton, Dongruo Zhou, Julian J. McAuley, and Lina Yao. Towards agentic recommender systems\\nin the era of multimodal large language models, arXiv preprint arXiv:2503.16734, 2025. URL\\nhttps://arxiv.org/abs/2503.16734v1.\\n[440] Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, and Shuo Shang. Ttpa: Token-\\nlevel tool-use preference alignment training framework with fine-grained evaluation, arXiv preprint\\narXiv:2505.20016, 2025. URL https://arxiv.org/abs/2505.20016v1.\\n[441] Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, and\\nJinqiao Wang. Recurrent context compression: Efficiently expanding the context window of llm,\\narXiv preprint arXiv:2406.06110, 2024. URL https://arxiv.org/abs/2406.06110v1.\\n[442] Jing Huang, X. Ruan, Naigong Yu, Qingwu Fan, Jiaming Li, and Jianxian Cai. A cognitive model\\nbased on neuromodulated plasticity. Computational Intelligence and Neuroscience, 2016.\\n[443] Ken Huang, Akram Sheriff, Vineeth Sai Narajala, and Idan Habler. Agent capability negotiation and\\nbinding protocol (acnbp), arXiv preprint arXiv:2506.13590, 2025. URL https://arxiv.org/\\nabs/2506.13590v1.\\n[444] Le Huang, Hengzhi Lan, Zijun Sun, Chuan Shi, and Ting Bai. Emotional rag: Enhancing role-playing\\nagents through emotional retrieval. 2024 IEEE International Conference on Knowledge Graph (ICKG),\\n2024.\\n[445] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin\\nZhao. Manusearch: Democratizing deep search in large language models with a transparent and\\nopen multi-agent framework, arXiv preprint arXiv:2505.18105, 2025. URL https://arxiv.org/\\nabs/2505.18105v1.\\n[446] Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, and\\nFeng Zhao. Critictool: Evaluating self-critique capabilities of large language models in tool-calling\\nerror scenarios, arXiv preprint arXiv:2506.13977, 2025. URL https://arxiv.org/abs/2506.\\n13977v1.\\n[447] Sirui Huang, Yanggan Gu, Xuming Hu, Zhonghao Li, Qing Li, and Guandong Xu. Reasoning factual\\nknowledge in structured data with large language models, arXiv preprint arXiv:2408.12188, 2024.\\nURL https://arxiv.org/abs/2408.12188v1.\\n[448] Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, and Guandong Xu. Hyperg: Hypergraph-\\nenhanced llms for structured knowledge, arXiv preprint arXiv:2502.18125, 2025. URL https:\\n//arxiv.org/abs/2502.18125v1.\\n92'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 92}, page_content='[449] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\\nplanners: Extracting actionable knowledge for embodied agents. International Conference on Machine\\nLearning, 2022.\\n[450] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent:\\nIntegrating large language models for interactive recommendations, arXiv preprint arXiv:2308.16505,\\n2024. URL https://arxiv.org/abs/2308.16505.\\n[451] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang,\\nRuiming Tang, and Enhong Chen. Understanding the planning of llm agents: A survey, arXiv preprint\\narXiv:2402.02716, 2024. URL https://arxiv.org/abs/2402.02716v1.\\n[452] Y Huang, J Shi, Y Li, C Fan, S Wu, and Q Zhang.... Metatool benchmark for large language models:\\nDeciding whether to use tools and which to use. 2023. URL https://arxiv.org/abs/2310.\\n03128.\\n[453] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan\\nYang, Zhou Xin, and Xiaoxing Ma.\\nAdvancing transformer architecture in long-context large\\nlanguage models: A comprehensive survey, arXiv preprint arXiv:2311.12351, 2023. URL https:\\n//arxiv.org/abs/2311.12351v2.\\n[454] Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, and Yong Jae Lee.\\nVisualtoolagent (vista): A reinforcement learning framework for visual tool selection, arXiv preprint\\narXiv:2505.20289, 2025. URL https://arxiv.org/abs/2505.20289v1.\\n[455] Ziheng Huang, S. Gutierrez, Hemanth Kamana, and S. Macneil. Memory sandbox: Transparent\\nand interactive memory management for conversational agents. ACM Symposium on User Interface\\nSoftware and Technology, 2023.\\n[456] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sand-\\nbox: Transparent and interactive memory management for conversational agents, arXiv preprint\\narXiv:2308.01542, 2023. URL https://arxiv.org/abs/2308.01542.\\n[457] Alexis Huet, Zied Ben-Houidi, and Dario Rossi. Episodic memories generation and evaluation\\nbenchmark for large language models. International Conference on Learning Representations, 2025.\\n[458] Dom Huh and Prasant Mohapatra. Multi-agent reinforcement learning: A comprehensive survey,\\narXiv preprint arXiv:2312.10256, 2023. URL https://arxiv.org/abs/2312.10256v2.\\n[459] Eunjeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, and Sandeep\\nTata. Enhancing incremental summarization with structured representations. Conference on Empirical\\nMethods in Natural Language Processing, 2024.\\n[460] Thorsten Händler. Balancing autonomy and alignment: A multi-dimensional taxonomy for au-\\ntonomous llm-powered multi-agent architectures. arXiv preprint, 2023.\\n[461] Michael Iannelli, Sneha Kuchipudi, and Vera Dvorak. Sla management in reconfigurable multi-agent\\nrag: A systems approach to question answering, arXiv preprint arXiv:2412.06832, 2024. URL\\nhttps://arxiv.org/abs/2412.06832v2.\\n[462] IBM. What is agent communication protocol (acp)? https://www.ibm.com/think/topics/\\nagent-communication-protocol, 2025. [Online; accessed 17-July-2025].\\n93'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 93}, page_content='[463] T Inaba, H Kiyomaru, F Cheng, and S Kurohashi. Multitool-cot: Gpt-3 can use multiple external\\ntools with chain of thought prompting. 2023. URL https://arxiv.org/abs/2305.16896.\\n[464] G. Indiveri and Shih-Chii Liu.\\nMemory and information processing in neuromorphic systems.\\nProceedings of the IEEE, 2015.\\n[465] V. Ioannidis, Xiang Song, Da Zheng, Houyu Zhang, Jun Ma, Yi Xu, Belinda Zeng, Trishul M. Chilimbi,\\nand G. Karypis. Efficient and effective training of language and graph neural network models, arXiv\\npreprint arXiv:2206.10781, 2022. URL https://arxiv.org/abs/2206.10781v1.\\n[466] Yoichi Ishibashi, Taro Yano, and M. Oyamada. Can large language models invent algorithms to im-\\nprove themselves?: Algorithm discovery for recursive self-improvement through reinforcement learn-\\ning, arXiv preprint arXiv:2410.15639, 2024. URL https://arxiv.org/abs/2410.15639v5.\\n[467] Shadi Iskander, Nachshon Cohen, Zohar S. Karnin, Ori Shapira, and Sofia Tolmach. Quality matters:\\nEvaluating synthetic data for tool-using llms. Conference on Empirical Methods in Natural Language\\nProcessing, 2024.\\n[468] Z. Ismail and N. Sariff. A survey and analysis of cooperative multi-agent robot systems: Challenges\\nand directions. Applications of Mobile Robots, 2018.\\n[469] Yusuf Izmirlioglu, Loc Pham, Tran Cao Son, and Enrico Pontelli. A survey of multi-agent systems for\\nsmartgrids. Energies, 2024.\\n[470] Jace.AI. Jace.ai web agent, 2024. URL https://www.jace.ai/. Accessed: 2025-07-14.\\n[471] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and\\ngeneralization in neural networks. Neural Information Processing Systems, 2018.\\n[472] Tejas Jade and Alex Yartsev. Chatgpt for automated grading of short answer questions in mechani-\\ncal ventilation, arXiv preprint arXiv:2505.04645, 2025. URL https://arxiv.org/abs/2505.\\n04645v1.\\n[473] A. Jafarpour, L. Fuentemilla, A. Horner, W. Penny, and E. Duzel. Replay of very early encoding\\nrepresentations during recollection. Journal of Neuroscience, 2014.\\n[474] A. Jaiswal, Nurendra Choudhary, Ravinarayana Adkathimar, M. P. Alagappan, G. Hiranandani,\\nYing Ding, Zhangyang Wang, E-Wen Huang, and Karthik Subbian. All against some: Efficient\\nintegration of large language models for message passing in graph neural networks, arXiv preprint\\narXiv:2407.14996, 2024. URL https://arxiv.org/abs/2407.14996v1.\\n[475] H. Jaleel, Jane J. Stephan, and Sinan Naji. Multi-agent systems: A review study. Ibn AL- Haitham\\nJournal For Pure and Applied Sciences, 2020.\\n[476] Lawrence Jang, Yinheng Li, Charles Ding, Justin Lin, Paul Pu Liang, Dan Zhao, Rogerio Bonatti, and\\nK. Koishida. Videowebarena: Evaluating long context multimodal agents with video understanding\\nweb tasks. International Conference on Learning Representations, 2024.\\n[477] R. Janik. Aspects of human memory and large language models, arXiv preprint arXiv:2311.03839,\\n2023. URL https://arxiv.org/abs/2311.03839v3.\\n[478] Shumaila Javaid, Hamza Fahim, Bin He, and Nasir Saeed. Large language models for uavs: Current\\nstate and pathways to the future. IEEE Open Journal of Vehicular Technology, 2024.\\n94'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 94}, page_content='[479] T. S. Jayram, Younes Bouhadjar, Ryan L. McAvoy, Tomasz Kornuta, Alexis Asseman, K. Rocki, and\\nA. Ozcan. Learning to remember, forget and ignore using attention control in memory, arXiv preprint\\narXiv:1809.11087, 2018. URL https://arxiv.org/abs/1809.11087v1.\\n[480] Cheonsu Jeong. A study on the mcp x a2a framework for enhancing interoperability of llm-based\\nautonomous agents, arXiv preprint arXiv:2506.01804, 2025. URL https://arxiv.org/abs/\\n2506.01804v2.\\n[481] Gang Ji and J. Bilmes. Multi-speaker language modeling. North American Chapter of the Association\\nfor Computational Linguistics, 2004.\\n[482] Ke Ji, Junying Chen, Anningzhe Gao, Wenya Xie, Xiang Wan, and Benyou Wang. Llms could\\nautonomously learn without external supervision, arXiv preprint arXiv:2406.00606, 2024. URL\\nhttps://arxiv.org/abs/2406.00606v2.\\n[483] Shaoxiong Ji, Shirui Pan, E. Cambria, P. Marttinen, and Philip S. Yu. A survey on knowledge graphs:\\nRepresentation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning\\nSystems, 2020.\\n[484] Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan\\nWang, Haote Yang, et al. Evaluating large language model with knowledge oriented language\\nspecific simple question answering. 2025.\\n[485] Caigao Jiang, Siqiao Xue, James Zhang, Lingyue Liu, Zhibo Zhu, and Hongyan Hao. Learning large-\\nscale universal user representation with sparse mixture of experts, arXiv preprint arXiv:2207.04648,\\n2022. URL https://arxiv.org/abs/2207.04648v1.\\n[486] Feibo Jiang, Li Dong, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, D. Niyato, and O. Dobre. Large\\nlanguage model enhanced multi-agent systems for 6g communications. IEEE wireless communications,\\n2023.\\n[487] J Jiang, K Zhou, WX Zhao, Y Song, and C Zhu.... Kg-agent: An efficient autonomous agent\\nframework for complex reasoning over knowledge graph. 2024. URL https://arxiv.org/abs/\\n2402.11163.\\n[488] Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji rong Wen. Unikgqa: Unified retrieval and reasoning\\nfor solving multi-hop question answering over knowledge graph. International Conference on Learning\\nRepresentations, 2022.\\n[489] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji rong Wen. Structgpt: A\\ngeneral framework for large language model to reason over structured data. Conference on Empirical\\nMethods in Natural Language Processing, 2023.\\n[490] Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz,\\nYizhou Sun, Jinchao Li, Qifan Wang, and Asli Celikyilmaz. Resprompt: Residual connection prompting\\nadvances multi-step reasoning in large language models. North American Chapter of the Association\\nfor Computational Linguistics, 2023.\\n[491] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. Conference on Empirical\\nMethods in Natural Language Processing, 2023.\\n95'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 95}, page_content='[492] Zhonglin Jiang, Qian Tang, and Zequn Wang. Generative reliability-based design optimization using\\nin-context learning capabilities of large language models, arXiv preprint arXiv:2503.22401, 2025.\\nURL https://arxiv.org/abs/2503.22401v1.\\n[493] Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. Llms can find mathemat-\\nical reasoning mistakes by pedagogical chain-of-thought. International Joint Conference on Artificial\\nIntelligence, 2024.\\n[494] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\\nNarasimhan. Swe-bench: Can language models resolve real-world github issues?, arXiv preprint\\narXiv:2310.06770, 2024. URL https://arxiv.org/abs/2310.06770.\\n[495] B Jin, C Xie, J Zhang, KK Roy, Y Zhang, and Z Li.... Graph chain-of-thought: Augmenting large\\nlanguage models by reasoning on graphs. 2024. URL https://arxiv.org/abs/2404.07103.\\n[496] Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han.\\nHeterformer: Transformer-based deep node\\nrepresentation learning on heterogeneous text-rich networks. Knowledge Discovery and Data Mining,\\n2022.\\n[497] Feihu Jin, Jiajun Zhang, and Chengqing Zong. Parameter-efficient tuning for large language model\\nwithout calculating its gradients. Conference on Empirical Methods in Natural Language Processing,\\n2023.\\n[498] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llm-\\nbased agents for software engineering: A survey of current, challenges and future, arXiv preprint\\narXiv:2408.02479, 2024. URL https://arxiv.org/abs/2408.02479v2.\\n[499] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia yuan Chang, Huiyuan\\nChen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. International\\nConference on Machine Learning, 2024.\\n[500] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit\\nfor efficient retrieval-augmented generation research. The Web Conference, 2024.\\n[501] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models\\nwith domain tools for improved access to biomedical information. Bioinform., 2023.\\n[502] Tian Jin, W. Yazar, Zifei Xu, Sayeh Sharify, and Xin Wang. Self-selected attention span for accelerating\\nlarge language model inference, arXiv preprint arXiv:2404.09336, 2024. URL https://arxiv.\\norg/abs/2404.09336v1.\\n[503] Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, and Guang Yang. A compre-\\nhensive survey on multi-agent cooperative decision-making: Scenarios, approaches, challenges and\\nperspectives. arXiv preprint, 2025.\\n[504] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi\\nLei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, and Yadong Mu. Unified\\nlanguage-vision pretraining in llm with dynamic discrete visual tokenization. International Conference\\non Learning Representations, 2023.\\n96'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 96}, page_content='[505] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking\\nmultimodal large language models in social media platforms. Annual Meeting of the Association for\\nComputational Linguistics, 2024.\\n[506] Yiyang Jin, Kunzhao Xu, Hang Li, Xueting Han, Yanmin Zhou, Cheng Li, and Jing Bai. Reveal:\\nSelf-evolving code agents via iterative generation-verification, arXiv preprint arXiv:2506.11442,\\n2025. URL https://arxiv.org/abs/2506.11442v1.\\n[507] Jeff Johnson, Matthijs Douze, and H. Jégou. Billion-scale similarity search with gpus. IEEE Transac-\\ntions on Big Data, 2017.\\n[508] Jeff A. Johnson and Daniel H. Bullock. Fragility in ais using artificial neural networks. Communications\\nof the ACM, 2023.\\n[509] Zhao Kaiya, Michelangelo Naim, J. Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert\\nYang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions,\\narXiv preprint arXiv:2310.02172, 2023. URL https://arxiv.org/abs/2310.02172v1.\\n[510] Kurmanbek Kaiyrbekov, Nic Dobbins, and Sean D. Mooney. Automated survey collection with llm-\\nbased conversational agents, arXiv preprint arXiv:2504.02891, 2025. URL https://arxiv.org/\\nabs/2504.02891v1.\\n[511] A. Kakas, P. Mancarella, F. Sadri, Kostas Stathis, and Francesca Toni. Computational logic foundations\\nof kgp agents. Journal of Artificial Intelligence Research, 2008.\\n[512] Vikas Kamra, Lakshya Gupta, Dhruv Arora, and Ashwin Kumar Yadav. Enhancing document retrieval\\nusing ai and graph-based rag techniques. 2024 5th International Conference on Communication,\\nComputing & Industry 6.0 (C2I6), 2024.\\n[513] Eser Kandogan, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Sairam Gurajada, and Estevam R.\\nHruschka. Orchestrating agents and data for enterprise: A blueprint architecture for compound ai,\\narXiv preprint arXiv:2504.08148, 2025. URL https://arxiv.org/abs/2504.08148v1.\\n[514] Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang Central South University, Dalian University\\nof Technology, Nanjing University, and Xidian University. Sakr: Enhancing retrieval-augmented\\ngeneration via streaming algorithm and k-means clustering. arXiv preprint, 2024.\\n[515] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent, arXiv preprint\\narXiv:2506.06326, 2025. URL https://arxiv.org/abs/2506.06326v1.\\n[516] Jikun Kang, Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, George Thomas, Marvin\\nPurtorab, and Andy Toulis. Lm2: Large memory models, arXiv preprint arXiv:2502.06049, 2025.\\nURL https://arxiv.org/abs/2502.06049v1.\\n[517] Sungmin Kang, Gabin An, and S. Yoo. A quantitative and qualitative evaluation of llm-based\\nexplainable fault localization. Proc. ACM Softw. Eng., 2023.\\n[518] Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. From tokens to words: On the inner\\nlexicon of llms. International Conference on Learning Representations, 2024.\\n[519] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi\\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. Conference on\\nEmpirical Methods in Natural Language Processing, 2020.\\n97'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 97}, page_content='[520] Zdeněk Kasner and Ondrej Dusek. Beyond traditional benchmarks: Analyzing behaviors of open llms\\non data-to-text generation. Annual Meeting of the Association for Computational Linguistics, 2024.\\n[521] Kiran Kate, Tejaswini Pedapati, Kinjal Basu, Yara Rizk, Vijil Chenthamarakshan, Subhajit Chaudhury,\\nMayank Agarwal, and Ibrahim Abdelaziz. Longfunceval: Measuring the effectiveness of long context\\nmodels for function calling, arXiv preprint arXiv:2505.10570, 2025. URL https://arxiv.org/\\nabs/2505.10570v1.\\n[522] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers are\\nrnns: Fast autoregressive transformers with linear attention. International Conference on Machine\\nLearning, 2020.\\n[523] Richard Katrix, Quentin Carroway, Rowan Hawkesbury, and Matthias Heathfield. Context-aware\\nsemantic recomposition mechanism for large language models, arXiv preprint arXiv:2501.17386,\\n2025. URL https://arxiv.org/abs/2501.17386v2.\\n[524] Amirhossein Kazemnejad, Inkit Padhi, K. Ramamurthy, Payel Das, and Siva Reddy. The impact of\\npositional encoding on length generalization in transformers. Neural Information Processing Systems,\\n2023.\\n[525] T. Kelley, R. Thomson, and Jonathan Milton. Standard model of mind: Episodic memory. Biologically\\nInspired Cognitive Architectures, 2018.\\n[526] Daan Kepel and Konstantina Valogianni. Autonomous prompt engineering in large language models,\\narXiv preprint arXiv:2407.11000, 2024. URL https://arxiv.org/abs/2407.11000v1.\\n[527] R. Kesner. Neurobiological foundations of an attribute model of memory. arXiv preprint, 2013.\\n[528] A. Khan, Md Toufique Hasan, Kai-Kristian Kemell, Jussi Rasku, and Pekka Abrahamsson. Developing\\nretrieval augmented generation (rag) based llm systems from pdfs: An experience report, arXiv\\npreprint arXiv:2410.15944, 2024. URL https://arxiv.org/abs/2410.15944v1.\\n[529] Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, and Seung Ki\\nMoon. Leveraging vision-language models for manufacturing feature recognition in cad designs,\\narXiv preprint arXiv:2411.02810, 2024. URL https://arxiv.org/abs/2411.02810v1.\\n[530] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis. Generalization\\nthrough memorization: Nearest neighbor language models. International Conference on Learning\\nRepresentations, 2019.\\n[531] Elahe Khatibi, Ziyu Wang, and Amir M. Rahmani. Cdf-rag: Causal dynamic feedback for adaptive\\nretrieval-augmented generation. arXiv preprint, 2025.\\n[532] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Neural Information Processing\\nSystems, 2020.\\n[533] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. International\\nConference on Learning Representations, 2022.\\n98'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 98}, page_content='[534] Sambhav Khurana, Xiner Li, Shurui Gui, and Shuiwang Ji. A hierarchical language model for\\ninterpretable graph reasoning, arXiv preprint arXiv:2410.22372, 2024. URL https://arxiv.\\norg/abs/2410.22372v1.\\n[535] Daehee Kim, Deokhyung Kang, Sangwon Ryu, and Gary Geunbae Lee. Ontology-free general-domain\\nknowledge graph-to-text generation dataset synthesis using large language model, arXiv preprint\\narXiv:2409.07088, 2024. URL https://arxiv.org/abs/2409.07088v1.\\n[536] Geunwoo Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. Neural\\nInformation Processing Systems, 2023.\\n[537] Jaeyeon Kim, Injune Hwang, and Kyogu Lee. Learning semantic information from raw audio signal\\nusing both contextual and phonetic representations. IEEE International Conference on Acoustics,\\nSpeech, and Signal Processing, 2024.\\n[538] Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, and Hyun Oh Song. Compressed context memory\\nfor online language model interaction. International Conference on Learning Representations, 2023.\\n[539] Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. Kg-gpt: A general framework for reasoning\\non knowledge graphs using large language models. Conference on Empirical Methods in Natural\\nLanguage Processing, 2023.\\n[540] Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: De-\\nmystifying ai agents and test-time scaling from an ai infrastructure perspective, arXiv preprint\\narXiv:2506.04301, 2025. URL https://arxiv.org/abs/2506.04301v1.\\n[541] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and\\nA. Gholami. An llm compiler for parallel function calling. International Conference on Machine\\nLearning, 2023.\\n[542] Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark Neerincx, and Piek Vossen. A machine\\nwith short-term, episodic, and semantic memory systems. Proceedings of the AAAI Conference on\\nArtificial Intelligence, 37(1):48–56, 2023. ISSN 2159-5399. doi: 10.1609/aaai.v37i1.25075. URL\\nhttp://dx.doi.org/10.1609/aaai.v37i1.25075.\\n[543] Lukas Kirchdorfer, Robert Blümel, T. Kampik, Han van der Aa, and Heiner Stuckenschmidt. Dis-\\ncovering multi-agent systems for resource-centric business process simulation. Process Science,\\n2025.\\n[544] J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A.\\nRusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,\\nD. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of\\nthe National Academy of Sciences of the United States of America, 2016.\\n[545] Louis Kirsch, James Harrison, Jascha Narain Sohl-Dickstein, and Luke Metz. General-purpose\\nin-context learning by meta-learning transformers, arXiv preprint arXiv:2212.04458, 2022. URL\\nhttps://arxiv.org/abs/2212.04458v2.\\n[546] Yuval Kirstain, Patrick Lewis, Sebastian Riedel, and Omer Levy. A few more examples may be worth\\nbillions of parameters. Conference on Empirical Methods in Natural Language Processing, 2021.\\n99'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 99}, page_content='[547] Andrew Kiruluta, Preethi Raju, and Priscilla Burity. Breaking quadratic barriers: A non-attention\\nllm for ultra-long context horizons, arXiv preprint arXiv:2506.01963, 2025. URL https://arxiv.\\norg/abs/2506.01963v1.\\n[548] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. International\\nConference on Learning Representations, 2020.\\n[549] Vincent Koc, Jacques Verre, Douglas Blank, and Abigail Morgan. Mind the metrics: Patterns for\\ntelemetry-aware in-ide ai application development using the model context protocol (mcp), arXiv\\npreprint arXiv:2506.11019, 2025. URL https://arxiv.org/abs/2506.11019v1.\\n[550] Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,\\nand Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 2017.\\n[551] Jing Yu Koh, R. Salakhutdinov, and Daniel Fried.\\nGrounding language models to images for\\nmultimodal inputs and outputs. International Conference on Machine Learning, 2023.\\n[552] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Gra-\\nham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating\\nmultimodal agents on realistic visual web tasks, arXiv preprint arXiv:2401.13649, 2024. URL\\nhttps://arxiv.org/abs/2401.13649v2.\\n[553] Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models\\nare zero-shot reasoners. Neural Information Processing Systems, 2022.\\n[554] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru\\nHu, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu-v2: Boosting task planning and tool\\nusage of large language model-based agents in real-world systems, arXiv preprint arXiv:2311.11315,\\n2023. URL https://arxiv.org/abs/2311.11315.\\n[555] P. Korzyński, G. Mazurek, Pamela Krzypkowska, and Artur Kurasiński. Artificial intelligence prompt\\nengineering as a new digital competence: Analysis of generative ai technologies such as chatgpt.\\nEntrepreneurial Business and Economics Review, 2023.\\n[556] Oliver Kramer. Cognitive prompts using guilford’s structure of intellect model. arXiv preprint, 2025.\\n[557] Oliver Kramer. Conceptual metaphor theory as a prompting paradigm for large language models,\\narXiv preprint arXiv:2502.01901, 2025. URL https://arxiv.org/abs/2502.01901v1.\\n[558] Oliver Kramer and Jill Baumann. Unlocking structured thinking in language models with cognitive\\nprompting. ESANN 2025 proceesdings, 2024.\\n[559] K. Kravari and Nick Bassiliades. A survey of agent platforms. Journal of Artificial Societies and Social\\nSimulation, 2015.\\n[560] Prashant Krishnan, Zilong Wang, Yangkun Wang, and Jingbo Shang. Towards few-shot entity\\nrecognition in document images: A graph neural network approach robust to image manipulation.\\nInternational Conference on Language Resources and Evaluation, 2023.\\n[561] W. Kruijne, S. Bohté, P. Roelfsema, and C. Olivers. Flexible working memory through selective gating\\nand attentional tagging. bioRxiv, 2019.\\n100'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 100}, page_content='[562] L. Krupp, Daniel Geissler, P. Lukowicz, and Jakob Karolus. Towards sustainable web agents: A plea\\nfor transparency and dedicated metrics for energy consumption, arXiv preprint arXiv:2502.17903,\\n2025. URL https://arxiv.org/abs/2502.17903v1.\\n[563] M. Kuhail, Jose Berengueres, Fatma Taher, Sana Z. Khan, and Ansah Siddiqui. Designing a haptic\\nboot for space with prompt engineering: Process, insights, and implications. IEEE Access, 2024.\\n[564] Amandeep Kumar, Muzammal Naseer, Sanath Narayan, R. Anwer, Salman H. Khan, and\\nHisham Cholakkal. Multi-modal generation via cross-modal in-context learning, arXiv preprint\\narXiv:2405.18304v1, 2024. URL https://arxiv.org/abs/2405.18304v1.\\n[565] Rajeev Kumar, Harishankar Kumar, and Kumari Shalini. Detecting and mitigating bias in llms through\\nknowledge graph-augmented training. 2025 International Conference on Artificial Intelligence and\\nData Engineering (AIDE), 2025.\\n[566] Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong woo Kwak,\\nKuan-Hao Huang, and Jinyoung Yeo. Embodied agents meet personalization: Exploring memory\\nutilization for personalized assistance, arXiv preprint arXiv:2505.16348, 2025. URL https://\\narxiv.org/abs/2505.16348v1.\\n[567] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model\\nserving with pagedattention. Symposium on Operating Systems Principles, 2023.\\n[568] T. Lai, Quan Hung Tran, Trung Bui, and D. Kihara. A gated self-attention memory network for\\nanswer selection. Conference on Empirical Methods in Natural Language Processing, 2019.\\n[569] Divya Lamba. The role of prompt engineering in improving language understanding and generation.\\nInternational Journal For Multidisciplinary Research, 2024.\\n[570] Xiaochong Lan, Jie Feng, Jia Lei, Xinlei Shi, and Yong Li. Benchmarking and advancing large\\nlanguage models for local life services, arXiv preprint arXiv:2506.02720, 2025.\\nURL https:\\n//arxiv.org/abs/2506.02720v1.\\n[571] LangChain Team. Memory in langgraph. https://langchain-ai.github.io/langgraph/\\nconcepts/memory/, 2025. Accessed: 2025-07-17.\\n[572] Samuel T. Langlois, Oghenetekevwe Akoroda, Estefany Carrillo, J. Herrmann, S. Azarm, Huan Xu,\\nand Michael W. Otte. Metareasoning structures, problems, and modes for multiagent systems: A\\nsurvey. IEEE Access, 2020.\\n[573] B. Lattimer, Varun Gangal, Ryan McDonald, and Yi Yang.\\nSparse rewards can self-train dia-\\nlogue agents, arXiv preprint arXiv:2409.04617, 2024. URL https://arxiv.org/abs/2409.\\n04617v2.\\n[574] Pak Kin Lau and Stuart Michael McManus.\\nMining asymmetric intertextuality, arXiv preprint\\narXiv:2410.15145, 2024. URL https://arxiv.org/abs/2410.15145v1.\\n[575] Hung Le, T. Tran, and S. Venkatesh. Self-attentive associative memory. International Conference on\\nMachine Learning, 2020.\\n101'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 101}, page_content='[576] Dohyun Lee, Seungil Chad Lee, Chanwoo Yang, Yujin Baek, and Jaegul Choo. Exploring in-context\\nexample generation for machine translation, arXiv preprint arXiv:2506.00507, 2025. URL https:\\n//arxiv.org/abs/2506.00507v1.\\n[577] Dongyub Lee, Eunhwan Park, Hodong Lee, and Heuiseok Lim. Ask, assess, and refine: Rectifying\\nfactual consistency and hallucination in llms with metric-guided feedback learning. Conference of the\\nEuropean Chapter of the Association for Computational Linguistics, 2024.\\n[578] Eunhae Lee. Towards ethical personal ai applications: Practical considerations for ai assistants with\\nlong-term memory, arXiv preprint arXiv:2409.11192, 2024. URL https://arxiv.org/abs/\\n2409.11192v1.\\n[579] Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. Prompted\\nllms as chatbot modules for long open-domain conversation. In Findings of the Association for\\nComputational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. doi: 10.18653/\\nv1/2023.findings-acl.277. URL http://dx.doi.org/10.18653/v1/2023.findings-acl.\\n277.\\n[580] Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, and Sung Ju\\nHwang. A training-free sub-quadratic cost transformer model serving framework with hierarchically\\npruned attention, arXiv preprint arXiv:2406.09827, 2024. URL https://arxiv.org/abs/2406.\\n09827v3.\\n[581] Ho-Jun Lee, Junho Kim, Hyunjun Kim, and Yonghyun Ro. Refocus: Reinforcement-guided frame\\noptimization for contextual understanding, arXiv preprint arXiv:2506.01274v1, 2025. URL https:\\n//arxiv.org/abs/2506.01274v1.\\n[582] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and E. Brunskill.\\nSupervised pretraining can learn in-context reinforcement learning. Neural Information Processing\\nSystems, 2023.\\n[583] Namkyeong Lee, E. Brouwer, Ehsan Hajiramezanali, Chanyoung Park, and Gabriele Scalia. Rag-\\nenhanced collaborative llm agents for drug discovery, arXiv preprint arXiv:2502.17506, 2025. URL\\nhttps://arxiv.org/abs/2502.17506v2.\\n[584] Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, and M. Shin. Functionchat-bench:\\nComprehensive evaluation of language models’ generative capabilities in korean tool-use dialogs.\\narXiv preprint, 2024.\\n[585] Younghun Lee, Sungchul Kim, Ryan A. Rossi, Tong Yu, and Xiang Chen. Learning to reduce:\\nTowards improving performance of large language models on structured data, arXiv preprint\\narXiv:2407.02750, 2024. URL https://arxiv.org/abs/2407.02750v1.\\n[586] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. Learning to reduce:\\nOptimal representations of structured data in prompting large language models, arXiv preprint\\narXiv:2402.14195, 2024. URL https://arxiv.org/abs/2402.14195v1.\\n[587] Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, and Pei-Yuan Wu. An explanation of intrinsic self-\\ncorrection via linear representations and latent concepts, arXiv preprint arXiv:2505.11924, 2025.\\nURL https://arxiv.org/abs/2505.11924v1.\\n102'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 102}, page_content='[588] Melissa Lehman and Kenneth J. Malmberg. A buffer model of memory encoding and temporal\\ncorrelations in retrieval. Psychology Review, 2013.\\n[589] Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu,\\nand Yunhong Wang. Contextqformer: A new context modeling method for multi-turn multi-modal\\nconversations, arXiv preprint arXiv:2505.23121v1, 2025. URL https://arxiv.org/abs/2505.\\n23121v1.\\n[590] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\\ntuning. Conference on Empirical Methods in Natural Language Processing, 2021.\\n[591] Patrick Lewis, Ethan Perez, Aleksandara Piktus, F. Petroni, Vladimir Karpukhin, Naman Goyal,\\nHeinrich Kuttler, M. Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-intensive nlp tasks. Neural Information Processing\\nSystems, 2020.\\n[592] Bohan Li, Yutai Hou, and Wanxiang Che. Data augmentation approaches in natural language\\nprocessing: A survey. AI Open, 2021.\\n[593] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui,\\nLiangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored\\nsearch. Annual International ACM SIGIR Conference on Research and Development in Information\\nRetrieval, 2021.\\n[594] Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou\\nWang, Xiang Wang, Junyang Lin, and Dayiheng Liu. Cort: Code-integrated reasoning within thinking,\\narXiv preprint arXiv:2506.09820, 2025. URL https://arxiv.org/abs/2506.09820v2.\\n[595] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine,\\nFei-Fei Li, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented\\ncode emulator. International Conference on Machine Learning, 2023.\\n[596] Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuo-\\nran Yang. Stride: A tool-assisted llm agent framework for strategic and interactive decision-making,\\narXiv preprint arXiv:2405.16376, 2024. URL https://arxiv.org/abs/2405.16376v2.\\n[597] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix\\nYu, and Sanjiv Kumar. Large language models with controllable working memory, arXiv preprint\\narXiv:2211.05110, 2022. URL https://arxiv.org/abs/2211.05110.\\n[598] Daniel Li and Lincoln Murr. Humaneval on latest gpt models - 2024. arXiv preprint, 2024.\\n[599] Fu Li, Xueying Wang, Bin Li, Yunlong Wu, Yanzhen Wang, and Xiaodong Yi. A study on training and\\ndeveloping large language models for behavior tree generation, arXiv preprint arXiv:2401.08089,\\n2024. URL https://arxiv.org/abs/2401.08089v1.\\n[600] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-\\nseventh Conference on Neural Information Processing Systems, 2023.\\n103'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 103}, page_content='[601] Guozheng Li, Peng Wang, Jiajun Liu, Yikai Guo, Ke Ji, Ziyu Shang, and Zijie Xu. Meta in-context\\nlearning makes large language models better zero and few-shot relation extractors. International\\nJoint Conference on Artificial Intelligence, 2024.\\n[602] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and\\nskeleton parsing for text-to-sql. AAAI Conference on Artificial Intelligence, 2023.\\n[603] Jia Li, Ge Li, Yongming Li, and Zhi Jin. Structured chain-of-thought prompting for code generation.\\nACM Transactions on Software Engineering and Methodology, 2023.\\n[604] Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu. Graph intelligence with\\nlarge language models and prompt learning. Knowledge Discovery and Data Mining, 2024.\\n[605] Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. Omniactions: Predicting\\ndigital actions in response to real-world multimodal sensory inputs with llms. International Conference\\non Human Factors in Computing Systems, 2024.\\n[606] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hal-\\nlucinations: A case study on domain-specific queries in private knowledge-bases, arXiv preprint\\narXiv:2403.10446, 2024. URL https://arxiv.org/abs/2403.10446v1.\\n[607] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat\\nseng Chua, Siliang Tang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot\\ndemonstrative instructions. International Conference on Learning Representations, 2023.\\n[608] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and\\nS. Hoi. Align before fuse: Vision and language representation learning with momentum distillation.\\nNeural Information Processing Systems, 2021.\\n[609] Junnan Li, Dongxu Li, S. Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image\\npre-training with frozen image encoders and large language models. International Conference on\\nMachine Learning, 2023.\\n[610] Kun Li, Tianhua Zhang, Yunxiang Li, Hongyin Luo, Abdalla Moustafa, Xixin Wu, James Glass, and\\nHelen M. Meng. Generate, discriminate, evolve: Enhancing context faithfulness via fine-grained\\nsentence-level self-evolution, arXiv preprint arXiv:2503.01695, 2025. URL https://arxiv.org/\\nabs/2503.01695v1.\\n[611] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and\\nYu Qiao. Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355v2, 2023.\\nURL https://arxiv.org/abs/2305.06355v2.\\n[612] M Li, Y Zhao, B Yu, F Song, H Li, H Yu, and Z Li.... Api-bank: A comprehensive benchmark for\\ntool-augmented llms. 2023. URL https://arxiv.org/abs/2304.08244.\\n[613] Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo,\\nIsaac S. Kohane, and M. Zitnik. One patient, many contexts: Scaling medical ai through contextual\\nintelligence, arXiv preprint arXiv:2506.10157, 2025. URL https://arxiv.org/abs/2506.\\n10157v1.\\n104'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 104}, page_content='[614] Ming Li, Keyu Chen, Ziqian Bi, Ming Liu, Benji Peng, Qian Niu, Junyu Liu, Jinlang Wang, Sen\\nZhang, Xuanhe Pan, Jiawei Xu, and Pohsun Feng. Surveying the mllm landscape: A meta-review of\\ncurrent surveys, arXiv preprint arXiv:2409.18991, 2024. URL https://arxiv.org/abs/2409.\\n18991v1.\\n[615] Minghao Li, Feifan Song, Yu Bowen, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank:\\nA comprehensive benchmark for tool-augmented llms. Conference on Empirical Methods in Natural\\nLanguage Processing, 2023.\\n[616] Qiaomu Li and Ying Xie. From glue-code to protocols: A critical analysis of a2a and mcp integration\\nfor scalable agent systems, arXiv preprint arXiv:2505.03864, 2025. URL https://arxiv.org/\\nabs/2505.03864v1.\\n[617] Rongsheng Li, Jin Xu, Zhixiong Cao, Hai-Tao Zheng, and Hong-Gee Kim. Extending context window\\nin large language models with segmented base adjustment for rotary position embeddings. Applied\\nSciences, 2024.\\n[618] Shuaike Li, Kai Zhang, Qi Liu, and Enhong Chen. Mindbridge: Scalable and cross-model knowledge\\nediting via memory-augmented modality, arXiv preprint arXiv:2503.02701v1, 2025. URL https:\\n//arxiv.org/abs/2503.02701v1.\\n[619] Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao\\nMi, Dong Yu, and Wai Lam. Incomes: Integrating compression and selection mechanisms into llms\\nfor efficient model editing, arXiv preprint arXiv:2505.22156, 2025. URL https://arxiv.org/\\nabs/2505.22156v1.\\n[620] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. Large language\\nmodels can self-improve in long-context reasoning, arXiv preprint arXiv:2411.08147, 2024. URL\\nhttps://arxiv.org/abs/2411.08147v1.\\n[621] X Li, H Zou, and P Liu. Torl: Scaling tool-integrated rl. 2025. URL https://arxiv.org/abs/\\n2503.23383.\\n[622] Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao\\nWang, Zhaochen Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao.\\nA survey of personalization: From rag to agent, arXiv preprint arXiv:2504.10147, 2025. URL\\nhttps://arxiv.org/abs/2504.10147v1.\\n[623] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and\\nZhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability,\\narXiv preprint arXiv:2504.21776, 2025. URL https://arxiv.org/abs/2504.21776v1.\\n[624] Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Cheng Qian, Chuan Shi,\\nand Cheng Yang. Graphteam: Facilitating large language model-based graph analysis via multi-agent\\ncollaboration, arXiv preprint arXiv:2410.18032v4, 2024. URL https://arxiv.org/abs/2410.\\n18032v4.\\n[625] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems:\\nworkflow, infrastructure, and challenges. Vicinagearth, 2024.\\n105'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 105}, page_content='[626] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware\\nlanguage model pretraining improves dense retrieval on structured data. Annual Meeting of the\\nAssociation for Computational Linguistics, 2023.\\n[627] Yang Li, Jiacong He, Xiaoxia Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language\\ninstructions to mobile ui action sequences. Annual Meeting of the Association for Computational\\nLinguistics, 2020.\\n[628] Yinghao Li, R. Ramprasad, and Chao Zhang. A simple but effective approach to improve structured\\nlanguage model output for information extraction. Conference on Empirical Methods in Natural\\nLanguage Processing, 2024.\\n[629] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behav-\\niors for llm-based task-oriented coordination via collaborative generative agents, arXiv preprint\\narXiv:2310.06500, 2023. URL https://arxiv.org/abs/2310.06500.\\n[630] Yucheng Li.\\nUnlocking context constraints of llms: Enhancing context efficiency of llms with\\nself-information-based content filtering, arXiv preprint arXiv:2304.12102, 2023. URL https:\\n//arxiv.org/abs/2304.12102v1.\\n[631] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference\\nefficiency of large language models. Conference on Empirical Methods in Natural Language Processing,\\n2023.\\n[632] Zhaoxin Li, Xiaoming Zhang, Haifeng Zhang, and Chengxiang Liu. Refining interactions: Enhancing\\nanisotropy in graph neural networks with language semantics, arXiv preprint arXiv:2504.01429,\\n2025. URL https://arxiv.org/abs/2504.01429v1.\\n[633] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Naifan Cheung, Nanyun Peng, and Kai-Wei Chang.\\nThink carefully and check again! meta-generation unlocking llms for low-resource cross-lingual\\nsummarization. 2024.\\n[634] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, and Kai-Wei Chang. Drs: Deep\\nquestion reformulation with structured output. In Association for Computational Linguistics ACL,\\n2025., 2024.\\n[635] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun Peng, and Kai-Wei Chang.\\nVulnerability of llms to vertically aligned text manipulations. In Association for Computational\\nLinguistics ACL, 2025., 2024.\\n[636] Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, and Yiwei Wang. Texture or\\nsemantics? vision-language models get lost in font recognition. In Conference on Language Modeling\\nCOLM, 2025., 2025.\\n[637] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai,\\nJihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Ke-Rong\\nLi, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhiyang Xu, and Feiyu Xiong. Memos: An\\noperating system for memory-augmented generation (mag) in large language models, arXiv preprint\\narXiv:2505.22101, 2025. URL https://arxiv.org/abs/2505.22101v1.\\n[638] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. Lans: A layout-aware neural solver for\\nplane geometry problem. 2023.\\n106'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 106}, page_content='[639] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Fan-Hu Zeng,\\nJian Xu, Jia-Xin Zhang, and Cheng-Lin Liu. Cmmath: A chinese multi-modal math skill evaluation\\nbenchmark for foundation models. 2024.\\n[640] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian\\nXu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A survey of reasoning\\nlarge language models. 2025.\\n[641] Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei\\nHan, Le Sun, and Yongbin Li. Structrag: Boosting knowledge intensive reasoning of llms via inference-\\ntime hybrid information structurization. International Conference on Learning Representations, 2024.\\n[642] Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, F. Boussaid, Girish Dwivedi, Luqi Gong,\\nand Qiuhong Ke. Watch and listen: Understanding audio-visual-speech moments with multimodal\\nllm, arXiv preprint arXiv:2505.18110v2, 2025. URL https://arxiv.org/abs/2505.18110v2.\\n[643] Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan\\nLiang, Chengming Li, Zhenan Sun, Lingpeng Kong, and Ngai Wong. Uncertaintyrag: Span-level\\nuncertainty enhanced long-context modeling for retrieval-augmented generation, arXiv preprint\\narXiv:2410.02719, 2024. URL https://arxiv.org/abs/2410.02719v1.\\n[644] Zonglin Li, Ruiqi Guo, and Surinder Kumar. Decoupled context processing for context augmented\\nlanguage modeling. Neural Information Processing Systems, 2022.\\n[645] Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language\\nmodels: A survey. North American Chapter of the Association for Computational Linguistics, 2024.\\n[646] Wen li Yu and Junfeng Zhao. Quantum multi-agent reinforcement learning as an emerging ai\\ntechnology: A survey and future directions. International Conferences on Computing Advancements,\\n2023.\\n[647] Guannan Liang and Qianqian Tong. Llm-powered ai agent systems and their applications in industry,\\narXiv preprint arXiv:2505.16120, 2025. URL https://arxiv.org/abs/2505.16120v1.\\n[648] Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, and Ziyue Li. Reasoning rag via system 1 or\\nsystem 2: A survey on reasoning agentic retrieval-augmented generation for industry challenges,\\narXiv preprint arXiv:2506.10408, 2025. URL https://arxiv.org/abs/2506.10408v1.\\n[649] Xinnian Liang, Bing Wang, Huijia Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun\\nLi. Scm: Enhancing large language model with self-controlled memory framework, arXiv preprint\\narXiv:2304.13343, 2023. URL https://arxiv.org/abs/2304.13343v4.\\n[650] Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, and Jingsong Yang. Self-evolving\\nagents with reflective and memory-augmented abilities, arXiv preprint arXiv:2409.00872, 2024.\\nURL https://arxiv.org/abs/2409.00872v2.\\n[651] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Jingsong Yang, Tianyu\\nShi, Yuantao Wang, Miao Zhang, and Xueqian Wang. Mars: Memory-enhanced agents with reflective\\nself-improvement, arXiv preprint arXiv:2503.19271, 2025. URL https://arxiv.org/abs/2503.\\n19271v2.\\n107'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 107}, page_content='[652] Yanbiao Liang, Huihong Shi, Haikuo Shao, and Zhongfeng Wang. Accllm: Accelerating long-context\\nllm inference via algorithm-hardware co-design, arXiv preprint arXiv:2505.03745, 2025. URL\\nhttps://arxiv.org/abs/2505.03745v1.\\n[653] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yangyiwen Ou, Shuai Lu, Lei Ji,\\nShaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing\\ntasks by connecting foundation models with millions of apis. Intelligent Computing, 2023.\\n[654] Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng\\nLiu, Chenghua Lin, Lei Ma, Wenhao Huang, and Jiajun Zhang. I-sheep: Self-alignment of llm from\\nscratch through an iterative self-enhancement paradigm. arXiv preprint, 2024.\\n[655] Bingli Liao and Danilo Vasconcellos Vargas. Beyond kv caching: Shared attention for efficient llms.\\nNeurocomputing, 2024.\\n[656] Xiaoxuan Liao, Binrong Zhu, Jacky He, Guiran Liu, Hongye Zheng, and Jia Gao. A fine-tuning\\napproach for t5 using knowledge graphs to address complex tasks, arXiv preprint arXiv:2502.16484,\\n2025. URL https://arxiv.org/abs/2502.16484v1.\\n[657] David Lillis. Internalising interaction protocols as first-class programming elements in multi agent sys-\\ntems, arXiv preprint arXiv:1711.02634, 2017. URL https://arxiv.org/abs/1711.02634v1.\\n[658] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks\\nfor commonsense reasoning. Conference on Empirical Methods in Natural Language Processing, 2019.\\n[659] Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond english:\\nEvaluating and improving multilingual language models for commonsense reasoning. Annual\\nMeeting of the Association for Computational Linguistics, 2021.\\n[660] Bin Lin, Chen Zhang, Tao Peng, Hanyu Zhao, Wencong Xiao, Minmin Sun, Anmin Liu, Zhipeng\\nZhang, Lanbo Li, Xiafei Qiu, Shen Li, Zhigang Ji, Tao Xie, Yong Li, and Wei Lin. Infinite-llm:\\nEfficient llm service for long context with distattention and distributed kvcache, arXiv preprint\\narXiv:2401.02669, 2024. URL https://arxiv.org/abs/2401.02669.\\n[661] Jianhao Lin, Lexuan Sun, and Yixin Yan. Simulating macroeconomic expectations using llm agents,\\narXiv preprint arXiv:2505.17648, 2025. URL https://arxiv.org/abs/2505.17648v2.\\n[662] Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan\\nWang, Di Zhang, and Kun Gai. Just ask one more time! self-agreement improves reasoning of\\nlanguage models in (almost) all scenarios. Annual Meeting of the Association for Computational\\nLinguistics, 2023.\\n[663] Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao\\nHuang, and Yong-Jin Liu. Training of scaffolded language models with language supervision: A survey,\\narXiv preprint arXiv:2410.16392, 2024. URL https://arxiv.org/abs/2410.16392v2.\\n[664] Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu\\nZhou, Cheng Cheng, Yin Zhao, and Weinan Zhang. Hammer: Robust function-calling for on-\\ndevice language models via function masking, arXiv preprint arXiv:2410.04587, 2024. URL https:\\n//arxiv.org/abs/2410.04587v2.\\n108'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 108}, page_content='[665] Yu-Chen Lin, Akhilesh Kumar, Norman Chang, Wen-Liang Zhang, Muhammad Zakir, Rucha Apte,\\nHaiyang He, Chao Wang, and Jyh-Shing Roger Jang. Novel preprocessing technique for data\\nembedding in engineering code generation using large language model. 2024 IEEE LLM Aided Design\\nWorkshop (LAD), 2023.\\n[666] Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, and\\nYun-Nung Chen.\\nLlm inference enhanced by external knowledge: A survey, arXiv preprint\\narXiv:2505.24377, 2025. URL https://arxiv.org/abs/2505.24377v1.\\n[667] Jack W Lindsey and Ashok Litwin-Kumar. Selective consolidation of learning and memory via\\nrecall-gated plasticity. bioRxiv, 2024.\\n[668] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntax-\\nsensitive dependencies. Transactions of the Association for Computational Linguistics, 2016.\\n[669] Gili Lior, Yuval Shalev, Gabriel Stanovsky, and Ariel Goldstein. Computation or weight adaptation?\\nrethinking the role of plasticity in learning. bioRxiv, 2024.\\n[670] Bingyang Liu, Haoyi Zhang, Xiaohan Gao, Zichen Kong, Xiyuan Tang, Yibo Lin, Runsheng Wang,\\nand Ru Huang. Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive\\nanalog layout design. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,\\n2024.\\n[671] E. Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web\\ninterfaces using workflow-guided exploration. International Conference on Learning Representations,\\n2018.\\n[672] Guang-Da Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, K. Johnson, Jiliang Tang, and Rongrong Wang.\\nOn the intrinsic self-correction capability of llms: Uncertainty and latent concept, arXiv preprint\\narXiv:2406.02378, 2024. URL https://arxiv.org/abs/2406.02378v2.\\n[673] Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. Dual reasoning: A gnn-llm collaborative\\nframework for knowledge graph question answering, arXiv preprint arXiv:2406.01145, 2024. URL\\nhttps://arxiv.org/abs/2406.01145v2.\\n[674] Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han,\\nShuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Guanjing Xiong, Yong Liu,\\nand Hongsheng Li. Llm-powered gui agents in phone automation: Surveying progress and prospects,\\narXiv preprint arXiv:2504.19838, 2025. URL https://arxiv.org/abs/2504.19838v2.\\n[675] Hanchao Liu, Rong-Zhi Li, Weimin Xiong, Ziyu Zhou, and Wei Peng. Workteam: Constructing\\nworkflows from natural language with multi-agents. North American Chapter of the Association for\\nComputational Linguistics, 2025.\\n[676] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-\\ninfinite context. International Conference on Learning Representations, 2023.\\n[677] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Neural\\nInformation Processing Systems, 2023.\\n109'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 109}, page_content='[678] Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees G. M. Snoek, J. Sonke, and E. Gavves. Capo: Co-\\noperative plan optimization for efficient embodied multi-agent cooperation. International Conference\\non Learning Representations, 2024.\\n[679] Jun Liu, Ke Yu, Keliang Chen, Ke Li, Yuxinyue Qian, Xiaolian Guo, Haozhe Song, and Yinming Li.\\nAcps: Agent collaboration protocols for the internet of agents, arXiv preprint arXiv:2505.13523,\\n2025. URL https://arxiv.org/abs/2505.13523v1.\\n[680] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yil-\\ning Lou. Large language model-based agents for software engineering: A survey, arXiv preprint\\narXiv:2409.02977, 2024. URL https://arxiv.org/abs/2409.02977v1.\\n[681] Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yao-Shen Chen, Yue Wu,\\nand Jieping Ye. Enhancing llm’s cognition via structurization. Neural Information Processing Systems,\\n2024.\\n[682] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint,\\n2023.\\n[683] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory, arXiv preprint\\narXiv:2311.08719, 2023. URL https://arxiv.org/abs/2311.08719.\\n[684] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From llm to conversational\\nagent: A memory enhanced architecture with fine-tuning of large language models, arXiv preprint\\narXiv:2401.02777, 2024. URL https://arxiv.org/abs/2401.02777v2.\\n[685] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, F. Petroni, and Percy\\nLiang. Lost in the middle: How language models use long contexts. Transactions of the Association\\nfor Computational Linguistics, 2023.\\n[686] Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, and Jun Ma. Hm-rag: Hierar-\\nchical multi-agent multimodal retrieval augmented generation. arXiv preprint, 2025.\\n[687] Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui D’avid, and Monica S.\\nLam. Suql: Conversational search over structured and unstructured data with large language models.\\nNAACL-HLT, 2023.\\n[688] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang,\\nHonghao Fu, Chunrui Han, et al. Step1x-edit: A practical framework for general image editing.\\n2025.\\n[689] W Liu, X Huang, X Zeng, X Hao, S Yu, and D Li.... Toolace: Winning the points of llm function\\ncalling. 2024. URL https://arxiv.org/abs/2409.00920.\\n[690] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert:\\nEnabling language representation with knowledge graph. AAAI Conference on Artificial Intelligence,\\n2019.\\n110'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 110}, page_content='[691] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan,\\nZhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan\\nWu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming\\nTang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling,\\narXiv preprint arXiv:2409.00920, 2024. URL https://arxiv.org/abs/2409.00920v1.\\n[692] Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng\\nWang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, and Weinan Zhang. The real barrier to llm\\nagent usability is agentic roi, arXiv preprint arXiv:2505.17767, 2025. URL https://arxiv.org/\\nabs/2505.17767v1.\\n[693] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen,\\nBo Jiang, Aimin Zhou, and Liang He. Mathematical language models: A survey, arXiv preprint\\narXiv:2312.07622, 2023. URL https://arxiv.org/abs/2312.07622v4.\\n[694] Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, and JiaLi Liu. Echo: A large language model\\nwith temporal episodic memory, arXiv preprint arXiv:2502.16090, 2025. URL https://arxiv.\\norg/abs/2502.16090v1.\\n[695] Xu Liu, S. Ramirez, Petti T. Pang, C. Puryear, A. Govindarajan, K. Deisseroth, and S. Tonegawa.\\nOptogenetic stimulation of a hippocampal engram activates fear memory recall. Nature, 2012.\\n[696] Yang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. Finetuning generative large language models\\nwith discrimination instructions for knowledge graph completion. In International Semantic Web\\nConference, 2024.\\n[697] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi\\nHuang, and Bryan Hooi. Efficient inference for large reasoning models: A survey, arXiv preprint\\narXiv:2503.23077, 2025. URL https://arxiv.org/abs/2503.23077v2.\\n[698] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network for\\ntask-oriented agent collaboration, arXiv preprint arXiv:2310.02170, 2023. URL https://arxiv.\\norg/abs/2310.02170v2.\\n[699] Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Scaling external\\nknowledge input beyond context windows of llms via multi-agent collaboration, arXiv preprint\\narXiv:2505.21471, 2025. URL https://arxiv.org/abs/2505.21471v1.\\n[700] Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun\\nTang, and Lin Wang. Nature’s insight: A novel framework and comprehensive analysis of agentic\\nreasoning through the lens of neuroscience. arXiv preprint, 2025.\\n[701] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran\\nYao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles,\\nHuan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating\\nverifiable and diverse function-calling datasets. Neural Information Processing Systems, 2024.\\n[702] Leo S. Lo. The art and science of prompt engineering: A new literacy in the information age. Internet\\nReference Services Quarterly, 2023.\\n111'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 111}, page_content='[703] Joseph R. Loffredo and Suyeol Yun. Agent-enhanced large language models for researching political\\ninstitutions, arXiv preprint arXiv:2503.13524, 2025.\\nURL https://arxiv.org/abs/2503.\\n13524v1.\\n[704] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao\\nWang, Lifeng Shang, and Qun Liu. Self: Self-evolution with language feedback, arXiv preprint\\narXiv:2310.00533, 2023. URL https://arxiv.org/abs/2310.00533v4.\\n[705] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu.\\nMemochat: Tuning llms to use memos for consistent long-range open-domain conversation, arXiv\\npreprint arXiv:2308.08239, 2023. URL https://arxiv.org/abs/2308.08239.\\n[706] Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan\\nRajmohan, Dongmei Zhang, and Qi Zhang. Axis: Efficient human-agent-computer interaction with\\napi-first llm-based agents, arXiv preprint arXiv:2409.17140, 2025. URL https://arxiv.org/\\nabs/2409.17140.\\n[707] Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan\\nZhou, Guosheng Dong, Bin Cui, and Wentao Zhang. Datasculpt: Crafting data landscapes for\\nlong-context llms through multi-objective partitioning, arXiv preprint arXiv:2409.00997, 2024. URL\\nhttps://arxiv.org/abs/2409.00997v2.\\n[708] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A\\nco-design framework for enabling sparse attention using reconfigurable architecture. Micro, 2021.\\n[709] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Y. Wu, Song-Chun Zhu, and Jianfeng\\nGao. Chameleon: Plug-and-play compositional reasoning with large language models. Neural\\nInformation Processing Systems, 2023.\\n[710] Y Lu, H Yu, and D Khashabi. Gear: Augmenting language models with generalizable and efficient\\ntool resolution. 2023. URL https://arxiv.org/abs/2307.08775.\\n[711] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\\nprompts and where to find them: Overcoming few-shot prompt order sensitivity. Annual Meeting of\\nthe Association for Computational Linguistics, 2021.\\n[712] Yinquan Lu, H. Lu, Guirong Fu, and Qun Liu. Kelm: Knowledge enhanced pre-trained language repre-\\nsentations with message passing on hierarchical relational graphs, arXiv preprint arXiv:2109.04223,\\n2021. URL https://arxiv.org/abs/2109.04223v2.\\n[713] Elias Lumer, Anmol Gulati, V. K. Subbiah, Pradeep Honaganahalli Basavaraju, and James A. Burke.\\nScalemcp: Dynamic and auto-synchronizing model context protocol tools for llm agents, arXiv\\npreprint arXiv:2505.06416, 2025. URL https://arxiv.org/abs/2505.06416v1.\\n[714] Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, and Anima Anandkumar. Mini-sequence\\ntransformer:\\nOptimizing intermediate memory for long sequences training, arXiv preprint\\narXiv:2407.15892, 2024. URL https://arxiv.org/abs/2407.15892v4.\\n[715] Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu,\\nJiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, and Xia Hu. Autol2s: Auto long-\\nshort reasoning for efficient large language models, arXiv preprint arXiv:2505.22662, 2025. URL\\nhttps://arxiv.org/abs/2505.22662v1.\\n112'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 112}, page_content='[716] Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, and Yujun\\nZhang. Enhance graph alignment for large language models, arXiv preprint arXiv:2410.11370v1,\\n2024. URL https://arxiv.org/abs/2410.11370v1.\\n[717] Haoran Luo, E. Haihong, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng,\\nZemin Kuang, Meina Song, Yifan Zhu, and Anh Tuan Luu. Hypergraphrag: Retrieval-augmented\\ngeneration with hypergraph-structured knowledge representation. arXiv preprint, 2025.\\n[718] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and\\nDacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv\\npreprint, 2025.\\n[719] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi\\nChen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiaoming Luo, Wei Ju, Zhiping Xiao, Yifan Wang,\\nMengxue Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xianhong\\nWu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: A\\nsurvey on methodology, applications and challenges, arXiv preprint arXiv:2503.21460, 2025. URL\\nhttps://arxiv.org/abs/2503.21460v1.\\n[720] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and\\ninterpretable large language model reasoning. International Conference on Learning Representations,\\n2023.\\n[721] Renjie Luo, Jiaxi Li, Chen Huang, and Wei Lu. Through the valley: Path to effective long cot training\\nfor small language models, arXiv preprint arXiv:2506.07712, 2025. URL https://arxiv.org/\\nabs/2506.07712v1.\\n[722] Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and Wei Hu. Knowla: Enhancing parameter-efficient\\nfinetuning with knowledgeable adaptation. In Proceedings of the 2024 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics, 2024.\\n[723] Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, and Bin Dong. Prompt engineering\\nthrough the lens of optimal control. Journal of Machine Learning, 2023.\\n[724] Panagiotis Lymperopoulos and Vasanth Sarathy. Tools in the loop: Quantifying uncertainty of llm\\nquestion answering systems that use tools, arXiv preprint arXiv:2505.16113, 2025. URL https:\\n//arxiv.org/abs/2505.16113v1.\\n[725] Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, M. D. Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop:\\nA benchmark for deep research shopping agents, arXiv preprint arXiv:2506.02839, 2025. URL\\nhttps://arxiv.org/abs/2506.02839v1.\\n[726] Jianxiang Ma. Research on the role of llm in multi-agent systems: A survey. Applied and Computational\\nEngineering, 2024.\\n[727] Jie Ma, Zhitao Gao, Qianyi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun\\nSong, Jun Liu, Chen Zhang, and Li zhen Cui. Debate on graph: a flexible and reliable reasoning\\nframework for large language models, arXiv preprint arXiv:2409.03155, 2024. URL https://\\narxiv.org/abs/2409.03155v1.\\n113'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 113}, page_content='[728] Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan\\nShen, Peilin Ji, Juanjuan Li, Gang Wang, and Wanpeng Ma. Computational experiments meet large\\nlanguage model based agents: A survey and perspective, arXiv preprint arXiv:2402.00262, 2024.\\nURL https://arxiv.org/abs/2402.00262v1.\\n[729] Xin Ma, Yang Liu, Jingjing Liu, and Xiaoxu Ma. Mesa-extrapolation: A weave position encoding\\nmethod for enhanced extrapolation in llms. Neural Information Processing Systems, 2024.\\n[730] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval-\\naugmented large language models, arXiv preprint arXiv:2305.14283, 2023. URL https://arxiv.\\norg/abs/2305.14283v3.\\n[731] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke S.\\nZettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference\\nwith unlimited context length. Neural Information Processing Systems, 2024.\\n[732] Y Ma, Z Gou, J Hao, R Xu, S Wang, and L Pan.... Sciagent: Tool-augmented language models for\\nscientific reasoning. 2024. URL https://arxiv.org/abs/2402.11451.\\n[733] Zhiyuan Ma, Zhenya Huang, Jiayu Liu, Minmao Wang, Hongke Zhao, and Xin Li. Automated creation\\nof reusable and diverse toolsets for enhancing llm reasoning. AAAI Conference on Artificial Intelligence,\\n2025.\\n[734] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to\\nimprove gpt-3 after deployment. Conference on Empirical Methods in Natural Language Processing,\\n2022.\\n[735] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank\\nGupta, A. Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. Neural\\nInformation Processing Systems, 2023.\\n[736] Xinji Mai, Haotian Xu, W. Xing, Weinong Wang, Yingying Zhang, and Wenqiang Zhang. Agent rl\\nscaling law: Agent rl with spontaneous code execution for mathematical problem solving, arXiv\\npreprint arXiv:2505.07773, 2025. URL https://arxiv.org/abs/2505.07773v2.\\n[737] Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen, Vincent François-Lavet, R. V. Prasad, and\\nChris Verhoeven. Deep reinforcement learning versus evolution strategies: A comparative survey.\\nIEEE Transactions on Neural Networks and Learning Systems, 2021.\\n[738] D. Maldonado, Edison Cruz, Jackeline Abad Torres, P. Cruz, and Silvana del Pilar Gamboa Benitez.\\nMulti-agent systems: A survey about its components, framework and workflow. IEEE Access, 2024.\\n[739] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large\\nlanguage models. IEEE International Conference on Robotics and Automation, 2023.\\n[740] Jeremy R. Manning, Sean M. Polyn, G. Baltuch, B. Litt, and M. Kahana. Oscillatory patterns in\\ntemporal lobe reveal context reinstatement during memory search. Proceedings of the National\\nAcademy of Sciences of the United States of America, 2011.\\n114'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 114}, page_content='[741] Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. Advancing graph representa-\\ntion learning with large language models: A comprehensive survey of techniques, arXiv preprint\\narXiv:2402.05952v1, 2024. URL https://arxiv.org/abs/2402.05952v1.\\n[742] Amin Hosseiny Marani, Ulie Schnaithmann, Youngseo Son, Akil Iyer, Manas Paldhe, and Arushi\\nRaghuvanshi. Graph integrated language transformers for next action prediction in complex phone\\ncalls. North American Chapter of the Association for Computational Linguistics, 2024.\\n[743] Sophia Maria. Compass-v2 technical report, arXiv preprint arXiv:2504.15527, 2025. URL https:\\n//arxiv.org/abs/2504.15527v1.\\n[744] Viorica Marian and U. Neisser. Language-dependent recall of autobiographical memories. Journal of\\nexperimental psychology. General, 2000.\\n[745] S. Mariani and Andrea Omicini. Special issue “multi-agent systems”: Editorial. Applied Sciences,\\n2019.\\n[746] Vasilije Markovic, Lazar Obradović, L’aszl’o Hajdu, and Jovan Pavlović. Optimizing the interface\\nbetween knowledge graphs and llms for complex reasoning, arXiv preprint arXiv:2505.24478, 2025.\\nURL https://arxiv.org/abs/2505.24478v1.\\n[747] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Offer Akrabi, Aviad Sela, Asaf Adi,\\nand Nir Mashkif. Towards enterprise-ready computer using generalist agent. 2025.\\n[748] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent\\narchitectures for reasoning, planning, and tool calling: A survey, arXiv preprint arXiv:2404.11584,\\n2024. URL https://arxiv.org/abs/2404.11584v1.\\n[749] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E. Hernandez, Mythreye Venkatesan, Paul\\nWang, and Jason H. Moore. Kragen: a knowledge graph-enhanced rag framework for biomedical\\nproblem solving using large language models. Bioinformatics, 2024.\\n[750] Ryoga Matsuo, Stefan Uhlich, Arun Venkitaraman, Andrea Bonetti, Chia-Yu Hsieh, Ali Momeni,\\nLukas Mauch, Augusto Capone, Eisaku Ohbuchi, and Lorenzo Servadei. Schemato - an llm for\\nnetlist-to-schematic conversion. arXiv preprint, 2024.\\n[751] Costas Mavromatis, V. Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, C. Falout-\\nsos, and G. Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs.\\nECML/PKDD, 2023.\\n[752] James L. McClelland, B. McNaughton, and R. O’Reilly. Why there are complementary learning\\nsystems in the hippocampus and neocortex: insights from the successes and failures of connectionist\\nmodels of learning and memory. Psychology Review, 1995.\\n[753] R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\\nheuristics in natural language inference.\\nAnnual Meeting of the Association for Computational\\nLinguistics, 2019.\\n[754] Daniel McDuff, M. Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal,\\nYash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S. Mahdavi, Sushant\\nPrakash, Anupam Pathak, Christopher Semturs, Shwetak N. Patel, D. Webster, Ewa Dominowska,\\n115'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 115}, page_content='Juraj Gottweis, Joelle Barral, Katherine Chou, G. Corrado, Yossi Matias, Jacob Sunshine, A. Karthike-\\nsalingam, and Vivek Natarajan. Towards accurate differential diagnosis with large language models.\\nNature, 2023.\\n[755] AD McNaughton, G Ramalaxmi, A Kruel, and CR Knutson.... Cactus: Chemistry agent connecting\\ntool-usage to science, arxiv, 2024.\\n[756] Sushant Mehta, R. Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small\\nlanguage models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506.\\n09342v2.\\n[757] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent\\noperating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403.\\n16971v4.\\n[758] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng.\\nSlang: New concept\\ncomprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, page 12558–12575. Association for Computational Linguistics,\\n2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/\\n2024.emnlp-main.698.\\n[759] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard:\\nFine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684,\\n2024. URL https://arxiv.org/abs/2410.02684.\\n[760] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi\\nCheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint\\narXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597.\\n[761] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. \"not aligned\" is not\\n\"malicious\": Being careful about hallucinations of large language models’ jailbreak, arXiv preprint\\narXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668.\\n[762] T. Meiser and A. Bröder. Memory for multidimensional source information. Journal of Experimental\\nPsychology. Learning, Memory and Cognition, 2002.\\n[763] Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\\nmodels. International Conference on Learning Representations, 2017.\\n[764] Kevin Meng, David Bau, A. Andonian, and Yonatan Belinkov. Locating and editing factual associations\\nin gpt. Neural Information Processing Systems, 2022.\\n[765] Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, and Jiwei Li. Gnn-lm:\\nLanguage modeling based on global contexts via gnn. International Conference on Learning Represen-\\ntations, 2021.\\n[766] Agnieszka Mensfelt, Kostas Stathis, and Vince Trencsenyi. Towards logically sound natural language\\nreasoning with logic-enhanced language model agents, arXiv preprint arXiv:2408.16081, 2024. URL\\nhttps://arxiv.org/abs/2408.16081v2.\\n[767] G. M. Mensink and J. Raaijmakers. A model for interference and forgetting. arXiv preprint, 1988.\\n116'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 116}, page_content='[768] Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting:\\nImproving and accelerating retrieval-augmented generation. International Conference on Machine\\nLearning, 2024.\\n[769] B. Meskó. Prompt engineering as an important emerging skill for medical professionals: Tutorial.\\nJournal of Medical Internet Research, 2023.\\n[770] Yapeng Mi, Zhi Gao, Xiaojian Ma, and Qing Li. Building llm agents by incorporating insights from\\ncomputer systems, arXiv preprint arXiv:2504.04485, 2025. URL https://arxiv.org/abs/\\n2504.04485v1.\\n[771] G. Mialon, Roberto Dessì, M. Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, R. Raileanu,\\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and\\nThomas Scialom. Augmented language models: a survey. Trans. Mach. Learn. Res., 2023.\\n[772] G. Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom.\\nGaia: a benchmark for general ai assistants, arXiv preprint arXiv:2311.12983, 2023. URL https:\\n//arxiv.org/abs/2311.12983v1.\\n[773] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao\\nJia. Towards efficient generative large language model serving: A survey from algorithms to systems,\\narXiv preprint arXiv:2312.15234, 2023. URL https://arxiv.org/abs/2312.15234v1.\\n[774] Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for language modeling. arXiv\\npreprint, 2020.\\n[775] Xing ming Guo, Darioush Keivan, U. Syed, Lianhui Qin, Huan Zhang, G. Dullerud, Peter J. Seiler,\\nand Bin Hu. Controlagent: Automating control system design via novel integration of llm agents\\nand domain expertise, arXiv preprint arXiv:2410.19811, 2024. URL https://arxiv.org/abs/\\n2410.19811v1.\\n[776] Soroush Mirjalili, Patrick S. Powell, Jonathan Strunk, Taylor A James, and Audrey Duarte. Context\\nmemory encoding and retrieval temporal dynamics are modulated by attention across the adult\\nlifespan. eNeuro, 2021.\\n[777] Ishan Misra and L. Maaten. Self-supervised learning of pretext-invariant representations. Computer\\nVision and Pattern Recognition, 2019.\\n[778] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards a general\\nread-write memory for large language models, arXiv preprint arXiv:2305.14322, 2024. URL https:\\n//arxiv.org/abs/2305.14322.\\n[779] Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. Memllm:\\nFinetuning llms to use an explicit read-write memory. Trans. Mach. Learn. Res., 2024.\\n[780] Behnam Mohammadi. Pel, a programming language for orchestrating ai agents, arXiv preprint\\narXiv:2505.13453, 2025. URL https://arxiv.org/abs/2505.13453v2.\\n[781] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\\nlength for transformers. Neural Information Processing Systems, 2023.\\n117'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 117}, page_content='[782] Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Martin Jaggi. Skill: Structured knowledge infusion\\nfor large language models. North American Chapter of the Association for Computational Linguistics,\\n2022.\\n[783] Dimitri Coelho Mollo and Raphael Milliere.\\nThe vector grounding problem, arXiv preprint\\narXiv:2304.01481, 2023. URL https://arxiv.org/abs/2304.01481v2.\\n[784] Nieves Montes, N. Osman, and C. Sierra. Combining theory of mind and abduction for cooperation\\nunder imperfect information. European Workshop on Multi-Agent Systems, 2022.\\n[785] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, and\\nA. Gholami. Efficient and scalable estimation of tool representations in vector space, arXiv preprint\\narXiv:2409.02141, 2024. URL https://arxiv.org/abs/2409.02141v1.\\n[786] Shinsuke Mori. A stochastic parser based on an slm with arboreal context trees. International\\nConference on Computational Linguistics, 2002.\\n[787] Meredith Ringel Morris. Prompting considered harmful. Communications of the ACM, 2024.\\n[788] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, D. Klakow, and Yanai Elazar. Few-shot fine-tuning\\nvs. in-context learning: A fair comparison and evaluation. Annual Meeting of the Association for\\nComputational Linguistics, 2023.\\n[789] Sajad Mousavi, Ricardo Luna Guti’errez, Desik Rengarajan, Vineet Gundecha, Ashwin Ramesh Babu,\\nAvisek Naug, Antonio Guillen-Perez, and S. Sarkar. N-critics: Self-refinement of large language\\nmodels with ensemble of critics. arXiv preprint, 2023.\\n[790] Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, and Tung Mai. From documents\\nto dialogue: Building kg-rag enhanced ai assistants, arXiv preprint arXiv:2502.15237, 2025. URL\\nhttps://arxiv.org/abs/2502.15237v1.\\n[791] Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, and Se young Yun. Self-\\ntraining elicits concise reasoning in large language models, arXiv preprint arXiv:2502.20122, 2025.\\nURL https://arxiv.org/abs/2502.20122v3.\\n[792] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient\\ninfinite context transformers with infini-attention, arXiv preprint arXiv:2404.07143, 2024. URL\\nhttps://arxiv.org/abs/2404.07143v2.\\n[793] Eliya Nachmani, Alon Levkovitch, Julián Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad,\\nR. Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech con-\\ntinuation using spectrogram-powered llm. International Conference on Learning Representations,\\n2023.\\n[794] L. Nadel, Jessica D. Payne, and W. J. Jacobs. The relationship between episodic memory and context:\\nclues from memory errors made while under stress. Physiological Research, 2002.\\n[795] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\\nBrowser-assisted question-answering with human feedback, arXiv preprint arXiv:2112.09332, 2022.\\nURL https://arxiv.org/abs/2112.09332.\\n118'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 118}, page_content='[796] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging\\npixel-level semantic knowledge in diffusion models, arXiv preprint arXiv:2401.11739, 2024. URL\\nhttps://arxiv.org/abs/2401.11739.\\n[797] Sundaraparipurnan Narayanan and Sandeep Vishwakarma. Guard-d-llm: An llm-based risk assess-\\nment engine for the downstream uses of llms. arXiv preprint, 2024.\\n[798] Usman Naseem, Surendrabikram Thapa, Qi Zhang, Liang Hu, Anum Masood, and Mehwish Nasim.\\nReducing knowledge noise for improved semantic analysis in biomedical natural language processing\\napplications. Clinical Natural Language Processing Workshop, 2023.\\n[799] Deepak Nathani, David Wang, Liangming Pan, and W. Wang. Maf: Multi-aspect feedback for\\nimproving reasoning in large language models. Conference on Empirical Methods in Natural Language\\nProcessing, 2023.\\n[800] Aashutosh Nema, Samaksh Gulati, Evangelos Giakoumakis, and Bipana Thapaliya. Modp: Multi\\nobjective directional prompting, arXiv preprint arXiv:2504.18722, 2025. URL https://arxiv.\\norg/abs/2504.18722v1.\\n[801] Christian D. Newman, Anthony S Peruma, and Reem S. Alsuhaibani. Modeling the relationship\\nbetween identifier name and behavior. IEEE International Conference on Software Maintenance and\\nEvolution, 2019.\\n[802] M. Nieznański, Michał Obidziński, Emilia Zyskowska, and Daria Niedziałkowska. Executive resources\\nand item-context binding: Exploring the influence of concurrent inhibition, updating, and shifting\\ntasks on context memory. Advances in Cognitive Psychology, 2015.\\n[803] M. Nieznański, Michał Obidziński, and Daria Ford. Does context recollection depend on the base-rate\\nof contextual features? Cognitive Processing, 2023.\\n[804] C. Nourani and P. Eklund. Concepts ontology algebras and role descriptions. Conference on Computer\\nScience and Information Systems, 2017.\\n[805] Felix Ocker, Daniel Tanneberg, Julian Eggert, and Michael Gienger. Tulip agent - enabling llm-based\\nagents to solve tasks using large tool libraries. arXiv preprint, 2024.\\n[806] Felix Ocker, J. Deigmöller, Pavel Smirnov, and Julian Eggert. A grounded memory system for smart\\npersonal assistants, arXiv preprint arXiv:2505.06328, 2025. URL https://arxiv.org/abs/\\n2505.06328v1.\\n[807] OpenAI.\\nComputer-using\\nagent,\\n2025.\\nURL\\nhttps://openai.com/index/\\ncomputer-using-agent/. OpenAI Technical Report.\\n[808] OpenAI. Swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration.\\nhttps://github.com/openai/swarm, 2025. [Online; accessed 17-July-2025].\\n[809] Jonas Oppenlaender. Dangermaps: Personalized safety advice for travel in urban environments\\nusing a retrieval-augmented language model, arXiv preprint arXiv:2503.14103, 2025. URL https:\\n//arxiv.org/abs/2503.14103v3.\\n[810] A. Orhan. Recognition, recall, and retention of few-shot memories in large language models, arXiv\\npreprint arXiv:2303.17557, 2023. URL https://arxiv.org/abs/2303.17557v1.\\n119'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 119}, page_content='[811] Gustavo Ortiz-Hernández, Alejandro Guerra-Hernández, J. Hübner, and W. A. Luna-Ramírez. Mod-\\nularization in belief-desire-intention agent programming and artifact-based environments. PeerJ\\nComputer Science, 2022.\\n[812] Wendkûuni C. Ouédraogo, A. Kaboré, Haoye Tian, Yewei Song, Anil Koyuncu, Jacques Klein, David\\nLo, and Tegawend’e F. Bissyand’e. Large-scale, independent and comprehensive study of the power\\nof llms for test case generation. arXiv preprint, 2024.\\n[813] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez.\\nMemgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2023. URL https:\\n//arxiv.org/abs/2310.08560v2.\\n[814] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E.\\nGonzalez. Memgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2024.\\nURL https://arxiv.org/abs/2310.08560.\\n[815] Constantin-Valentin Pal, F. Leon, M. Paprzycki, and M. Ganzha. A review of platforms for the\\ndevelopment of agent systems. Inf., 2020.\\n[816] Qianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shilian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min\\nZhang, Yulan Wu, and Liang He. A survey of slow thinking-based reasoning llms using reinforced\\nlearning and inference-time scaling law, arXiv preprint arXiv:2505.02665, 2025. URL https:\\n//arxiv.org/abs/2505.02665v2.\\n[817] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large\\nlanguage models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data\\nEngineering, 2023.\\n[818] Xu Pan, Ely Hahami, Zechen Zhang, and H. Sompolinsky. Memorization and knowledge injection\\nin gated llms, arXiv preprint arXiv:2504.21239, 2025. URL https://arxiv.org/abs/2504.\\n21239v1.\\n[819] Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. Bolt: Bootstrap\\nlong chain-of-thought in language models without distillation, arXiv preprint arXiv:2502.03860,\\n2025. URL https://arxiv.org/abs/2502.03860v1.\\n[820] Jianhui Pang, Fanghua Ye, Derek F. Wong, and Longyue Wang. Anchor-based large language models.\\nAnnual Meeting of the Association for Computational Linguistics, 2024.\\n[821] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and\\nMarco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models,\\narXiv preprint arXiv:2303.09014, 2023. URL https://arxiv.org/abs/2303.09014v1.\\n[822] A Parisi, Y Zhao, and N Fiedel. Talm: Tool augmented language models. 2022. URL https:\\n//arxiv.org/abs/2205.12255.\\n[823] Dongju Park and Chang Wook Ahn. Self-supervised contextual data augmentation for natural\\nlanguage processing. Symmetry, 2019.\\n[824] J. Park, Lindsay Popowski, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Social\\nsimulacra: Creating populated prototypes for social computing systems. ACM Symposium on User\\nInterface Software and Technology, 2022.\\n120'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 120}, page_content='[825] J. Park, Joseph C. O’Brien, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Generative\\nagents: Interactive simulacra of human behavior. ACM Symposium on User Interface Software and\\nTechnology, 2023.\\n[826] Soya Park, J. Zamfirescu-Pereira, and Chinmay Kulkarni. Model behavior specification by leveraging\\nllm self-playing and self-improving, arXiv preprint arXiv:2503.03967, 2025. URL https://arxiv.\\norg/abs/2503.03967v1.\\n[827] Rajvardhan Patil and Venkat Gudivada. A review of current trends, techniques, and challenges in\\nlarge language models (llms). Applied Sciences, 2024.\\n[828] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model\\nconnected with massive apis, arXiv preprint arXiv:2305.15334, 2023. URL https://arxiv.org/\\nabs/2305.15334.\\n[829] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and\\nJoseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic\\nevaluation of large language models. In Forty-second International Conference on Machine Learning,\\n2025.\\n[830] Shuva Paul, Farhad Alemi, and Richard Macwan. Llm-assisted proactive threat intelligence for\\nautomated reasoning, arXiv preprint arXiv:2504.00428, 2025. URL https://arxiv.org/abs/\\n2504.00428v1.\\n[831] Saurav Pawar, S. Tonmoy, S. M. M. Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what,\\nwhy, and how of context length extension techniques in large language models - a detailed survey.\\narXiv preprint, 2024.\\n[832] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and\\nSiliang Tang. Graph retrieval-augmented generation: A survey. ArXiv, abs/2408.08921, 2024. URL\\nhttps://api.semanticscholar.org/CorpusID:271903170.\\n[833] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\\nextension of large language models. International Conference on Learning Representations, 2023.\\n[834] Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu, Maosong Sun, and Jie Zhou.\\nLearning from context or names? an empirical study on neural relation extraction. Conference on\\nEmpirical Methods in Natural Language Processing, 2020.\\n[835] Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung\\nChen. A survey of useful llm evaluation, arXiv preprint arXiv:2406.00936, 2024. URL https:\\n//arxiv.org/abs/2406.00936v1.\\n[836] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and\\nJonathan J. Halcrow. Let your graph do the talking: Encoding structured data for llms, arXiv preprint\\narXiv:2402.05862, 2024. URL https://arxiv.org/abs/2402.05862v1.\\n[837] E. Pesce and G. Montana. Improving coordination in small-scale multi-agent deep reinforcement\\nlearning through memory-driven communication. Machine-mediated learning, 2019.\\n121'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 121}, page_content='[838] F. Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and\\nSebastian Riedel. How context affects language models’ factual predictions. Conference on Automated\\nKnowledge Base Construction, 2020.\\n[839] Yue Pi, Wang Zhang, Yong Zhang, Hairong Huang, Baoquan Rao, Yulong Ding, and Shuanghua\\nYang. Applications of multi-agent deep reinforcement learning communication in network manage-\\nment: A survey, arXiv preprint arXiv:2407.17030, 2024. URL https://arxiv.org/abs/2407.\\n17030v1.\\n[840] Nancirose Piazza and Vahid Behzadan. A theory of mind approach as test-time mitigation against\\nemergent adversarial communication. Adaptive Agents and Multi-Agent Systems, 2023.\\n[841] Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman,\\nSebastian Michelmann, Alexander Huth, and Mariya Toneva. Assessing episodic memory in llms\\nwith sequence order recall tasks, arXiv preprint arXiv:2410.08133, 2024. URL https://arxiv.\\norg/abs/2410.08133v1.\\n[842] Fahmida Liza Piya and Rahmatollah Beheshti. Advancing feature extraction in healthcare through\\nthe integration of knowledge graphs and large language models. AAAI Conference on Artificial\\nIntelligence, 2025.\\n[843] A. Plaat, M. V. Duijn, N. V. Stein, Mike Preuss, P. V. D. Putten, and K. Batenburg. Agentic large\\nlanguage models, a survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/\\nabs/2503.23037v2.\\n[844] Moritz Plenz and Anette Frank. Graph language models. Annual Meeting of the Association for\\nComputational Linguistics, 2024.\\n[845] Sean M. Polyn, K. Norman, and M. Kahana. A context maintenance and retrieval model of organiza-\\ntional processes in free recall. Psychology Review, 2009.\\n[846] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chain-\\nof-thought prompting: Pedagogical strategies for machines. International Conference on Computer\\nSupported Education, 2025.\\n[847] V Porcu. The role of memory in llms: Persistent context for smarter conversations. Int. J. Sci. Res.\\nManag.(IJSRM), 12:1673–1691, 2024.\\n[848] Ofir Press, Noah A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables\\ninput length extrapolation. International Conference on Learning Representations, 2021.\\n[849] Xavier Puig, K. Ra, Marko Boben, Jiaman Li, Tingwu Wang, S. Fidler, and A. Torralba. Virtualhome:\\nSimulating household activities via programs. 2018 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2018.\\n[850] Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, and Rafael\\nRafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, arXiv preprint\\narXiv:2408.07199, 2024. URL https://arxiv.org/abs/2408.07199v1.\\n[851] S. Qasim, Hassan Mahmood, and F. Shafait. Rethinking table recognition using graph neural networks.\\nIEEE International Conference on Document Analysis and Recognition, 2019.\\n122'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 122}, page_content='[852] Peng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. Answering open-domain\\nquestions of varying reasoning steps from text. Conference on Empirical Methods in Natural Language\\nProcessing, 2020.\\n[853] Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng\\nWang, and Shuo Jiang. Safety control of service robots with llms and embodied knowledge graphs,\\narXiv preprint arXiv:2405.17846, 2024. URL https://arxiv.org/abs/2405.17846v1.\\n[854] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang,\\nShuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents\\nvia self-evolving online curriculum reinforcement learning, arXiv preprint arXiv:2411.02337, 2024.\\nURL https://arxiv.org/abs/2411.02337v3.\\n[855] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen,\\nYusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative\\nagents for software development, arXiv preprint arXiv:2307.07924, 2024. URL https://arxiv.\\norg/abs/2307.07924.\\n[856] Cheng Qian, Chi Han, Y. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for\\ndisentangling abstract and concrete reasoning of large language models. Conference on Empirical\\nMethods in Natural Language Processing, 2023.\\n[857] Cheng Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang,\\nYingli Zhang, Zhiyuan Liu, and Maosong Sun.\\nIterative experience refinement of software-\\ndeveloping agents, arXiv preprint arXiv:2405.04219, 2024. URL https://arxiv.org/abs/\\n2405.04219v1.\\n[858] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur,\\nand Heng Ji. Toolrl: Reward is all tool learning needs, arXiv preprint arXiv:2504.13958, 2025. URL\\nhttps://arxiv.org/abs/2504.13958v1.\\n[859] Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, and Defu Lian. Boosting long-context\\nmanagement via query-guided activation refilling, arXiv preprint arXiv:2412.12486, 2024. URL\\nhttps://arxiv.org/abs/2412.12486v3.\\n[860] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang.\\nMemorag: Boosting long context processing with global memory-enhanced retrieval augmentation,\\narXiv preprint arXiv:2409.05591, 2025. URL https://arxiv.org/abs/2409.05591.\\n[861] Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng\\nYang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Hongyi Wang, Huang\\nYu, Yifei Hu, Guang Li, Guangyao Chen, Hao Ye, Lijun Sun, and Diange Yang. Agentthink: A\\nunified framework for tool-augmented chain-of-thought reasoning in vision-language models for\\nautonomous driving, arXiv preprint arXiv:2505.15298, 2025. URL https://arxiv.org/abs/\\n2505.15298v3.\\n[862] Changze Qiao and Mingming Lu. Efficiently enhancing general agents with hierarchical-categorical\\nmemory, arXiv preprint arXiv:2505.22006, 2025.\\nURL https://arxiv.org/abs/2505.\\n22006v1.\\n123'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 123}, page_content='[863] S Qiao, H Gui, C Lv, Q Jia, H Chen, and N Zhang. Making language models better tool learners with\\nexecution feedback. 2023. URL https://arxiv.org/abs/2305.13068.\\n[864] Binjie Qin, Haohao Mao, Ruipeng Zhang, Y. Zhu, Song Ding, and Xu Chen. Working memory\\ninspired hierarchical video decomposition with transformative representations, arXiv preprint\\narXiv:2204.10105, 2022. URL https://arxiv.org/abs/2204.10105v3.\\n[865] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao,\\nJian Sun, Luo Si, Fei Huang, and Yongbin Li. A survey on text-to-sql parsing: Concepts, methods,\\nand future directions, arXiv preprint arXiv:2208.13629, 2022. URL https://arxiv.org/abs/\\n2208.13629v1.\\n[866] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\\nChaojun Xiao, Chi Han, Y. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun\\nZhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu,\\nZhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han,\\nXian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong\\nSun. Tool learning with foundation models. ACM Computing Surveys, 2023.\\n[867] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu,\\nand Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis.\\nInternational Conference on Learning Representations, 2023.\\n[868] Zhen Qin and Yiran Zhong. Accelerating toeplitz neural network with constant-time inference\\ncomplexity. Conference on Empirical Methods in Natural Language Processing, 2023.\\n[869] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng\\nKong, and Yiran Zhong. Toeplitz neural network for sequence modeling. International Conference on\\nLearning Representations, 2023.\\n[870] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2:\\nA free lunch for handling unlimited sequence lengths in large language models. arXiv preprint, 2024.\\n[871] Jiahao Qiu, Xinzhe Juan, Yiming Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu\\nLu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, and Mengdi Wang. Agentdistill:\\nTraining-free agent distillation with generalizable mcp boxes, arXiv preprint arXiv:2506.14728,\\n2025. URL https://arxiv.org/abs/2506.14728v1.\\n[872] Jiahao Qiu, Fulian Xiao, Yiming Wang, Yuchen Mao, Yijia Chen, Xinzhe Juan, Siran Wang, Xuan\\nQi, Tongcheng Zhang, Zixin Yao, Jiacheng Guo, Yifu Lu, Charles Argon, Jundi Cui, Daixin Chen,\\nJunran Zhou, Shuyao Zhou, Zhanpeng Zhou, Ling Yang, Shilong Liu, Hongru Wang, Kaixuan\\nHuang, Xun Jiang, Yuming Cao, Yue Chen, Yunfei Chen, Zhengyi Chen, Ruowei Dai, Mengqiu\\nDeng, Jiye Fu, Yu Gu, Zijie Guan, Zirui Huang, Xiaoyan Ji, Yumeng Jiang, Delong Kong, Haolong\\nLi, Jiaqi Li, Ruipeng Li, Tianze Li, Zhuo-Yang Li, Haixia Lian, Meng Lin, Xudong Liu, Jiayi Lu,\\nJinghan Lu, Wanyu Luo, Ziyue Luo, Zihao Pu, Zhi Qiao, Rui-Fang Ren, Liang Wan, Ruixiang Wang,\\nTianhui Wang, Yang Wang, Zeyu Wang, Zihua Wang, Yujia Wu, Zhaoyi Wu, Hao Xin, Weiao Xing,\\nRuojun Xiong, Weijie Xu, Yao Shu, Xiao Yao, Xiaorui Yang, Yuchen Yang, Nan Yi, Jiadong Yu, Yang\\nYu, Huiting Zeng, Danni Zhang, Yunjie Zhang, Zhaoyu Zhang, Zhiheng Zhang, Xiaofeng Zheng,\\nPeirong Zhou, Li-Ying Zhong, Xiaoyin Zong, Ying Zhao, Zhen Chen, Lin Ding, Xiaoyu Gao, Bingbing\\n124'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 124}, page_content='Gong, Yichao Li, Yang Liao, Guang Ma, Tianyuan Ma, Xinrui Sun, Tianyi Wang, Han Xia, Ruobing\\nXian, Gen Ye, Tengfei Yu, Wentao Zhang, Yuxi Wang, Xi Gao, and Mengdi Wang. On path to\\nmultimodal historical reasoning: Histbench and histagent, arXiv preprint arXiv:2505.20246, 2025.\\nURL https://arxiv.org/abs/2505.20246v3.\\n[873] Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, and Bing Li. Autobench: Automatic\\ntestbench generation and evaluation using llms for hdl design. Workshop on Machine Learning for\\nCAD, 2024.\\n[874] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Jirong\\nWen. Tool learning with large language models: A survey. Frontiers Comput. Sci., 2024.\\n[875] Xiaoye Qu, Yafu Li, Zhao yu Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong\\nLiu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, Xian-\\nSheng Hua, Bowen Zhou, and Yu Cheng.\\nA survey of efficient reasoning for large reasoning\\nmodels: Language, multimodality, and beyond, arXiv preprint arXiv:2503.21614, 2025.\\nURL\\nhttps://arxiv.org/abs/2503.21614v1.\\n[876] Victor Quintanar-Zilinskas. A neuromimetic approach to the serial acquisition, long-term storage,\\nand selective utilization of overlapping memory engrams. bioRxiv, 2019.\\n[877] Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom Kouwenhoven, and Tessa\\nVerhoef. Memory-augmented generative adversarial transformers, arXiv preprint arXiv:2402.19218,\\n2024. URL https://arxiv.org/abs/2402.19218.\\n[878] Ella Rabinovich and Ateret Anaby-Tavor. On the robustness of agentic function calling. Proceedings\\nof the 5th Workshop on Trustworthy NLP (TrustNLP 2025), 2025.\\n[879] Neil C. Rabinowitz, Frank Perbet, H. F. Song, Chiyuan Zhang, S. Eslami, and M. Botvinick. Machine\\ntheory of mind. International Conference on Machine Learning, 2018.\\n[880] Zackary Rackauckas. Rag-fusion: a new take on retrieval-augmented generation. International\\nJournal on Natural Language Computing, 2024.\\n[881] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and I. Sutskever. Learning\\ntransferable visual models from natural language supervision. International Conference on Machine\\nLearning, 2021.\\n[882] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\\ntransformer. Journal of machine learning research, 2019.\\n[883] Mohaimenul Azam Khan Raiaan, Md. Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad\\nFahad, S. Sakib, Most. Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami\\nAzam. A review on large language models: Architectures, applications, taxonomies, open issues and\\nchallenges. IEEE Access, 2024.\\n[884] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian,\\nand S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv\\npreprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1.\\n125'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 125}, page_content='[885] Sumedh Rasal. An artificial neuron for enhanced problem solving in large language models, arXiv\\npreprint arXiv:2404.14222, 2024. URL https://arxiv.org/abs/2404.14222v1.\\n[886] Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis. Trism for agentic ai:\\nA review of trust, risk, and security management in llm-based agentic multi-agent systems, arXiv\\npreprint arXiv:2506.04133, 2025. URL https://arxiv.org/abs/2506.04133v2.\\n[887] Jing Ren and Feng Xia. Brain-inspired artificial intelligence: A comprehensive review, arXiv preprint\\narXiv:2408.14811, 2024. URL https://arxiv.org/abs/2408.14811v1.\\n[888] Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, and Jiajun Zhang. Towards scientific\\nintelligence: A survey of llm-based scientific agents, arXiv preprint arXiv:2503.24047, 2025. URL\\nhttps://arxiv.org/abs/2503.24047v2.\\n[889] Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, and Chao Huang. A survey of large language\\nmodels for graphs. Knowledge Discovery and Data Mining, 2024.\\n[890] Yi Ren, Shangmin Guo, Linlu Qiu, Bailin Wang, and Danica J. Sutherland. Bias amplification in\\nlanguage model evolution: An iterated learning perspective. Neural Information Processing Systems,\\n2024.\\n[891] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical\\nschemas: Dynamic tree memory representation for llms. International Conference on Learning\\nRepresentations, 2024.\\n[892] Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consistency\\nof question-answering models. Annual Meeting of the Association for Computational Linguistics, 2019.\\n[893] Yara Rizk, Abhishek Bhandwalder, S. Boag, Tathagata Chakraborti, Vatche Isahagian, Y. Khazaeni,\\nFalk Pollock, and Merve Unuvar. A unified conversational assistant framework for business process\\nautomation, arXiv preprint arXiv:2001.03543, 2020. URL https://arxiv.org/abs/2001.\\n03543v1.\\n[894] Yara Rizk, Vatche Isahagian, S. Boag, Y. Khazaeni, Merve Unuvar, Vinod Muthusamy, and Rania Y.\\nKhalaf. A conversational digital assistant for intelligent process automation. International Conference\\non Business Process Management, 2020.\\n[895] S. Rizvi, Nazreen Pallikkavaliyaveetil, David Zhang, Zhuoyang Lyu, Nhi Nguyen, Haoran Lyu,\\nB. Christensen, J. O. Caro, Antonio H. O. Fonseca, E. Zappala, Maryam Bagherian, Christopher\\nAverill, C. Abdallah, Amin Karbasi, Rex Ying, M. Brbic, R. M. Dhodapkar, and David van Dijk.\\nFimp: Foundation model-informed message passing for graph neural networks, arXiv preprint\\narXiv:2210.09475v5, 2022. URL https://arxiv.org/abs/2210.09475v5.\\n[896] Joshua Robinson, Christopher Rytting, and D. Wingate. Leveraging large language models for\\nmultiple choice question answering. International Conference on Learning Representations, 2022.\\n[897] Juri Di Rocco, D. D. Ruscio, Claudio Di Sipio, P. T. Nguyen, and Riccardo Rubei. On the use of large\\nlanguage models in model-driven engineering. Journal of Software and Systems Modeling, 2024.\\n[898] Juan David Salazar Rodriguez, Sam Conrad Joyce, and Julfendi Julfendi. Using customized gpt to\\ndevelop prompting proficiency in architectural ai-generated images, arXiv preprint arXiv:2504.13948,\\n2025. URL https://arxiv.org/abs/2504.13948v2.\\n126'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 126}, page_content='[899] Albert Roethel, M. Ganzha, and Anna Wr’oblewska. Enriching language models with graph-based\\ncontext information to better understand textual data. Electronics, 2023.\\n[900] Reudismam Rolim, Gustavo Soares, Rohit Gheyi, and Loris D’antoni. Learning quick fixes from code\\nrepositories. Brazilian Symposium on Software Engineering, 2018.\\n[901] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and B. Ommer. High-resolution image\\nsynthesis with latent diffusion models. Computer Vision and Pattern Recognition, 2021.\\n[902] Hayley Ross, A. Mahabaleshwarkar, and Yoshi Suhara. When2call: When (not) to call tools. North\\nAmerican Chapter of the Association for Computational Linguistics, 2025.\\n[903] J. Rosser and Jakob N. Foerster. Agentbreeder: Mitigating the ai safety impact of multi-agent scaffolds,\\narXiv preprint arXiv:2502.00757, 2025. URL https://arxiv.org/abs/2502.00757v3.\\n[904] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Review of multi-agent\\nalgorithms for collective behavior: a structural taxonomy, arXiv preprint arXiv:1803.05464, 2018.\\nURL https://arxiv.org/abs/1803.05464v1.\\n[905] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Multi-agent algorithms\\nfor collective behavior: A structural and application-focused atlas, arXiv preprint arXiv:2103.11067,\\n2021. URL https://arxiv.org/abs/2103.11067v1.\\n[906] Alex Roxin and Stefano Fusi. Efficient partitioning of memory systems and its importance for memory\\nconsolidation. PLoS Comput. Biol., 2013.\\n[907] Kaushik Roy, Yuxin Zi, Vignesh Narayanan, Manas Gaur, and Amit P. Sheth. Knowledge-infused self\\nattention transformers, arXiv preprint arXiv:2306.13501, 2023. URL https://arxiv.org/abs/\\n2306.13501v1.\\n[908] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng,\\nCunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He,\\nZhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. Ragchecker: A fine-grained framework for\\ndiagnosing retrieval-augmented generation. Neural Information Processing Systems, 2024.\\n[909] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu\\nMao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task\\nplanning and tool usage, arXiv preprint arXiv:2308.03427, 2023. URL https://arxiv.org/\\nabs/2308.03427.\\n[910] M. Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, and\\nWaseem Alshikh. Writing in the margins: Better inference pattern for long context retrieval, arXiv\\npreprint arXiv:2408.14906, 2024. URL https://arxiv.org/abs/2408.14906v1.\\n[911] Hyun Ryu and Eric Kim. Closer look at efficient inference methods: A survey of speculative decoding,\\narXiv preprint arXiv:2411.13157, 2024. URL https://arxiv.org/abs/2411.13157v2.\\n[912] Iman Saberi and Fatemeh Fard. Context-augmented code generation using programming knowl-\\nedge graphs, arXiv preprint arXiv:2410.18251, 2024. URL https://arxiv.org/abs/2410.\\n18251v2.\\n127'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 127}, page_content='[913] Abdulfattah Safa and Gözde Gül Sahin. A zero-shot open-vocabulary pipeline for dialogue under-\\nstanding. North American Chapter of the Association for Computational Linguistics, 2024.\\n[914] Alireza Akhavan Safaei, Pegah Saboori, Reza Ramezani, and Mohammadali Nematbakhsh. Kglm-qa:\\nA novel approach for knowledge graph-enhanced large language models for question answering.\\nConference on Information and Knowledge Technology, 2024.\\n[915] Avirup Saha, Lakshmi Mandal, Balaji Ganesan, Sambit Ghosh, Renuka Sindhgatta, Carlos Eberhardt,\\nDan Debrunner, and Sameep Mehta. Sequential api function calling using graphql schema. Conference\\non Empirical Methods in Natural Language Processing, 2024.\\n[916] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, and Aman Chadha. A\\nsystematic survey of prompt engineering in large language models: Techniques and applications,\\narXiv preprint arXiv:2402.07927, 2024. URL https://arxiv.org/abs/2402.07927v2.\\n[917] Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yas-\\nsine Benajiba. Meminsight: Autonomous memory augmentation for llm agents, arXiv preprint\\narXiv:2503.21760, 2025. URL https://arxiv.org/abs/2503.21760.\\n[918] Jefferson Salan, Devyn E Smith, Erica S Shafer, and Rachel A Diana. Variation in encoding context\\nbenefits item recognition. Memory & Cognition, 2024.\\n[919] Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, S. Dustdar, Susanna\\nPirttikangas, and Lauri Lov’en. Usercentrix: An agentic memory-augmented ai framework for\\nsmart spaces, arXiv preprint arXiv:2505.00472, 2025. URL https://arxiv.org/abs/2505.\\n00472v1.\\n[920] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context\\nimpersonation reveals large language models’ strengths and biases. Neural Information Processing\\nSystems, 2023.\\n[921] A. Samsonovich. Toward a unified catalog of implemented cognitive architectures. Biologically\\nInspired Cognitive Architectures, 2010.\\n[922] Narendra Reddy Sanikommu. Model context protocol: Enhancing llm performance for observability\\nand analytics. European journal of computer science and information technology, 2025.\\n[923] S. Santhanam. Context based text-generation using lstm networks, arXiv preprint arXiv:2005.00048,\\n2020. URL https://arxiv.org/abs/2005.00048v1.\\n[924] G. Santos, Rita Maria Silva Julia, and Marcelo Zanchetta do Nascimento. Diverse prompts: Illumi-\\nnating the prompt space of large language models with map-elites. IEEE Congress on Evolutionary\\nComputation, 2025.\\n[925] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee.\\nAi agents vs. agentic ai: A\\nconceptual taxonomy, applications and challenges, arXiv preprint arXiv:2505.10468, 2025. URL\\nhttps://arxiv.org/abs/2505.10468v4.\\n[926] Anjana Sarkar and Soumyendu Sarkar. Survey of llm agent communication with mcp: A software\\ndesign pattern centric review, arXiv preprint arXiv:2506.05364, 2025. URL https://arxiv.org/\\nabs/2506.05364v1.\\n128'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 128}, page_content='[927] Soumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence llm\\npretraining on diverse table data tasks, arXiv preprint arXiv:2310.00789, 2023. URL https:\\n//arxiv.org/abs/2310.00789v1.\\n[928] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning.\\nRaptor: Recursive abstractive processing for tree-organized retrieval. International Conference on\\nLearning Representations, 2024.\\n[929] Gabriele Sarti. Umberto-mtsa @ accompl-it: Improving complexity and acceptability prediction\\nwith multi-task learning on self-supervised annotations (short paper). International Workshop on\\nEvaluation of Natural Language and Speech Tools for Italian, 2020.\\n[930] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph\\ncompletion and question answering. Annual Meeting of the Association for Computational Linguistics,\\n2022.\\n[931] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, R. Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.\\nNeural Information Processing Systems, 2023.\\n[932] Guido Schillaci, Uwe Schmidt, and Luis Miranda. Prediction error-driven memory consolidation for\\ncontinual learning: On the case of adaptive greenhouse models. KI - Künstliche Intelligenz, 35(1):\\n71–80, 2021. ISSN 1610-1987. doi: 10.1007/s13218-020-00700-8. URL http://dx.doi.org/\\n10.1007/s13218-020-00700-8.\\n[933] Florian Schneider, Narges Baba Ahmadi, Niloufar Baba Ahmadi, Iris Vogel, Martin Semmann, and\\nChristian Biemann. Collex - a multimodal agentic rag system enabling interactive exploration of\\nscientific collections. arXiv preprint, 2025.\\n[934] Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan,\\nZahin Sufiyan, Osmar R. Zaiane, and Matthew E. Taylor. The evolving landscape of llm- and vlm-\\nintegrated reinforcement learning. arXiv preprint, 2025.\\n[935] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu.\\nCognitive memory in large\\nlanguage models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504.\\n02441v2.\\n[936] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:\\nQuery, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv.\\norg/abs/2404.14809v2.\\n[937] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,\\narXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.\\n[938] Yutong Shao and N. Nakashole. On linearizing structured data in encoder-decoder language models:\\nInsights from text-to-sql. North American Chapter of the Association for Computational Linguistics,\\n2024.\\n[939] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic\\nprompting: Generating chain-of-thought demonstrations for large language models. International\\nConference on Machine Learning, 2023.\\n129'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 129}, page_content='[940] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi\\nKhandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow\\ninstructions via graphical user interfaces. Neural Information Processing Systems, 2023.\\n[941] Jonathan Shen, Ruoming Pang, Ron J. Weiss, M. Schuster, N. Jaitly, Zongheng Yang, Z. Chen,\\nYu Zhang, Yuxuan Wang, R. Skerry-Ryan, R. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu.\\nNatural tts synthesis by conditioning wavenet on mel spectrogram predictions. IEEE International\\nConference on Acoustics, Speech, and Signal Processing, 2017.\\n[942] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet\\nTalwalkar. Scribeagent: Towards specialized web agents using production-scale workflow data.\\n2024.\\n[943] Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples,\\nNan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason\\nby scaling test-time interaction, arXiv preprint arXiv:2506.07976, 2025. URL https://arxiv.\\norg/abs/2506.07976.\\n[944] Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi,\\nYuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, and Ming Yan.\\nQwenlong-cprs: Towards ∞-llms with dynamic context optimization. arXiv preprint, 2025.\\n[945] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Y. Zhuang. Hugginggpt:\\nSolving ai tasks with chatgpt and its friends in hugging face. Neural Information Processing Systems,\\n2023.\\n[946] Zhuocheng Shen. Llm with tools: A survey, arXiv preprint arXiv:2409.18807, 2024. URL https:\\n//arxiv.org/abs/2409.18807v1.\\n[947] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie,\\nBeidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang.\\nHigh-throughput generative inference of large language models with a single gpu. International\\nConference on Machine Learning, 2023.\\n[948] Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong,\\nTianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun\\nWang, Y. Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, arXiv\\npreprint arXiv:2506.10055, 2025. URL https://arxiv.org/abs/2506.10055v2.\\n[949] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and J. Kwok. Sparsebert:\\nRethinking the importance analysis in self-attention. International Conference on Machine Learning,\\n2021.\\n[950] Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, C. D. Santos, and\\nBing Xiang. Learning contextual representations for semantic parsing with generation-augmented\\npre-training. AAAI Conference on Artificial Intelligence, 2020.\\n[951] Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and S. Yih. Trusting your\\nevidence: Hallucinate less with context-aware decoding. North American Chapter of the Association\\nfor Computational Linguistics, 2023.\\n130'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 130}, page_content='[952] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin\\nChen, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models\\nas automatic tool agents. The Web Conference, 2024.\\n[953] Jay Shim, Grant Kruttschnitt, Alyssa Ma, Daniel Kim, Benjamin Chek, Athul Anand, Kevin Zhu,\\nand Sean O’Brien. Chain-of-thought augmentation with logit contrast for enhanced reasoning in\\nlanguage models, arXiv preprint arXiv:2407.03600, 2024. URL https://arxiv.org/abs/2407.\\n03600v2.\\n[954] Jiho Shin, Reem Aleithan, Hadi Hemmati, and Song Wang. Retrieval-augmented test generation:\\nHow far are we?, arXiv preprint arXiv:2409.12682, 2024. URL https://arxiv.org/abs/2409.\\n12682v1.\\n[955] Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim,\\nKyunghyun Cho, Gichang Lee, W. Park, Jung-Woo Ha, and Nako Sung.\\nOn the effect of pre-\\ntraining corpora on in-context learning by a large-scale language model. North American Chapter of\\nthe Association for Computational Linguistics, 2022.\\n[956] Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: language agents with verbal reinforcement learning. Neural Information Processing\\nSystems, 2023.\\n[957] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis\\non spatial reasoning capabilities of large multimodal models. Conference on Empirical Methods in\\nNatural Language Processing, 2024.\\n[958] Masoud Shokrnezhad, Hao Yu, T. Taleb, Renwei Li, Kyunghan Lee, Jaeseung Song, and Cedric\\nWestphal. Toward a dynamic future with adaptable computing and network convergence (acnc).\\nIEEE Network, 2024.\\n[959] Connor Shorten, T. Khoshgoftaar, and B. Furht. Text data augmentation for deep learning. Journal\\nof Big Data, 2021.\\n[960] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\\nLuke Zettlemoyer, and D. Fox. Alfred: A benchmark for interpreting grounded instructions for\\neveryday tasks. Computer Vision and Pattern Recognition, 2019.\\n[961] Altun Shukurlu. Improving deep knowledge tracing via gated architectures and adaptive optimization,\\narXiv preprint arXiv:2504.20070, 2025. URL https://arxiv.org/abs/2504.20070v1.\\n[962] Lynn L Siegel and M. Kahana. A retrieved context account of spacing and repetition effects in free\\nrecall. Journal of Experimental Psychology. Learning, Memory and Cognition, 2014.\\n[963] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and T. T.\\nKhoei. Exploring prompt engineering: A systematic review with swot analysis, arXiv preprint\\narXiv:2410.12843, 2024. URL https://arxiv.org/abs/2410.12843v1.\\n[964] Anmolika Singh and Yuhang Diao. Leveraging large language models for optimized item categoriza-\\ntion using unspsc taxonomy. International Journal on Cybernetics & Informatics, 2024.\\n131'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 131}, page_content='[965] Joykirat Singh, Raghav Magazine, Yash Pandya, and A. Nambi. Agentic reasoning and tool integration\\nfor llms via reinforcement learning, arXiv preprint arXiv:2505.01441, 2025. URL https://arxiv.\\norg/abs/2505.01441v1.\\n[966] Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, and Stefan Roth.\\nIs synthetic data all we need? benchmarking the robustness of models trained with synthetic images.\\n2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2024.\\n[967] Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna Bairi, Aditya Kanade,\\nand Nagarajan Natarajan. Code researcher: Deep research agent for large systems code and\\ncommit history, arXiv preprint arXiv:2506.11060, 2025. URL https://arxiv.org/abs/2506.\\n11060v1.\\n[968] Aarush Sinha and CU Omkumar. Gmlm: Bridging graph neural networks and language models for\\nheterophilic node classification, arXiv preprint arXiv:2503.05763, 2025. URL https://arxiv.\\norg/abs/2503.05763v3.\\n[969] Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, and Aidong Zhang. Maml-\\nen-llm: Model agnostic meta-training of llms for improved in-context learning. Knowledge Discovery\\nand Data Mining, 2024.\\n[970] Colin Sisate, Alistair Goldfinch, Vincent Waterstone, Sebastian Kingsley, and Mariana Black-\\nthorn. Contextually entangled gradient mapping for optimized llm comprehension, arXiv preprint\\narXiv:2502.00048, 2025. URL https://arxiv.org/abs/2502.00048v1.\\n[971] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web\\nactions, arXiv preprint arXiv:2310.03720, 2024. URL https://arxiv.org/abs/2310.03720.\\n[972] Manthankumar Solanki. Efficient document retrieval with g-retriever. arXiv preprint, 2025.\\n[973] Karthik Soman, Peter W Rose, John H Morris, Rabia E Akbas, Brett Smith, Braian Peetoom, Catalina\\nVillouta-Reyes, G. Cerono, Yongmei Shi, Angela Rizk-Jackson, Sharat Israni, Charlotte A. Nelson,\\nSui Huang, and Sergio Baranzini. Biomedical knowledge graph-optimized prompt generation for\\nlarge language models. Bioinformatics, 2023.\\n[974] Lilian Some, Wenli Yang, Michael Bain, and Byeong Kang. A comprehensive survey on integrating\\nlarge language models with knowledge-based methods. Knowledge-Based Systems, 2025.\\n[975] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llm-\\nplanner: Few-shot grounded planning for embodied agents with large language models. IEEE\\nInternational Conference on Computer Vision, 2022.\\n[976] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and\\nJi-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\\narXiv preprint, 2025.\\n[977] Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and\\nJinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms.\\nInternational Conference on Learning Representations, 2024.\\n132'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 132}, page_content='[978] Woomin Song, Sai Muralidhar Jayanthi, S. Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin,\\nA. Galstyan, Shubham Katiyar, and S. Bodapati. Compress, gather, and recompute: Reforming\\nlong-context processing in transformers, arXiv preprint arXiv:2506.01215, 2025. URL https:\\n//arxiv.org/abs/2506.01215v1.\\n[979] Yewei Song, Xunzhu Tang, Cedric Lothritz, Saad Ezzini, Jacques Klein, Tegawend’e F. Bissyand’e,\\nA. Boytsov, Ulrick Ble, and Anne Goujon. Callnavi, a challenge and empirical study on llm function\\ncalling and routing, arXiv preprint arXiv:2501.05255, 2025. URL https://arxiv.org/abs/\\n2501.05255v2.\\n[980] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web agents,\\narXiv preprint arXiv:2410.16464, 2025. URL https://arxiv.org/abs/2410.16464.\\n[981] S. Srinivasa and Jayati Deshmukh. Paradigms of computational agency. Novel Approaches to Informa-\\ntion Systems Design, 2021.\\n[982] B. Staresina, R. Henson, N. Kriegeskorte, and Arjen Alink. Episodic reinstatement in the medial\\ntemporal lobe. Journal of Neuroscience, 2012.\\n[983] T. Staudigl, C. Vollmar, S. Noachtar, and S. Hanslmayr. Temporal-pattern similarity analysis reveals\\nthe beneficial and detrimental effects of context reinstatement on human memory. Journal of\\nNeuroscience, 2015.\\n[984] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness? an\\nanalysis of cot in planning. Neural Information Processing Systems, 2024.\\n[985] R. Sterken and James Ravi Kirkpatrick. Conversational alignment with artificial intelligence in\\ncontext. Philosophical Perspectives, 2025.\\n[986] Paul Stoewer, Achim Schilling, Andreas K. Maier, and Patrick Krauss. Multi-modal cognitive maps\\nbased on neural networks trained on successor representations, arXiv preprint arXiv:2401.01364,\\n2023. URL https://arxiv.org/abs/2401.01364v1.\\n[987] Olly Styles, Sam Miller, Patricio Cerda-Mardini, T. Guha, Victor Sanchez, and Bertie Vidgen.\\nWorkbench: a benchmark dataset for agents in a realistic workplace setting, arXiv preprint\\narXiv:2405.00823, 2024. URL https://arxiv.org/abs/2405.00823v2.\\n[988] Guangxin Su, Yifan Zhu, Wenjie Zhang, Hanchen Wang, and Ying Zhang. Bridging large language\\nmodels and graph structure learning models for robust representation learning, arXiv preprint\\narXiv:2410.12096, 2024. URL https://arxiv.org/abs/2410.12096v1.\\n[989] Hong Su, Elke A. Rundensteiner, and Murali Mani. Automaton in or out: run-time plan optimization\\nfor xml stream processing. International Symposium on Signal Processing Systems, 2008.\\n[990] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö. Arık. Learn-by-interact:\\nA data-centric framework for self-adaptive agents in realistic environments. 2025.\\n[991] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthink-\\ning: An empirical study of reasoning length and correctness in llms, arXiv preprint arXiv:2505.00127,\\n2025. URL https://arxiv.org/abs/2505.00127v1.\\n133'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 133}, page_content='[992] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval\\naugmented generation based on the real-time information needs of large language models. Annual\\nMeeting of the Association for Computational Linguistics, 2024.\\n[993] Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, and Phillip Howard. Sk-vqa: Synthetic\\nknowledge generation at scale for training context-augmented multimodal llms. arXiv preprint,\\n2024.\\n[994] Budhitama Subagdja and A. Tan. Neural modeling of sequential inferences and learning over episodic\\nmemory. Neurocomputing, 2015.\\n[995] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi\\nLiu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: A survey on\\nefficient reasoning for large language models, arXiv preprint arXiv:2503.16419, 2025. URL https:\\n//arxiv.org/abs/2503.16419v3.\\n[996] Chuanneng Sun, Songjun Huang, and D. Pompili. Llm-based multi-agent reinforcement learning:\\nCurrent and future directions, arXiv preprint arXiv:2405.11106, 2024. URL https://arxiv.org/\\nabs/2405.11106v1.\\n[997] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless\\nacceleration of long sequence generation with hierarchical speculative decoding, arXiv preprint\\narXiv:2404.11912, 2024. URL https://arxiv.org/abs/2404.11912v3.\\n[998] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive\\nplanning from feedback with language models. Neural Information Processing Systems, 2023.\\n[999] Jiankai Sun, Chuanyang Zheng, E. Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu\\nDing, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe\\nRen, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang,\\nP. Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Jingwei Wen, Xipeng Qiu, Yi-Chen Guo, Hui Xiong,\\nQun Liu, and Zhenguo Li. A survey of reasoning with foundation models: Concepts, methodologies,\\nand outlook. ACM Computing Surveys, 2023.\\n[1000] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Sai Wang, Chen Lin, Yeyun Gong, H. Shum, and Jian\\nGuo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge\\ngraph. arXiv preprint, 2023.\\n[1001] Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. Oda: Observation-driven agent for integrating\\nllms and knowledge graphs, arXiv preprint arXiv:2404.07677, 2024. URL https://arxiv.org/\\nabs/2404.07677.\\n[1002] Lei Sun, Xinchen Wang, and Youdi Li. Pyramid-driven alignment: Pyramid principle guided integra-\\ntion of large language models and knowledge graphs, arXiv preprint arXiv:2410.12298, 2024. URL\\nhttps://arxiv.org/abs/2410.12298v2.\\n[1003] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-\\nmodal conversational agents on mobile gui. Conference on Empirical Methods in Natural Language\\nProcessing, 2022.\\n134'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 134}, page_content='[1004] Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, and\\nYang Shen.\\nMulti-agent coordination across diverse applications: A survey, arXiv preprint\\narXiv:2502.14743, 2025. URL https://arxiv.org/abs/2502.14743v2.\\n[1005] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and B. Schiele. Meta-transfer learning for few-shot learning.\\nComputer Vision and Pattern Recognition, 2018.\\n[1006] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie\\n2.0: A continual pre-training framework for language understanding. AAAI Conference on Artificial\\nIntelligence, 2019.\\n[1007] Rao\\nSurapaneni,\\nMiku\\nJha,\\nMichael\\nVakoc,\\nand\\nTodd\\nSegal.\\nAnnouncing\\nthe\\nagent2agent\\nprotocol\\n(a2a).\\nhttps://developers.googleblog.com/en/\\na2a-a-new-era-of-agent-interoperability/, April 2025.\\n[Online;\\naccessed 17-\\nJuly-2025].\\n[1008] Stefan Szeider. Mcp-solver: Integrating language models with constraint programming systems.\\narXiv preprint, 2024.\\n[1009] Daniel Szelogowski. Engram memory encoding and retrieval: A neurocomputational perspective,\\narXiv preprint arXiv:2506.01659, 2025. URL https://arxiv.org/abs/2506.01659v1.\\n[1010] N. Taatgen, David Huss, D. Dickison, and John R. Anderson. The acquisition of robust and flexible\\ncognitive skills. Journal of experimental psychology. General, 2008.\\n[1011] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard\\nSchwarz. Online adaptation of language models with a memory of amortized contexts. Neural\\nInformation Processing Systems, 2024.\\n[1012] Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, and Ziwei Liu. Link-context learning for\\nmultimodal llms. Computer Vision and Pattern Recognition, 2023.\\n[1013] Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J. Z. Kolter, Jeff Schneider,\\nand Ruslan Salakhutdinov. Training a generally curious agent, arXiv preprint arXiv:2502.17543,\\n2025. URL https://arxiv.org/abs/2502.17543v3.\\n[1014] K. Tallam. From autonomous agents to integrated systems, a new paradigm: Orchestrated distributed\\nintelligence, arXiv preprint arXiv:2503.13754, 2025. URL https://arxiv.org/abs/2503.\\n13754v2.\\n[1015] A. Tan, Budhitama Subagdja, Di Wang, and Lei Meng. Self-organizing neural networks for universal\\nlearning and multimodal memory encoding. Neural Networks, 2019.\\n[1016] Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, and Wenliang Chen. Make a choice! knowledge\\nbase question answering with in-context learning, arXiv preprint arXiv:2305.13972, 2023. URL\\nhttps://arxiv.org/abs/2305.13972v1.\\n[1017] Sijun Tan, Xiuyu Li, Shishir G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez,\\nand Raluca A. Popa. Lloco: Learning long contexts offline. Conference on Empirical Methods in\\nNatural Language Processing, 2024.\\n135'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 135}, page_content='[1018] Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, and Yuan Qi. Struct-x:\\nEnhancing large language models reasoning with structured data. arXiv preprint, 2024.\\n[1019] Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research\\nand future directions. IEEE Data Engineering Bulletin, 2023.\\n[1020] Zhijie Tan, Xu Chu, Weiping Li, and Tong Mo.\\nOrder matters: Exploring order sensitivity in\\nmultimodal large language models, arXiv preprint arXiv:2410.16983v1, 2024. URL https://\\narxiv.org/abs/2410.16983v1.\\n[1021] Matthew Tancik, Pratul P. Srinivasan, B. Mildenhall, Sara Fridovich-Keil, N. Raghavan, Utkarsh\\nSinghal, R. Ramamoorthi, J. Barron, and Ren Ng. Fourier features let networks learn high frequency\\nfunctions in low dimensional domains. Neural Information Processing Systems, 2020.\\n[1022] Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang\\nHou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, and Yueting Zhuang. A\\nsurvey on (m)llm-based gui agents, arXiv preprint arXiv:2504.13865, 2025. URL https://arxiv.\\norg/abs/2504.13865v2.\\n[1023] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.\\nGraphgpt: Graph instruction tuning for large language models. Annual International ACM SIGIR\\nConference on Research and Development in Information Retrieval, 2023.\\n[1024] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.\\nGraphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International\\nACM SIGIR Conference on Research and Development in Information Retrieval, pages 491–500, 2024.\\n[1025] Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and Jia Li. Grapharena: Evaluating and exploring\\nlarge language models on graph computation. arXiv preprint arXiv:2407.00379, 2024.\\n[1026] Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark\\nGerstein. Struc-bench: Are large language models good at generating complex structured tabular\\ndata? North American Chapter of the Association for Computational Linguistics, 2024.\\n[1027] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou,\\nPan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, and Mark Gerstein. Chemagent: Self-updating\\nlibrary in large language models improves chemical reasoning, arXiv preprint arXiv:2501.06590,\\n2025. URL https://arxiv.org/abs/2501.06590v1.\\n[1028] Xuemei Tang, Jun Wang, and Q. Su. Chinese word segmentation with heterogeneous graph neural net-\\nwork, arXiv preprint arXiv:2201.08975, 2022. URL https://arxiv.org/abs/2201.08975v1.\\n[1029] Yiqing Tang, Xingyuan Dai, Chengchong Zhao, Qi Cheng, and Yisheng Lv. Large language model-\\ndriven urban traffic signal control. Australian and New Zealand Control Conference, 2024.\\n[1030] Yongjian Tang, Rakebul Hasan, and Thomas Runkler. Fsponer: Few-shot prompt optimization for\\nnamed entity recognition in domain-specific scenarios. European Conference on Artificial Intelligence,\\n2024.\\n[1031] Yunlong Tang, Daiki Shimada, Jing Bi, Hang Hua, and Chenliang Xu. Empowering llms with pseudo-\\nuntrimmed videos for audio-visual temporal understanding. AAAI Conference on Artificial Intelligence,\\n2024.\\n136'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 136}, page_content='[1032] Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, and Yunhe Wang. Saliency-driven\\ndynamic token pruning for large language models, arXiv preprint arXiv:2504.04514, 2025. URL\\nhttps://arxiv.org/abs/2504.04514v2.\\n[1033] Denis Tarasov and Kumar Shridhar. Distilling llms’ decomposition abilities into compact language mod-\\nels, arXiv preprint arXiv:2402.01812, 2024. URL https://arxiv.org/abs/2402.01812v1.\\n[1034] Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, and Kunat Pipatanakul.\\nTyphoon t1: An open thai reasoning model, arXiv preprint arXiv:2502.09042, 2025. URL https:\\n//arxiv.org/abs/2502.09042v2.\\n[1035] Yi Tay, Anh Tuan Luu, Minh C. Phan, and S. Hui. Multi-task neural network for non-discrete attribute\\nprediction in knowledge graphs. International Conference on Information and Knowledge Management,\\n2017.\\n[1036] 36Kr Editorial Team. The future of ai: From parameter scaling to context scaling. Online, 2025.\\nURL https://36kr.com/p/3337269379328264. Chinese business and technology media pub-\\nlication discussing context scaling in large language models.\\n[1037] Junfeng Tian, Da Zheng, Yang Cheng, Rui Wang, Colin Zhang, and Debing Zhang. Untie the knots:\\nAn efficient data augmentation strategy for long-context pre-training in language models, arXiv\\npreprint arXiv:2409.04774, 2024. URL https://arxiv.org/abs/2409.04774v1.\\n[1038] S Tian, R Wang, H Guo, P Wu, Y Dong, and X Wang.... Ego-r1: Chain-of-tool-thought for ultra-long\\negocentric video reasoning. 2025. URL https://arxiv.org/abs/2506.13654.\\n[1039] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang,\\nHao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric\\nvideo reasoning. arXiv preprint, 2025.\\n[1040] Ramine Tinati, Xin Wang, Ian C. Brown, T. Tiropanis, and W. Hall. A streaming real-time web\\nobservatory architecture for monitoring the health of social machines. The Web Conference, 2015.\\n[1041] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization\\nwithout overfitting: Analyzing the training dynamics of large language models. Neural Information\\nProcessing Systems, 2022.\\n[1042] Despina Tomkou, George Fatouros, Andreas Andreou, Georgios Makridis, F. Liarokapis, Dimitrios\\nDardanis, Athanasios Kiourtis, John Soldatos, and D. Kyriazis. Bridging industrial expertise and\\nxr with llm-powered conversational agents, arXiv preprint arXiv:2504.05527, 2025. URL https:\\n//arxiv.org/abs/2504.05527v1.\\n[1043] Sabrina Toro, A. V. Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody,\\nA. Diehl, Damion M. Dooley, William Duncan, P. Fey, Pascale Gaudet, Nomi L. Harris, marcin p.\\njoachimiak, Leila Kiani, Tiago Lubiana, M. Munoz-Torres, Shawn T. O’Neil, David Osumi-Sutherland,\\nAleix Puig, Justin Reese, L. Reiser, Sofia M C Robb, Troy Ruemping, James Seager, Eric Sid, Ray\\nStefancsik, Magalie Weber, Valerie Wood, M. Haendel, and Christopher J. Mungall. Dynamic retrieval\\naugmented generation of ontologies using artificial intelligence (dragon-ai). Journal of Biomedical\\nSemantics, 2023.\\n137'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 137}, page_content='[1044] Fernanda M De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores\\nFernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language\\nmodels. International Conference on Human Factors in Computing Systems, 2023.\\n[1045] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken a step forward?\\nIEEE Access, 2025.\\n[1046] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models\\nversus fine-tuned vision transformers in image-based security applications. ACM Transactions on\\nIntelligent Systems and Technology, 2024.\\n[1047] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D.\\nNguyen. Multi-agent collaboration mechanisms: A survey of llms, arXiv preprint arXiv:2501.06322,\\n2025. URL https://arxiv.org/abs/2501.06322v1.\\n[1048] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious\\ncode, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1.\\n[1049] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta,\\nAshish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people\\nfor benchmarking interactive coding agents. Annual Meeting of the Association for Computational\\nLinguistics, 2024.\\n[1050] Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, and Shou-De Lin. Text-centric alignment for\\nmulti-modality learning, arXiv preprint arXiv:2402.08086v2, 2024. URL https://arxiv.org/\\nabs/2402.08086v2.\\n[1051] Tao Tu, M. Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang,\\nBrenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomašev, Shekoofeh Azizi, Karan\\nSinghal, Le Hou, Albert Webson, Kavita Kulkarni, S. Mahdavi, Christopher Semturs, Juraj Gottweis,\\nJoelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, A. Karthikesalingam, and Vivek\\nNatarajan. Towards conversational diagnostic artificial intelligence. Nature, 2025.\\n[1052] Eduard Tulchinskii, Laida Kushnareva, Kristian Kuznetsov, Anastasia Voznyuk, Andrei Andriiainen,\\nIrina Piontkovskaya, Evgeny Burnaev, and Serguei Barannikov. Listening to the wise few: Select-\\nand-copy attention heads for multiple-choice qa, arXiv preprint arXiv:2410.02343, 2024. URL\\nhttps://arxiv.org/abs/2410.02343v1.\\n[1053] Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu,\\nBrendan Dolan-Gavitt, S. K. Shukla, P. Krishnamurthy, F. Khorrami, Ramesh Karri, and Muham-\\nmad Shafique. D-cipher: Dynamic collaborative intelligent multi-agent system with planner and\\nheterogeneous executors for offensive security. arXiv preprint, 2025.\\n[1054] M. Ursino, Nicole Cesaretti, and G. Pirazzini. A model of working memory for encoding multiple\\nitems and ordered sequences exploiting the theta-gamma code. Cognitive Neurodynamics, 2022.\\n[1055] D. H. V. Uytsel, Filip Van Aelten, and Dirk Van Compernolle. A structured language model based\\non context-sensitive probabilistic left-corner parsing. North American Chapter of the Association for\\nComputational Linguistics, 2001.\\n138'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 138}, page_content='[1056] Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, C. Senni,\\nand Markus Leippold. Ai for climate finance: Agentic retrieval and multi-step reasoning for early\\nwarning system investments, arXiv preprint arXiv:2504.05104, 2025. URL https://arxiv.org/\\nabs/2504.05104v2.\\n[1057] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation vs. experience: Evaluating\\nthe usability of code generation tools powered by large language models. CHI Extended Abstracts,\\n2022.\\n[1058] Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, and Huy-Phan Thanh. Rx strategist: Prescription\\nverification using llm agents system, arXiv preprint arXiv:2409.03440, 2024. URL https://arxiv.\\norg/abs/2409.03440v1.\\n[1059] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\\nLukasz Kaiser, and I. Polosukhin. Attention is all you need. Neural Information Processing Systems,\\n2017.\\n[1060] J. D. Velásquez-Henao, Carlos Jaime Franco-Cardona, and Lorena Cadavid-Higuita. Prompt engineer-\\ning: a methodology for optimizing interactions with ai-language models in the field of engineering.\\nDYNA, 2023.\\n[1061] Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, T. Balch, and Manuela Veloso. Adap-\\ntagent: Adapting multimodal web agents with few-shot learning from human demonstrations, arXiv\\npreprint arXiv:2411.13451, 2024. URL https://arxiv.org/abs/2411.13451v1.\\n[1062] Aliaksei Vertsel and Mikhail Rumiantsau. Hybrid llm/rule-based approaches to business insights\\ngeneration from structured data, arXiv preprint arXiv:2404.15604, 2024. URL https://arxiv.\\norg/abs/2404.15604v1.\\n[1063] Aishwarya Vijayan. A prompt engineering approach for structured data extraction from unstructured\\ntext using conversational llms. International Conference on Advances in Computing and Artificial\\nIntelligence, 2023.\\n[1064] Juraj Vladika, Alexander Fichtl, and Florian Matthes. Diversifying knowledge enhancement of\\nbiomedical language models using adapter modules and knowledge graphs. International Conference\\non Agents and Artificial Intelligence, 2023.\\n[1065] James Vo. Sparseaccelerate: Efficient long-context inference for mid-range gpus, arXiv preprint\\narXiv:2412.06198, 2024. URL https://arxiv.org/abs/2412.06198v1.\\n[1066] Blavz vSkrlj, Boshko Koloski, S. Pollak, and Nada Lavravc. From symbolic to neural and back:\\nExploring knowledge graph-large language model synergies. arXiv preprint, 2025.\\n[1067] Tom Völker, Jan Pfister, Tobias Koopmann, and Andreas Hotho. From chat to publication management:\\nOrganizing your related work using bibsonomy & llms. Conference on Human Information Interaction\\nand Retrieval, 2024.\\n[1068] D. Walton. Using argumentation schemes to find motives and intentions of a rational agent. Argument\\nComput., 2020.\\n[1069] Hanlong Wan, Jian Zhang, Yan Chen, Weili Xu, and Fan Feng. Generative ai application for building\\nindustry. Building Simulation, 2024.\\n139'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 139}, page_content='[1070] Jun Wan and Lingrui Mei. Large language models as computable approximations to solomonoff in-\\nduction, arXiv preprint arXiv:2505.15784, 2025. URL https://arxiv.org/abs/2505.15784.\\n[1071] Luanbo Wan and Weizhi Ma. Storybench: A dynamic benchmark for evaluating long-term memory\\nwith multi turns, arXiv preprint arXiv:2506.13356, 2025. URL https://arxiv.org/abs/2506.\\n13356v1.\\n[1072] Bernie Wang, Si ting Xu, K. Keutzer, Yang Gao, and Bichen Wu.\\nImproving context-based\\nmeta-reinforcement learning with self-supervised trajectory contrastive learning, arXiv preprint\\narXiv:2103.06386, 2021. URL https://arxiv.org/abs/2103.06386v1.\\n[1073] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and\\nZhoujun Li. Scm: Enhancing large language model with self-controlled memory framework, arXiv\\npreprint arXiv:2304.13343, 2025. URL https://arxiv.org/abs/2304.13343.\\n[1074] Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, and Chengqian Fu.\\nAdapting llms for efficient context processing through soft prompt compression. Proceedings of the\\nInternational Conference on Modeling, Natural Language Processing and Machine Learning, 2024.\\n[1075] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R. Lyu.\\nPrompt tuning in code intelligence: An experimental evaluation. IEEE Transactions on Software\\nEngineering, 2023.\\n[1076] Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena Shah. Docgraphlm:\\nDocumental graph language model for information extraction. Annual International ACM SIGIR\\nConference on Research and Development in Information Retrieval, 2023.\\n[1077] Fan Wang, Chuan Lin, Yang Cao, and Yu Kang. Benchmarking general purpose in-context learning,\\narXiv preprint arXiv:2405.17234, 2024. URL https://arxiv.org/abs/2405.17234v6.\\n[1078] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\\nAnima Anandkumar. Voyager: An open-ended embodied agent with large language models, arXiv\\npreprint arXiv:2305.16291, 2023. URL https://arxiv.org/abs/2305.16291.\\n[1079] Guoqing Wang, Zeyu Sun, Zhihao Gong, Sixiang Ye, Yizhou Chen, Yifan Zhao, Qing-Lin Liang, and\\nDan Hao. Do advanced language models eliminate the need for prompt engineering in software\\nengineering?, arXiv preprint arXiv:2411.02093, 2024. URL https://arxiv.org/abs/2411.\\n02093v1.\\n[1080] Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, and Limin Wang. Contextual ad narration\\nwith interleaved multimodal sequence. Computer Vision and Pattern Recognition, 2024.\\n[1081] Hanrui Wang, Zhekai Zhang, and Song Han.\\nSpatten: Efficient sparse attention architecture\\nwith cascade token and head pruning. International Symposium on High-Performance Computer\\nArchitecture, 2020.\\n[1082] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng,\\nSule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning:\\nEvaluation and methodology. arXiv preprint arXiv:2507.07999, 2025.\\n140'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 140}, page_content='[1083] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:\\nTuning llama model with chinese medical knowledge, arXiv preprint arXiv:2304.06975, 2023. URL\\nhttps://arxiv.org/abs/2304.06975.\\n[1084] Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, and Yunhe Wang.\\nUnshackling context length: An efficient selective attention approach through query-key compression,\\narXiv preprint arXiv:2502.14477, 2025. URL https://arxiv.org/abs/2502.14477v1.\\n[1085] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can\\nlanguage models solve graph problems in natural language? Neural Information Processing Systems,\\n2023.\\n[1086] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, A. Nambi,\\nT. Ganu, and Hao Wang. Multimodal needle in a haystack: Benchmarking long-context capability of\\nmultimodal large language models. North American Chapter of the Association for Computational\\nLinguistics, 2024.\\n[1087] Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, and Kam-Fai\\nWong. Toward a theory of agents as tool-use decision-makers, arXiv preprint arXiv:2506.00886,\\n2025. URL https://arxiv.org/abs/2506.00886v1.\\n[1088] Jingjin Wang. Proprag: Guiding retrieval with beam search over proposition paths, arXiv preprint\\narXiv:2504.18070, 2025. URL https://arxiv.org/abs/2504.18070v1.\\n[1089] Jingyu Wang, Lu Zhang, Xueqing Li, Huazhong Yang, and Yongpan Liu. Ulseq-ta: Ultra-long sequence\\nattention fusion transformer accelerator supporting grouped sparse softmax and dual-path sparse\\nlayernorm. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024.\\n[1090] Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: A\\nbenchmark for general tool agents. Neural Information Processing Systems, 2024.\\n[1091] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang\\nChen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. A survey\\non large language model based autonomous agents. Frontiers Comput. Sci., 2023.\\n[1092] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-\\nand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.\\nAnnual Meeting of the Association for Computational Linguistics, 2023.\\n[1093] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin,\\nRuihua Song, Wayne Xin Zhao, et al. When large language model based agent meets user behavior\\nanalysis: A novel user simulation paradigm. 2023.\\n[1094] Libo Wang. Towards humanoid robot autonomy: A dynamic architecture integrating continuous\\nthought machines (ctm) and model context protocol (mcp), arXiv preprint arXiv:2505.19339, 2025.\\nURL https://arxiv.org/abs/2505.19339v1.\\n[1095] Liya Wang, Jason Chou, Xin Zhou, A. Tien, and Diane M. Baumgartner. Aviationgpt: A large\\nlanguage model for the aviation domain, arXiv preprint arXiv:2311.17686, 2023. URL https:\\n//arxiv.org/abs/2311.17686v1.\\n141'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 141}, page_content='[1096] Liyuan Wang, Bo Lei, Qian Li, Hang Su, Jun Zhu, and Yi Zhong. Triple-memory networks: A\\nbrain-inspired method for continual learning. IEEE Transactions on Neural Networks and Learning\\nSystems, 2020.\\n[1097] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray\\nHuang, Si Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan,\\nDongmei Zhang, and Qi Zhang. Large action models: From inception to implementation, arXiv\\npreprint arXiv:2412.10047, 2025. URL https://arxiv.org/abs/2412.10047.\\n[1098] Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating\\nmultimodal math reasoning in multi-visual contexts. 2025.\\n[1099] Qineng Wang, Zihao Wang, Ying Su, and Yangqiu Song. On the discussion of large language models:\\nSymmetry of agents and interplay with prompts, arXiv preprint arXiv:2311.07076, 2023. URL\\nhttps://arxiv.org/abs/2311.07076v1.\\n[1100] Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, and Ke Qin. Graphtool-instruction:\\nRevolutionizing graph reasoning in llms through decomposed subtask instruction. Knowledge Discovery\\nand Data Mining, 2024.\\n[1101] Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang,\\nRenhai Chen, Hua Xu, and Hongwei Sun. Xl3m: A training-free framework for llm length extension\\nbased on segment-wise inference, arXiv preprint arXiv:2405.17755, 2024. URL https://arxiv.\\norg/abs/2405.17755v1.\\n[1102] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge\\nediting for large language models: A survey. ACM Computing Surveys, 2023.\\n[1103] Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, and Yada Zhu. Reasoning of large\\nlanguage models over knowledge graphs with super-relations. International Conference on Learning\\nRepresentations, 2025.\\n[1104] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao,\\nChunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang,\\nXinle Deng, Teng Yu, Gangan Ma, Han Xiao, Z. Chen, Danjun Xiang, Yunxia Wang, Yuanyuan\\nZhu, Yichen Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamujiang\\nTayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yu-Jie Ye, Yihan Li, Lei Wan, Xinyue Jiang,\\nYujie Wang, Siyuan Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen,\\nY. Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, arXiv preprint\\narXiv:2401.17268, 2024. URL https://arxiv.org/abs/2401.17268v1.\\n[1105] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu\\nWei. Visually-augmented language modeling. International Conference on Learning Representations,\\n2022.\\n[1106] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat\\nseng Chua. Learning intents behind interactions with knowledge graph for recommendation. The\\nWeb Conference, 2021.\\n[1107] Xiao Wang, Isaac Lyngaas, A. Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo,\\nHong-Jun Yoon, M. Wahib, and J. Gounley. Ultra-long sequence distributed transformer, arXiv\\npreprint arXiv:2311.02382, 2023. URL https://arxiv.org/abs/2311.02382v2.\\n142'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 142}, page_content='[1108] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and S. Yeung-Levy. Videoagent: Long-form video under-\\nstanding with large language model as agent. European Conference on Computer Vision, 2024.\\n[1109] Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang,\\nYawen Wang, Kaiyu Huang, Yile Wang, Peng Li, and Yang Liu.\\nMucar: Benchmarking multi-\\nlingual cross-modal ambiguity resolution for multimodal large language models, arXiv preprint\\narXiv:2506.17046v1, 2025. URL https://arxiv.org/abs/2506.17046v1.\\n[1110] Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. R3mem: Bridging memory retention and\\nretrieval via reversible compression. arXiv preprint, 2025.\\n[1111] Xiaoyang Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, I. Abdelaziz, Maria\\nChang, Achille Fokoue, B. Makni, Nicholas Mattei, and M. Witbrock. Improving natural language\\ninference using external knowledge in the science questions domain. AAAI Conference on Artificial\\nIntelligence, 2018.\\n[1112] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and A. Eshaghi.\\nBeyond the limits: A survey of techniques to extend the context length in large language models.\\nInternational Joint Conference on Artificial Intelligence, 2024.\\n[1113] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable\\ncode actions elicit better llm agents. International Conference on Machine Learning, 2024.\\n[1114] Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. Self-consistency\\nimproves chain of thought reasoning in language models. International Conference on Learning\\nRepresentations, 2022.\\n[1115] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang\\nHuang, Yanbin Lu, and Yingzhen Yang. Recmind: Large language model powered agent for rec-\\nommendation, arXiv preprint arXiv:2308.14296, 2024. URL https://arxiv.org/abs/2308.\\n14296.\\n[1116] Yani Wang. Application of large language models based on knowledge graphs in question-answering\\nsystems: A review. Applied and Computational Engineering, 2024.\\n[1117] Yanlin Wang, Wanjun Zhong, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma,\\nQianxiang Wang, and Zibin Zheng. Agents in software engineering: Survey, landscape, and vision,\\narXiv preprint arXiv:2409.09030, 2024. URL https://arxiv.org/abs/2409.09030v2.\\n[1118] Yaqi Wang and Haipei Xu. Srsa: A cost-efficient strategy-router search agent for real-world human-\\nmachine interactions. 2024 IEEE International Conference on Data Mining Workshops (ICDMW),\\n2024.\\n[1119] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyun Zeng, Chenting Wang, Changlian\\nMa, Haian Huang, Jianfei Gao, Min Dou, Kaiming Chen, Wenhai Wang, Yu Qiao, Yali Wang, and\\nLimin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv\\npreprint, 2025.\\n[1120] Yiming Wang, Zhuosheng Zhang, and Rui Wang. Element-aware summarization with large language\\nmodels: Expert-aligned evaluation and chain-of-thought method. Annual Meeting of the Association\\nfor Computational Linguistics, 2023.\\n143'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 143}, page_content='[1121] Yingming Wang and Pepa Atanasova. Self-critique and refinement for faithful natural language\\nexplanations, arXiv preprint arXiv:2505.22823, 2025. URL https://arxiv.org/abs/2505.\\n22823v1.\\n[1122] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph\\nclassification. In Proceedings of the Web Conference 2021, pages 3663–3674, 2021.\\n[1123] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li,\\nXian Li, Bing Yin, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large\\nlanguage models, arXiv preprint arXiv:2402.04624, 2024. URL https://arxiv.org/abs/2402.\\n04624.\\n[1124] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gut-\\nfreund, Rogério Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory.\\narXiv preprint, 2025.\\n[1125] Yubin Wang, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, and Cairong Zhao. Hpt++:\\nHierarchically prompting vision-language models with multi-granularity knowledge generation and\\nimproved structure modeling. arXiv preprint, 2024.\\n[1126] Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li,\\nFaming Wu, and Bin Cui. Flexsp: Accelerating large language model training via flexible sequence\\nparallelism. International Conference on Architectural Support for Programming Languages and\\nOperating Systems, 2024.\\n[1127] Yuntao Wang, Yanghe Pan, Zhou Su, Yi Deng, Quan Zhao, L. Du, Tom H. Luan, Jiawen Kang, and\\nD. Niyato. Large model based agents: State-of-the-art, cooperation paradigms, security and privacy,\\nand future trends. IEEE Communications Surveys & Tutorials, 2024.\\n[1128] Yuntao Wang, Shaolong Guo, Yanghe Pan, Zhou Su, Fahao Chen, Tom H. Luan, Peng Li, Jiawen Kang,\\nand Dusit Niyato. Internet of agents: Fundamentals, applications, and challenges, arXiv preprint\\narXiv:2505.07176, 2025. URL https://arxiv.org/abs/2505.07176v1.\\n[1129] Yuxiang Wang, Xinnan Dai, Wenqi Fan, and Yao Ma. Exploring graph tasks with pure llms: A\\ncomprehensive benchmark and investigation, arXiv preprint arXiv:2502.18771v1, 2025.\\nURL\\nhttps://arxiv.org/abs/2502.18771v1.\\n[1130] Z. Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning\\nShi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, and\\nWenhao Huang. Mio: A foundation model on multimodal tokens, arXiv preprint arXiv:2409.17692v3,\\n2024. URL https://arxiv.org/abs/2409.17692v3.\\n[1131] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. M-rag: Reinforcing large\\nlanguage model performance through retrieval-augmented generation with multiple partitions.\\nAnnual Meeting of the Association for Computational Linguistics, 2024.\\n[1132] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway?\\na survey from the language model perspective, arXiv preprint arXiv:2403.15452, 2024.\\nURL\\nhttps://arxiv.org/abs/2403.15452v1.\\n144'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 144}, page_content='[1133] Ziyang Wang, Jianzhou You, Haining Wang, Tianwei Yuan, Shichao Lv, Yang Wang, and Limin Sun.\\nHoneygpt: Breaking the trilemma in terminal honeypots with large language model, arXiv preprint\\narXiv:2406.01882, 2024. URL https://arxiv.org/abs/2406.01882v2.\\n[1134] Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun,\\nand Yang Liu. Browse and concentrate: Comprehending multimodal content via prior-llm context\\nfusion. Annual Meeting of the Association for Computational Linguistics, 2024.\\n[1135] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, arXiv\\npreprint arXiv:2409.07429, 2024. URL https://arxiv.org/abs/2409.07429.\\n[1136] Irene Weber. Large language models are pattern matchers: Editing semi-structured and structured\\ndocuments with chatgpt. AKWI Jahrestagung, 2024.\\n[1137] Hui Wei, Chenyue Feng, and Jianning Zhang. Modeling of memory mechanisms in cerebral cortex\\nand simulation of storage performance, arXiv preprint arXiv:2401.00381, 2023. URL https:\\n//arxiv.org/abs/2401.00381v2.\\n[1138] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny\\nZhou. Chain of thought prompting elicits reasoning in large language models. Neural Information\\nProcessing Systems, 2022.\\n[1139] Jerry W. Wei, Le Hou, Andrew Kyle Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen,\\nYifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. Symbol tuning improves in-context learning in\\nlanguage models. Conference on Empirical Methods in Natural Language Processing, 2023.\\n[1140] Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, and Gang Kou. Graph\\nlearning and its advancements on large language models: A holistic survey, arXiv preprint\\narXiv:2212.08966, 2022. URL https://arxiv.org/abs/2212.08966v5.\\n[1141] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao\\nZhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end\\nmulti-turn reinforcement learning. arXiv preprint, 2025.\\n[1142] Zhiyuan Wei, Jing Sun, Zijian Zhang, and Xianhao Zhang. Llm-smartaudit: Advanced smart contract\\nvulnerability detection. arXiv preprint, 2024.\\n[1143] Rebecca Westhäußer, Frederik Berenz, Wolfgang Minker, and Sebastian Zepf. Caim: Development\\nand evaluation of a cognitive ai memory framework for long-term interaction with intelligent agents.\\narXiv preprint, 2025.\\n[1144] Danny Weyns and F. Oquendo. An architectural style for self-adaptive multi-agent systems, arXiv\\npreprint arXiv:1909.03475, 2019. URL https://arxiv.org/abs/1909.03475v1.\\n[1145] Erik Wijmans, Brody Huval, Alexander Hertzberg, V. Koltun, and Philipp Krähenbühl. Cut your losses\\nin large-vocabulary language models. International Conference on Learning Representations, 2024.\\n[1146] Wikipedia contributors. Agent communications language — Wikipedia, the free encyclopedia,\\n2025. URL https://en.wikipedia.org/wiki/Agent_Communications_Language. [On-\\nline; accessed 17-July-2025].\\n145'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 145}, page_content='[1147] Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng,\\nMayank Kulkarni, and Daniel Preotiuc-Pietro. Overcoming catastrophic forgetting in massively\\nmultilingual continual learning, arXiv preprint arXiv:2305.16252, 2023. URL https://arxiv.\\norg/abs/2305.16252.\\n[1148] Beong woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil\\nYang, and Jinyoung Yeo. Toolhaystack: Stress-testing tool-augmented language models in realistic\\nlong-term interactions, arXiv preprint arXiv:2505.23662, 2025. URL https://arxiv.org/abs/\\n2505.23662v1.\\n[1149] Biao Wu, Yanda Li, Meng Fang, Zirui Song, Zhiwei Zhang, Yunchao Wei, and Ling Chen. Foundations\\nand recent trends in multimodal mobile agents: A survey, arXiv preprint arXiv:2411.02006, 2024.\\nURL https://arxiv.org/abs/2411.02006v2.\\n[1150] Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, and Hung yi Lee. Streambench:\\nTowards benchmarking continuous improvement of language agents. Neural Information Processing\\nSystems, 2024.\\n[1151] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang,\\nZekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous\\ninformation seeking agency, arXiv preprint arXiv:2505.22648, 2025. URL https://arxiv.org/\\nabs/2505.22648v2.\\n[1152] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou,\\nPengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, arXiv preprint\\narXiv:2501.07572, 2025. URL https://arxiv.org/abs/2501.07572v2.\\n[1153] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the\\ndeep research, arXiv preprint arXiv:2502.04644, 2025. URL https://arxiv.org/abs/2502.\\n04644v1.\\n[1154] Likang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen\\nZhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. A survey on large language models for\\nrecommendation. World wide web (Bussum), 2023.\\n[1155] M Wu, J Yang, J Jiang, M Li, K Yan, and H Yu.... Vtool-r1: Vlms learn to think with images via\\nreinforcement learning on multimodal tool use. 2025. URL https://arxiv.org/abs/2505.\\n19255.\\n[1156] Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. Seal-tools:\\nSelf-instruct tool learning dataset for agent tuning and detailed benchmark. Natural Language\\nProcessing and Chinese Computing, 2024.\\n[1157] Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, and Fangxin Wang. Deep-\\nform: Reasoning large language model for communication system formulation, arXiv preprint\\narXiv:2506.08551, 2025. URL https://arxiv.org/abs/2506.08551v2.\\n[1158] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\\nShaokun Zhang, Jiale Liu, A. Awadallah, Ryen W. White, Doug Burger, and Chi Wang. Autogen:\\nEnabling next-gen llm applications via multi-agent conversation, arXiv preprint arXiv:2308.08155,\\n2023. URL https://arxiv.org/abs/2308.08155v2.\\n146'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 146}, page_content='[1159] Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung won Hwang, Zhewei Yao, Yuxiong He,\\nand Feng Yan. Composerag: A modular and composable rag for corpus-grounded multi-hop ques-\\ntion answering, arXiv preprint arXiv:2506.00232, 2025. URL https://arxiv.org/abs/2506.\\n00232v1.\\n[1160] Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu,\\nTei-Wei Kuo, Nan Guan, and C. Xue. Retrieval-augmented generation for natural language process-\\ning: A survey, arXiv preprint arXiv:2407.13193, 2024. URL https://arxiv.org/abs/2407.\\n13193v3.\\n[1161] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, V. Ioannidis, Karthik\\nSubbian, J. Leskovec, and James Zou. Avatar: Optimizing llm agents for tool usage via contrastive\\nreasoning. Neural Information Processing Systems, 2024.\\n[1162] Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: A new benchmark\\nfor evaluating knowledge editing of llms, arXiv preprint arXiv:2308.09954, 2023. URL https:\\n//arxiv.org/abs/2308.09954.\\n[1163] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason E Weston,\\nand Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with\\nllm-as-a-meta-judge. arXiv preprint, 2024.\\n[1164] Tong Wu, Chong Xiang, Jiachen T. Wang, and Prateek Mittal. Effectively controlling reasoning\\nmodels through thinking intervention, arXiv preprint arXiv:2503.24370, 2025. URL https://\\narxiv.org/abs/2503.24370v3.\\n[1165] Xinbo Wu and L. Varshney. A meta-learning perspective on transformers for causal language modeling.\\nAnnual Meeting of the Association for Computational Linguistics, 2023.\\n[1166] Xue Wu and Kostas Tsioutsiouliklis. Thinking with knowledge graphs: Enhancing llm reasoning\\nthrough structured data, arXiv preprint arXiv:2412.10654, 2024. URL https://arxiv.org/\\nabs/2412.10654v1.\\n[1167] Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang,\\nand Yong Liu. From human memory to ai memory: A survey on memory mechanisms in the era of\\nllms, arXiv preprint arXiv:2504.15965, 2025. URL https://arxiv.org/abs/2504.15965v2.\\n[1168] Zengqing Wu and Takayuki Ito. The hidden strength of disagreement: Unraveling the consensus-\\ndiversity tradeoff in adaptive multi-agent systems, arXiv preprint arXiv:2502.16565, 2025. URL\\nhttps://arxiv.org/abs/2502.16565v2.\\n[1169] Zihao Wu, Lu Zhang, Chao-Yang Cao, Xiao-Xing Yu, Haixing Dai, Chong-Yi Ma, Zheng Liu, Lin Zhao,\\nGang Li, Wei Liu, Quanzheng Li, Dinggang Shen, Xiang Li, Dajiang Zhu, and Tianming Liu. Exploring\\nthe trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology\\nnli task. IEEE Transactions on Big Data, 2023.\\n[1170] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe\\nWang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao\\nZhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou,\\nRongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing\\nHuan, and Tao Gui. The rise and potential of large language model based agents: A survey, arXiv\\npreprint arXiv:2309.07864, 2023. URL https://arxiv.org/abs/2309.07864v3.\\n147'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 147}, page_content='[1171] Menglin Xia, Victor Ruehle, Saravan Rajmohan, and Reza Shokri. Minerva: A programmable\\nmemory test benchmark for language models, arXiv preprint arXiv:2502.03358, 2025. URL https:\\n//arxiv.org/abs/2502.03358v2.\\n[1172] Yuchen Xia, Manthan Shenoy, N. Jazdi, and M. Weyrich. Towards autonomous system: flexible mod-\\nular production system enhanced with large language model agents. IEEE International Conference\\non Emerging Technologies and Factory Automation, 2023.\\n[1173] Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger\\nZimmermann, and Jinhua Zhao. Reimagining urban science: Scaling causal inference with large\\nlanguage models, arXiv preprint arXiv:2504.12345v3, 2025. URL https://arxiv.org/abs/\\n2504.12345v3.\\n[1174] Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and\\nJinsong Su. When to use graphs in rag: A comprehensive analysis for graph retrieval-augmented gener-\\nation, arXiv preprint arXiv:2506.05690, 2025. URL https://arxiv.org/abs/2506.05690v1.\\n[1175] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu,\\nSong Han, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an\\nefficient context memory. Neural Information Processing Systems, 2024.\\n[1176] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming\\nlanguage models with attention sinks. International Conference on Learning Representations, 2023.\\n[1177] MiniCPM Team Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen,\\nXin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan,\\nJunshao Guo, Yu-Xuan Han, Bingxiang He, Yuxian Huang, Cunliang Kong, Qiu-Tong Li, Siyuan Li,\\nWenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu,\\nYa-Ting Lu, Pei Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayu Su,\\nZhou Su, Ao Sun, Xiang ping Sun, Peijun Tang, Fang-Ming Wang, Feng Wang, Shuo Wang, Yudong\\nWang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zi-Kang Xie, Yukun Yan, Jia-Li Yuan, Kai Zhang, Lei Zhang,\\nLinyu Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao,\\nZhijun Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zi-An Zhou, Zhiyuan Liu, Guoyang Zeng,\\nChaochao Jia, Dahai Li, and Maosong Sun. Minicpm4: Ultra-efficient llms on end devices, arXiv\\npreprint arXiv:2506.07900, 2025. URL https://arxiv.org/abs/2506.07900v1.\\n[1178] Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, and Pengfei Liu. Limopro:\\nReasoning refinement for efficient and effective test-time scaling, arXiv preprint arXiv:2505.19187,\\n2025. URL https://arxiv.org/abs/2505.19187v1.\\n[1179] Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, and Xiao Huang. Reliable reason-\\ning path: Distilling effective guidance for llm reasoning with knowledge graphs, arXiv preprint\\narXiv:2506.10508, 2025. URL https://arxiv.org/abs/2506.10508v1.\\n[1180] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and\\nYu Su. Travelplanner: A benchmark for real-world planning with language agents. International\\nConference on Machine Learning, 2024.\\n[1181] Yuxi Xie, Anirudh Goyal, Xiaobao Wu, Xunjian Yin, Xiao Xu, Min-Yen Kan, Liangming Pan, and\\nWilliam Yang Wang. Coral: Order-agnostic language modeling for efficient iterative refinement,\\narXiv preprint arXiv:2410.09675, 2024. URL https://arxiv.org/abs/2410.09675v1.\\n148'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 148}, page_content='[1182] Yue Xing, Tao Yang, Yijiashun Qi, Minggu Wei, Yu Cheng, and Honghui Xin. Structured memory mech-\\nanisms for stable context representation in large language models, arXiv preprint arXiv:2505.22921,\\n2025. URL https://arxiv.org/abs/2505.22921v1.\\n[1183] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song,\\nDengyu Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Rag-gym: Systematic optimization of\\nlanguage agents for retrieval-augmented generation. arXiv preprint, 2025.\\n[1184] Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, and Laura E.\\nBarnes. Converging paradigms: The synergy of symbolic and connectionist ai in llm-empowered\\nautonomous agents, arXiv preprint arXiv:2407.08516, 2024. URL https://arxiv.org/abs/\\n2407.08516v5.\\n[1185] Junjie Xiong, Changjia Zhu, Shuhang Lin, Chong Zhang, Yongfeng Zhang, Yao Liu, and Lingyao Li. In-\\nvisible prompts, visible threats: Malicious font injection in external resources for large language mod-\\nels, arXiv preprint arXiv:2505.16957, 2025. URL https://arxiv.org/abs/2505.16957v1.\\n[1186] Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, and Yiwei Wang. Enhancing llm\\ncharacter-level manipulation via divide and conquer. 2025.\\n[1187] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Mapping the minds of llms: A graph-based\\nanalysis of reasoning llm. 2025.\\n[1188] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Unveiling the potential of diffusion large\\nlanguage model in controllable generation. 2025.\\n[1189] Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Jiliang Tang, Himabindu Lakkaraju, and Zhen Xiang.\\nHow memory management impacts llm agents: An empirical study of experience-following behavior,\\narXiv preprint arXiv:2505.16067, 2025. URL https://arxiv.org/abs/2505.16067v1.\\n[1190] Chunmei Xu, Shengheng Liu, Cheng Zhang, Yongming Huang, Zhaohua Lu, and Luxi Yang. Multi-\\nagent reinforcement learning based distributed transmission in collaborative cloud-edge systems.\\nIEEE Transactions on Vehicular Technology, 2021.\\n[1191] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang,\\nJiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning\\nsystems? 2025.\\n[1192] Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng Cao, Shuai Fan, Lu Chen, and\\nKai Yu. Reducing tool hallucination via reliability alignment, arXiv preprint arXiv:2412.04141, 2024.\\nURL https://arxiv.org/abs/2412.04141v3.\\n[1193] Hu Xu, Gargi Ghosh, Po-Yao (Bernie) Huang, Dmytro Okhonko, Armen Aghajanyan, and Florian\\nMetze Luke Zettlemoyer Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot\\nvideo-text understanding. Conference on Empirical Methods in Natural Language Processing, 2021.\\n[1194] Mengjia Xu. Understanding graph embedding methods and their applications. SIAM Review, 2020.\\n[1195] Minrui Xu, Hongyang Du, Dusist Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, A. Ja-\\nmalipour, Dong In Kim, X. Shen, Victor C. M. Leung, and H. Poor. Unleashing the power of edge-cloud\\ngenerative ai in mobile networks: A survey of aigc services. IEEE Communications Surveys and Tuto-\\nrials, 2023.\\n149'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 149}, page_content='[1196] Minrui Xu, D. Niyato, and Christopher G. Brinton. Serving long-context llms at the mobile edge:\\nTest-time reinforcement learning-based model caching and inference offloading, arXiv preprint\\narXiv:2501.14205, 2025. URL https://arxiv.org/abs/2501.14205v1.\\n[1197] Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, and Muhao Chen. From introspection to best\\npractices: Principled analysis of demonstrations in multimodal in-context learning. North American\\nChapter of the Association for Computational Linguistics, 2024.\\n[1198] Shuhang Xu and Fangwei Zhong. Comet: Metaphor-driven covert communication for multi-agent\\nlanguage games, arXiv preprint arXiv:2505.18218, 2025. URL https://arxiv.org/abs/2505.\\n18218v1.\\n[1199] Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun.\\nNoderag: Structuring graph-based rag with heterogeneous nodes, arXiv preprint arXiv:2504.11544,\\n2025. URL https://arxiv.org/abs/2504.11544v1.\\n[1200] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and W. Wang. Pride and prejudice:\\nLlm amplifies self-bias in self-refinement. Annual Meeting of the Association for Computational\\nLinguistics, 2024.\\n[1201] Wenrui Xu and Keshab K. Parhi. A survey of attacks on large language models, arXiv preprint\\narXiv:2505.12567, 2025. URL https://arxiv.org/abs/2505.12567v1.\\n[1202] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic\\nmemory for llm agents. arXiv preprint, 2025.\\n[1203] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic\\nmemory for llm agents, arXiv preprint arXiv:2502.12110, 2025. URL https://arxiv.org/abs/\\n2502.12110.\\n[1204] Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, and Ying Wu. Sas: Self-\\naugmentation strategy for language model pre-training. AAAI Conference on Artificial Intelligence,\\n2021.\\n[1205] Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, and Ying Shen. Mindgym: What matters in\\nquestion synthesis for thinking-centric fine-tuning?, arXiv preprint arXiv:2503.09499, 2025. URL\\nhttps://arxiv.org/abs/2503.09499v2.\\n[1206] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang,\\nand Zheng Li. Retrieval-augmented generation with knowledge graphs for customer service question\\nanswering. Annual International ACM SIGIR Conference on Research and Development in Information\\nRetrieval, 2024.\\n[1207] Eric Xue, Ke Chen, Zeyi Huang, Yuyang Ji, Yong Jae Lee, and Haohan Wang. Improve: Iterative\\nmodel pipeline refinement and optimization leveraging llm experts, arXiv preprint arXiv:2502.18530,\\n2025. URL https://arxiv.org/abs/2502.18530v2.\\n[1208] Huiyin Xue and Nikolaos Aletras. Pit one against many: Leveraging attention-head embeddings\\nfor parameter-efficient multi-head attention. Conference on Empirical Methods in Natural Language\\nProcessing, 2023.\\n150'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 150}, page_content='[1209] Xiangyuan Xue, Zeyu Lu, Di Huang, Zidong Wang, Wanli Ouyang, and Lei Bai. Comfybench:\\nBenchmarking llm-based agents in comfyui for autonomously designing collaborative ai systems,\\narXiv preprint arXiv:2409.01392, 2024. URL https://arxiv.org/abs/2409.01392v2.\\n[1210] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and Chaozhuo\\nLi. Beyond self-talk: A communication-centric survey of llm-based multi-agent systems. arXiv\\npreprint, 2025.\\n[1211] Tianqiang Yan and Tiansheng Xu. Refining the responses of llms by themselves, arXiv preprint\\narXiv:2305.04039, 2023. URL https://arxiv.org/abs/2305.04039v1.\\n[1212] Xu Yan, Junliang Du, Lun Wang, Yingbin Liang, Jiacheng Hu, and Bingxing Wang. The synergistic\\nrole of deep learning and neural architecture search in advancing artificial intelligence. 2024\\nInternational Conference on Electronics and Devices, Computational Science (ICEDCS), 2024.\\n[1213] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann,\\nand Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive\\nlanguage-image pretraining from the web. The Web Conference, 2023.\\n[1214] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang.\\nInftythink: Breaking the length limits of long-context reasoning in large language models, arXiv\\npreprint arXiv:2503.06692, 2025. URL https://arxiv.org/abs/2503.06692v3.\\n[1215] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\\nLarge language models as optimizers. International Conference on Learning Representations, 2023.\\n[1216] Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang,\\nZeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and E. Weinan.\\nMemory3: Language modeling with explicit memory. Journal of Machine Learning, 2024.\\n[1217] Jianxin Yang. Longqlora: Efficient and effective method to extend context length of large lan-\\nguage models, arXiv preprint arXiv:2311.04879, 2023. URL https://arxiv.org/abs/2311.\\n04879v2.\\n[1218] Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision.\\narXiv preprint, 2023.\\n[1219] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Ji-\\namu Kang, Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Octopus: Embodied vision-language\\nprogrammer from environmental feedback. European Conference on Computer Vision, 2023.\\n[1220] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Adriano Lieret, Shunyu Yao, Karthik\\nNarasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software\\nengineering. Neural Information Processing Systems, 2024.\\n[1221] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh,\\nGuangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning\\non textual graph. Neural Information Processing Systems, 2021.\\n[1222] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa\\nRangwala. Agentoccam: A simple yet strong baseline for llm-based web agents. 2024.\\n151'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 151}, page_content='[1223] Lin F. Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Give us the facts: Enhancing\\nlarge language models with knowledge graphs for fact-aware language modeling. IEEE Transactions\\non Knowledge and Data Engineering, 2023.\\n[1224] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada:\\nMultimodal large diffusion language models, arXiv preprint arXiv:2505.15809v1, 2025.\\nURL\\nhttps://arxiv.org/abs/2505.15809v1.\\n[1225] R Yang, L Song, Y Li, S Zhao, and Y Ge.... Gpt4tools: Teaching large language model to use\\ntools via self-instruction. 2023. URL https://proceedings.neurips.cc/paper_files/\\npaper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.\\nhtml?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email&\\nutm_source=Artificial_Intelligence_Weekly_411.\\n[1226] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching\\nlarge language model to use tools via self-instruction. Neural Information Processing Systems, 2023.\\n[1227] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun\\nLin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified\\nsparse attention, arXiv preprint arXiv:2502.14866, 2025. URL https://arxiv.org/abs/2502.\\n14866v2.\\n[1228] Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, and Po Yang. Gpt-4 as evaluator:\\nEvaluating large language models on pest management in agriculture. arXiv preprint, 2024.\\n[1229] Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, and Xiaotian Han. Longer con-\\ntext, deeper thinking: Uncovering the role of long-context ability in reasoning, arXiv preprint\\narXiv:2505.17315, 2025. URL https://arxiv.org/abs/2505.17315v1.\\n[1230] Wen Yang, Kai Fan, and Minpeng Liao. Markov chain of thought for efficient mathematical reasoning.\\nNorth American Chapter of the Association for Computational Linguistics, 2024.\\n[1231] Yaodong Yang, Chengdong Ma, Zihan Ding, S. McAleer, Chi Jin, and Jun Wang. Game-theoretic\\nmultiagent reinforcement learning, arXiv preprint arXiv:2011.00583, 2020. URL https://arxiv.\\norg/abs/2011.00583v4.\\n[1232] Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, and Qi Liu. Unleashing the potential of large\\nlanguage models for predictive tabular tasks in data science, arXiv preprint arXiv:2403.20208, 2024.\\nURL https://arxiv.org/abs/2403.20208v7.\\n[1233] Yi Yang, Yixuan Tang, and Kar Yan Tam. Investlm: A large language model for investment using\\nfinancial domain instruction tuning, arXiv preprint arXiv:2309.13064, 2023. URL https://arxiv.\\norg/abs/2309.13064.\\n[1234] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji ping\\nWang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data augmentation\\nfor commonsense reasoning. Findings, 2020.\\n[1235] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi\\nHu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, and Weinan Zhang. A survey\\nof ai agent protocols, arXiv preprint arXiv:2504.16736, 2025. URL https://arxiv.org/abs/\\n2504.16736v3.\\n152'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 152}, page_content='[1236] Yuan Yang, Siheng Xiong, Ehsan Shareghi, and F. Fekri. The compressor-retriever architecture for\\nlanguage model os, arXiv preprint arXiv:2409.01495, 2024. URL https://arxiv.org/abs/\\n2409.01495v1.\\n[1237] Yuxin Yang, Haoyang Wu, Tao Wang, Jia Yang, Hao Ma, and Guojie Luo. Pseudo-knowledge graph:\\nMeta-path guided retrieval and in-graph text for rag-equipped llm, arXiv preprint arXiv:2503.00309,\\n2025. URL https://arxiv.org/abs/2503.00309v1.\\n[1238] Zhen Yang, Fang Liu, Zhongxing Yu, J. Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and\\nGe Li. Exploring and unleashing the power of large language models in automated code translation.\\nProc. ACM Softw. Eng., 2024.\\n[1239] Chengyuan Yao and Satoshi Fujita. Adaptive control of retrieval-augmented generation for large\\nlanguage models through reflective tags. Electronics, 2024.\\n[1240] Huaiyuan Yao, Longchao Da, Vishnu Nandam, J. Turnau, Zhiwei Liu, Linsey Pang, and Hua Wei.\\nComal: Collaborative multi-agent large language models for mixed-autonomy traffic, arXiv preprint\\narXiv:2410.14368, 2024. URL https://arxiv.org/abs/2410.14368v2.\\n[1241] Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi\\nCheng. Who is in the spotlight: The hidden bias undermining multimodal retrieval-augmented\\ngeneration. 2025.\\n[1242] Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, and\\nDhabaleswar K. Panda. Training ultra long context language model with fully pipelined distributed\\ntransformer, arXiv preprint arXiv:2408.16978, 2024. URL https://arxiv.org/abs/2408.\\n16978v2.\\n[1243] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification.\\nAAAI Conference on Artificial Intelligence, 2018.\\n[1244] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\\nreal-world web interaction with grounded language agents. Neural Information Processing Systems,\\n2022.\\n[1245] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. International Conference on Learning\\nRepresentations, 2022.\\n[1246] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, and Karthik Narasimhan.\\nTree of thoughts: Deliberate problem solving with large language models. Neural Information\\nProcessing Systems, 2023.\\n[1247] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models, arXiv preprint arXiv:2210.03629, 2023.\\nURL https://arxiv.org/abs/2210.03629.\\n[1248] Shunyu Yao, Noah Shinn, P. Razavi, and Karthik Narasimhan. τ-bench: A benchmark for tool-agent-\\nuser interaction in real-world domains. arXiv preprint, 2024.\\n153'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 153}, page_content='[1249] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy,\\nZeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and\\nSilvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization,\\narXiv preprint arXiv:2308.02151, 2024. URL https://arxiv.org/abs/2308.02151.\\n[1250] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and J. Leskovec. Qa-gnn: Reasoning\\nwith language models and knowledge graphs for question answering. North American Chapter of the\\nAssociation for Computational Linguistics, 2021.\\n[1251] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy\\nLiang, and J. Leskovec. Deep bidirectional language-knowledge graph pretraining. Neural Information\\nProcessing Systems, 2022.\\n[1252] Fahd Yazin, Moumita Das, A. Banerjee, and Dipanjan Roy. Contextual prediction errors reorganize\\nnaturalistic episodic memories in time. Scientific Reports, 2021.\\n[1253] J Ye, G Li, S Gao, C Huang, Y Wu, S Li, and X Fan.... Tooleyes: Fine-grained evaluation for\\ntool learning capabilities of large language models in real-world scenarios. 2024. URL https:\\n//arxiv.org/abs/2401.00741.\\n[1254] J Ye, S Li, G Li, C Huang, S Gao, and Y Wu.... Toolsword: Unveiling safety issues of large language\\nmodels in tool learning across three stages. 2024. URL https://arxiv.org/abs/2402.10753.\\n[1255] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining\\nZhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiechao Chen. Toolhop: A\\nquery-driven benchmark for evaluating large language models in multi-hop tool use, arXiv preprint\\narXiv:2501.02506, 2025. URL https://arxiv.org/abs/2501.02506v4.\\n[1256] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering a prompt\\nengineer. Annual Meeting of the Association for Computational Linguistics, 2023.\\n[1257] Sixiang Ye, Zeyu Sun, Guoqing Wang, Liwei Guo, Qing-Lin Liang, Zheng Li, and Yong Liu.\\nPrompt alchemy: Automatic prompt refinement for enhancing code generation, arXiv preprint\\narXiv:2503.11085, 2025. URL https://arxiv.org/abs/2503.11085v1.\\n[1258] Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan\\nKautz, Pavlo Molchanov, and Y. Lin. Longmamba: Enhancing mamba’s long-context capabilities\\nvia training-free receptive field enlargement. International Conference on Learning Representations,\\n2025.\\n[1259] Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal\\nShmueli-Scheuer. Survey on evaluation of llm-based agents, arXiv preprint arXiv:2503.16416, 2025.\\nURL https://arxiv.org/abs/2503.16416v1.\\n[1260] Peiling Yi and Yuhan Xia. Irony detection, reasoning and understanding in zero-shot learning. IEEE\\nTransactions on Artificial Intelligence, 2025.\\n[1261] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi,\\nand Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents.\\nAnnual Meeting of the Association for Computational Linguistics, 2023.\\n154'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 154}, page_content='[1262] Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T. Le,\\nKai-Wei Chang, Chen-Yu Lee, Hamid Palangi, and Tomas Pfister. Magnet: Multi-turn tool-use\\ndata synthesis and distillation via graph translation, arXiv preprint arXiv:2503.07826, 2025. URL\\nhttps://arxiv.org/abs/2503.07826v1.\\n[1263] Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu,\\nXiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe Zhang, K. Ahnert, Vik Kamath, Mathias Berglund,\\nDominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng\\nCao, Ruoming Pang, and Zirui Wang. Mmau: A holistic benchmark of agent capabilities across\\ndiverse domains. North American Chapter of the Association for Computational Linguistics, 2024.\\n[1264] Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint\\nunderstanding of textual and tabular data. Annual Meeting of the Association for Computational\\nLinguistics, 2020.\\n[1265] S Yin, W You, Z Ji, G Zhong, and J Bai. Mumath-code: Combining tool-use large language models\\nwith multi-perspective data augmentation for mathematical reasoning. 2024. URL https://\\narxiv.org/abs/2405.07551.\\n[1266] Gunwoo Yong, Kahyun Jeon, Daeyoung Gil, and Ghang Lee. Prompt engineering for zero-shot and\\nfew-shot defect detection and classification using a visual-language pretrained model. Comput. Aided\\nCiv. Infrastructure Eng., 2022.\\n[1267] Aspen H. Yoo and A. Collins. How working memory and reinforcement learning are intertwined: A\\ncognitive, neural, and computational perspective. Journal of Cognitive Neuroscience, 2021.\\n[1268] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact:\\nCompressing retrieved documents actively for question answering. Conference on Empirical Methods\\nin Natural Language Processing, 2024.\\n[1269] Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, M. Patwary, M. Shoeybi, and Bryan Catanzaro.\\nLlm-evolve: Evaluation for llm’s evolving capability on benchmarks. Conference on Empirical Methods\\nin Natural Language Processing, 2024.\\n[1270] Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, and Wei Ai. Large language models meet\\ngraph neural networks: A perspective of graph mining. Mathematics, 2024.\\n[1271] Dian Yu, Yuheng Zhang, Jiahao Xu, Tian Liang, Linfeng Song, Zhaopeng Tu, Haitao Mi, and\\nDong Yu. Teaching llms to refine with tools, arXiv preprint arXiv:2412.16871, 2024. URL https:\\n//arxiv.org/abs/2412.16871v1.\\n[1272] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen,\\nKun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. A survey on trustworthy\\nllm agents: Threats and countermeasures, arXiv preprint arXiv:2503.09648, 2025. URL https:\\n//arxiv.org/abs/2503.09648v1.\\n[1273] Ye Yu, Yaoning Yu, and Haohan Wang. Premise: Scalable and strategic prompt optimization for\\nefficient mathematical reasoning in large models, arXiv preprint arXiv:2506.10716, 2025. URL\\nhttps://arxiv.org/abs/2506.10716v1.\\n155'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 155}, page_content='[1274] Zeping Yu and Sophia Ananiadou. Understanding multimodal llms: the mechanistic interpretability\\nof llava in visual question answering, arXiv preprint arXiv:2411.10950v2, 2024. URL https:\\n//arxiv.org/abs/2411.10950v2.\\n[1275] Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao\\nZeng, Eryk Helenowski, Chen Zhu, Si-Yuan Wang, Hao Ma, and Han Fang. Think smarter not harder:\\nAdaptive reasoning with inference aware optimization, arXiv preprint arXiv:2501.17974, 2025. URL\\nhttps://arxiv.org/abs/2501.17974v2.\\n[1276] Zhao yu Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen,\\nJiawei Gu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via\\nvisual tool reinforcement learning, arXiv preprint arXiv:2505.08617, 2025. URL https://arxiv.\\norg/abs/2505.08617v1.\\n[1277] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training\\nlanguage model agents to reflect via iterative self-training. arXiv preprint, 2025.\\n[1278] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason E\\nWeston. Self-rewarding language models. International Conference on Machine Learning, 2024.\\n[1279] Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, and Kang\\nLiu. Exploiting contextual knowledge in llms through v-usable information based layer enhancement.\\narXiv preprint, 2025.\\n[1280] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin\\nHou, Jinwei Chen, Peng-Tao Jiang, and Bo Li. Enhancing visual grounding for gui agents via\\nself-evolutionary reinforcement learning, arXiv preprint arXiv:2505.12370, 2025. URL https:\\n//arxiv.org/abs/2505.12370v2.\\n[1281] Murong Yue. A survey of large language model agents for question answering, arXiv preprint\\narXiv:2503.19213, 2025. URL https://arxiv.org/abs/2503.19213v1.\\n[1282] Xihang Yue, Linchao Zhu, and Yi Yang. Fragrel: Exploiting fragment-level relations in the external\\nmemory of large language models. Annual Meeting of the Association for Computational Linguistics,\\n2024.\\n[1283] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer\\nnetworks. Neural Information Processing Systems, 2019.\\n[1284] Ge Yuyao, Cheng Yiting, Wang Jia, Zhou Hanlin, and Chen Lizhe. Vision transformer based on\\nknowledge distillation in tcm image classification. In 2022 IEEE 5th International Conference on\\nComputer and Communication Engineering Technology (CCET), pages 120–125. IEEE, 2022.\\n[1285] M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Ontañón,\\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for\\nlonger sequences. Neural Information Processing Systems, 2020.\\n[1286] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection\\nwith multimodal large language models. International Journal of Computer Vision, 2023.\\n[1287] E. Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning,\\narXiv preprint arXiv:2203.14465, 2022. URL https://arxiv.org/abs/2203.14465v2.\\n156'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 156}, page_content='[1288] E. Zelikman, Eliana Lorch, Lester Mackey, and A. Kalai. Self-taught optimizer (stop): Recursively\\nself-improving code generation, arXiv preprint arXiv:2310.02304, 2023. URL https://arxiv.\\norg/abs/2310.02304v3.\\n[1289] Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, XuSheng Chen, and Yizhou\\nShan. The cap principle for llm serving: A survey of long-context large language model serving,\\narXiv preprint arXiv:2405.11299, 2024. URL https://arxiv.org/abs/2405.11299v2.\\n[1290] Ruihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. On the structural memory of llm agents,\\narXiv preprint arXiv:2412.15266, 2024. URL https://arxiv.org/abs/2412.15266v1.\\n[1291] Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and\\nTing Liu. itool: Reinforced fine-tuning with dynamic deficiency calibration for advanced tool use,\\narXiv preprint arXiv:2501.09766, 2025. URL https://arxiv.org/abs/2501.09766v4.\\n[1292] Yongcheng Zeng, Xinyu Cui, Xuanfa Jin, Guoqing Liu, Zexu Sun, Dong Li, Ning Yang, Jianye Hao,\\nHaifeng Zhang, and Jun Wang. Evolving llms’ self-refinement capability via iterative preference\\noptimization, arXiv preprint arXiv:2502.05605, 2025. URL https://arxiv.org/abs/2502.\\n05605v3.\\n[1293] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On\\ngenerative agents in recommendation. Annual International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval, 2023.\\n[1294] B Zhang, K Zhou, X Wei, and X Zhao.... Evaluating and improving tool-augmented computation-\\nintensive math reasoning. 2023. URL https://proceedings.neurips.cc/paper_files/\\npaper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_\\nBenchmarks.html.\\n[1295] Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and\\nEnhong Chen. Notellm-2: Multimodal large representation models for recommendation. Knowledge\\nDiscovery and Data Mining, 2024.\\n[1296] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao,\\nChao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025.\\n[1297] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification\\nwith differentiable earth mover’s distance and structured classifiers. In Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, pages 12203–12213, 2020.\\n[1298] Dan Zhang, G. Feng, Yang Shi, and D. Srinivasan. Physical safety and cyber security analysis of\\nmulti-agent systems: A survey of recent advances. IEEE/CAA Journal of Automatica Sinica, 2021.\\n[1299] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language\\nmodels are semi-parametric reinforcement learning agents. Neural Information Processing Systems,\\n2023.\\n[1300] Daoan Zhang, Weitong Zhang, Bing He, Jiang Zhang, Chenchen Qin, and Jianhua Yao. Dnagpt: A\\ngeneralized pre-trained tool for multiple dna sequence analysis tasks. bioRxiv, 2024.\\n157'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 157}, page_content='[1301] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-\\nllms: Recent advances in multimodal large language models. In Findings of the Association for\\nComputational Linguistics ACL 2024, pages 12401–12430, 2024.\\n[1302] Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, and Jinfeng\\nBai. Enhancing multimodal continual instruction tuning with branchlora. In Proceedings of the 63rd\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025.\\n[1303] Han Zhang, Langshi Zhou, and Hanfang Yang. Learning to retrieve and reason on knowledge graph\\nthrough active self-reflection, arXiv preprint arXiv:2502.14932, 2025. URL https://arxiv.org/\\nabs/2502.14932v1.\\n[1304] Hengyu Zhang. Sinklora: Enhanced efficiency and chat capabilities for long-context large lan-\\nguage models, arXiv preprint arXiv:2406.05678, 2024. URL https://arxiv.org/abs/2406.\\n05678v1.\\n[1305] Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, and Yashar Moshfeghi. Geoeval:\\nbenchmark for evaluating llms and multi-modal models on geometry problem-solving. 2024.\\n[1306] Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph\\nretrieval enhanced model for multi-hop knowledge base question answering. Annual Meeting of the\\nAssociation for Computational Linguistics, 2022.\\n[1307] Kai Zhang, Fubang Zhao, Yangyang Kang, and Xiaozhong Liu. Llm-based medical assistant personal-\\nization with short- and long-term memory coordination. North American Chapter of the Association\\nfor Computational Linguistics, 2023.\\n[1308] Kai Zhang, Yejin Kim, and Xiaozhong Liu. Personalized llm response generation with parameterized\\nmemory injection, arXiv preprint arXiv:2404.03565, 2025. URL https://arxiv.org/abs/2404.\\n03565.\\n[1309] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation.\\nAnnual Meeting of the Association for Computational Linguistics, 2023.\\n[1310] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with\\ntool-integrated agent systems for real-world repo-level coding challenges. Annual Meeting of the\\nAssociation for Computational Linguistics, 2024.\\n[1311] Kechi Zhang, Ge Li, Jia Li, Huangzhao Zhang, Jingjing Xu, Hao Zhu, Lecheng Wang, Yihong Dong,\\nJing Mai, Bin Gu, and Zhi Jin. Computational thinking reasoning in large language models, arXiv\\npreprint arXiv:2506.02658, 2025. URL https://arxiv.org/abs/2506.02658v2.\\n[1312] Ming-Liang Zhang, Zhong-Zhi Li, Fei Yin, Liang Lin, and Cheng-Lin Liu. Fuse, reason and verify:\\nGeometry problem solving with parsed clauses from diagram. 2024.\\n[1313] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin\\nKing, Xue Liu, and Chen Ma. A survey on test-time scaling in large language models: What, how,\\nwhere, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/\\n2503.24235v3.\\n158'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 158}, page_content='[1314] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z.\\nAgudelo, Peter Qian, and Tianlong Chen.\\nSymbiotic cooperation for web agents: Harnessing\\ncomplementary strengths of large and small llms, arXiv preprint arXiv:2502.07942, 2025. URL\\nhttps://arxiv.org/abs/2502.07942v2.\\n[1315] Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Patrik P. Süli, Rui Qin, and Fei-Yue Wang.\\nCoordfield: Coordination field for agentic uav task allocation in low-altitude urban scenarios, arXiv\\npreprint arXiv:2505.00091, 2025. URL https://arxiv.org/abs/2505.00091v3.\\n[1316] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei,\\nHenry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo\\nLiu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personaliza-\\ntion at test time, arXiv preprint arXiv:2506.06254, 2025. URL https://arxiv.org/abs/2506.\\n06254v1.\\n[1317] Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang,\\nand Hua zeng Chen. Trustuqa: A trustful framework for unified structured data question answering.\\nAAAI Conference on Artificial Intelligence, 2024.\\n[1318] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang,\\nDerong Xu, Zhaochen Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Process vs. outcome\\nreward: Which is better for agentic rag reinforcement learning, arXiv preprint arXiv:2505.14069,\\n2025. URL https://arxiv.org/abs/2505.14069v2.\\n[1319] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hi-\\nerarchical multi-agent framework for general-purpose task solving, arXiv preprint arXiv:2506.12508,\\n2025. URL https://arxiv.org/abs/2506.12508v2.\\n[1320] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep\\nvideo discovery: Agentic search with tool use for long-form video understanding, arXiv preprint\\narXiv:2505.18079, 2025. URL https://arxiv.org/abs/2505.18079v2.\\n[1321] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D.\\nManning, and J. Leskovec. Greaselm: Graph reasoning enhanced language models for question\\nanswering. International Conference on Learning Representations, 2022.\\n[1322] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: A versatile and\\nautonomous multi-agent system for web task execution with strategic exploration, arXiv preprint\\narXiv:2408.15978, 2024. URL https://arxiv.org/abs/2408.15978.\\n[1323] Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing Zheng. Reverse chain: A generic-rule for\\nllms to master multi-api planning, arXiv preprint arXiv:2310.04474, 2023. URL https://arxiv.\\norg/abs/2310.04474v3.\\n[1324] Youjia Zhang, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Ma-bert: Learning representation by\\nincorporating multi-attribute knowledge in transformers. Findings, 2021.\\n[1325] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min\\nZhang. Evaluating and steering modality preferences in multimodal large language model, arXiv\\npreprint arXiv:2505.20977v1, 2025. URL https://arxiv.org/abs/2505.20977v1.\\n159'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 159}, page_content='[1326] Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, and Jiawei Han.\\nAutomated mining of structured knowledge from text in the era of large language models. Knowledge\\nDiscovery and Data Mining, 2024.\\n[1327] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. Chain of agents:\\nLarge language models collaborating on long-context tasks. Neural Information Processing Systems,\\n2024.\\n[1328] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing\\nchain-of-action generation into reasoning models, arXiv preprint arXiv:2503.06580, 2025. URL\\nhttps://arxiv.org/abs/2503.06580v1.\\n[1329] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and\\nJi-Rong Wen. A survey on the memory mechanism of large language model based agents, arXiv\\npreprint arXiv:2404.13501, 2024. URL https://arxiv.org/abs/2404.13501v1.\\n[1330] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua\\nDong, and Ji-Rong Wen. Memsim: A bayesian simulator for evaluating memory of llm-based\\npersonal assistants, arXiv preprint arXiv:2409.20163, 2024. URL https://arxiv.org/abs/\\n2409.20163v1.\\n[1331] Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, and Zhenhua Dong. Memengine: A unified\\nand modular library for developing advanced memory of llm-based agents. The Web Conference,\\n2025.\\n[1332] Zheng Zhang, Liang Ding, Dazhao Cheng, Xuebo Liu, Min Zhang, and Dacheng Tao.\\nBliss:\\nRobust sequence-to-sequence learning via self-supervised input representation, arXiv preprint\\narXiv:2204.07837, 2022. URL https://arxiv.org/abs/2204.07837v2.\\n[1333] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao\\nSong, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o:\\nHeavy-hitter oracle for efficient generative inference of large language models. Neural Information\\nProcessing Systems, 2023.\\n[1334] Zhihan Zhang, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang.\\nLearn beyond the answer: Training language models with reflection for mathematical reasoning.\\nConference on Empirical Methods in Natural Language Processing, 2024.\\n[1335] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents.\\nAnnual Meeting of the Association for Computational Linguistics, 2023.\\n[1336] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang. Expel: Llm agents\\nare experiential learners. AAAI Conference on Artificial Intelligence, 2023.\\n[1337] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm\\nagents are experiential learners, arXiv preprint arXiv:2308.10144, 2024. URL https://arxiv.\\norg/abs/2308.10144.\\n[1338] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning\\non large-scale text-attributed graphs via variational inference. International Conference on Learning\\nRepresentations, 2022.\\n160'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 160}, page_content='[1339] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,\\nWentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey,\\narXiv preprint arXiv:2402.19473, 2024. URL https://arxiv.org/abs/2402.19473v6.\\n[1340] Pengyu Zhao, Zijian Jin, and Ning Cheng. An in-depth survey of large language model-based artificial\\nintelligence agents, arXiv preprint arXiv:2309.14365, 2023. URL https://arxiv.org/abs/\\n2309.14365v1.\\n[1341] Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu\\nNan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yongchun He, and\\nYanzhi Wang. 7b fully open source moxin-llm/vlm – from pretraining to grpo-based reinforcement\\nlearning enhancement. arXiv preprint, 2024.\\n[1342] Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, and Xiangyang Li. Knowpath: Knowledge-enhanced\\nreasoning via llm-generated inference paths over knowledge graphs, arXiv preprint arXiv:2502.12029,\\n2025. URL https://arxiv.org/abs/2502.12029v3.\\n[1343] Qifang Zhao, Weidong Ren, Tianyu Li, Xiaoxiao Xu, and Hong Liu. Graphgpt: Generative pre-trained\\ngraph eulerian transformer, arXiv preprint arXiv:2401.00529v3, 2023. URL https://arxiv.org/\\nabs/2401.00529v3.\\n[1344] Ruilin Zhao, Feng Zhao, Long Wang, Xianzhi Wang, and Guandong Xu. Kg-cot: Chain-of-thought\\nprompting of large language models over knowledge graphs for knowledge-aware question answering.\\nInternational Joint Conference on Artificial Intelligence, 2024.\\n[1345] Shangziqi Zhao, Jiahao Yuan, Guisong Yang, and Usman Naseem. Can pruning improve reason-\\ning? revisiting long-cot compression with capability in mind for better reasoning, arXiv preprint\\narXiv:2505.14582, 2025. URL https://arxiv.org/abs/2505.14582v1.\\n[1346] Shitian Zhao, Zhuowan Li, Yadong Lu, Alan L. Yuille, and Yan Wang. Causal-cog: A causal-effect\\nlook at context generation for boosting multi-modal language models. Computer Vision and Pattern\\nRecognition, 2023.\\n[1347] Tony Zhao, Eric Wallace, Shi Feng, D. Klein, and Sameer Singh. Calibrate before use: Improving\\nfew-shot performance of language models. International Conference on Machine Learning, 2021.\\n[1348] Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang\\nChe, Tat-Seng Chua, and Ting Liu. Trade-offs in large reasoning models: An empirical analysis of\\ndeliberative and adaptive reasoning over foundational capabilities, arXiv preprint arXiv:2503.17979,\\n2025. URL https://arxiv.org/abs/2503.17979v1.\\n[1349] Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, and Xiang Li. E2graphrag: Streamlining graph-based\\nrag for high efficiency and effectiveness. arXiv preprint, 2025.\\n[1350] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web\\nagent, if grounded. International Conference on Machine Learning, 2024.\\n[1351] Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao Wei, Tat seng Chua, and Qing Li. A picture\\nis worth a graph: A blueprint debate paradigm for multimodal reasoning. ACM Multimedia, 2024.\\n161'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 161}, page_content='[1352] Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,\\nMichael Ng, Xin Jiang, Zhenguo Li, and Yu Li. Dape: Data-adaptive positional encoding for length\\nextrapolation. Neural Information Processing Systems, 2024.\\n[1353] Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang, Ahsan Nawaz, and Mohamad Kassem. Automating\\nconstruction contract review using knowledge graph-enhanced large language models. Automation\\nin Construction, 2023.\\n[1354] Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large\\nlanguage models: A survey. ACM Computing Surveys, 2024.\\n[1355] Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, Zhongzhi Li, Yingying Zhang, Le Song, and Qianli\\nMa. Lifelongagentbench: Evaluating llm agents as lifelong learners, arXiv preprint arXiv:2505.11942,\\n2025. URL https://arxiv.org/abs/2505.11942v3.\\n[1356] Longtao Zheng, R. Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for\\ncomputer control. International Conference on Learning Representations, 2023.\\n[1357] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An.\\nSynapse: Trajectory-as-exemplar\\nprompting with memory for computer control, arXiv preprint arXiv:2306.07863, 2024.\\nURL\\nhttps://arxiv.org/abs/2306.07863.\\n[1358] Xu Zheng, Chenfei Liao, Yuqian Fu, Kaiyu Lei, Yuanhuiyi Lyu, Lutao Jiang, Bin Ren, Jialei Chen,\\nJiawen Wang, Chengxin Li, Linfeng Zhang, D. Paudel, Xuanjing Huang, Yu-Gang Jiang, N. Sebe,\\nDacheng Tao, L. V. Gool, and Xuming Hu. Mllms are deeply affected by modality bias, arXiv preprint\\narXiv:2505.18657v1, 2025. URL https://arxiv.org/abs/2505.18657v1.\\n[1359] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\\nDeepresearcher: Scaling deep research via reinforcement learning in real-world environments, arXiv\\npreprint arXiv:2504.03160, 2025. URL https://arxiv.org/abs/2504.03160v4.\\n[1360] Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger\\nvia verifying runtime execution step by step. Annual Meeting of the Association for Computational\\nLinguistics, 2024.\\n[1361] Rui Zhong, Yang Cao, Jun Yu, and M. Munetomo. Large language model assisted adversarial\\nrobustness neural architecture search. 2024 6th International Conference on Data-driven Optimization\\nof Complex Systems (DOCS), 2024.\\n[1362] Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing\\nlarge language models with long-term memory. AAAI Conference on Artificial Intelligence, 2023.\\n[1363] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing\\nlarge language models with long-term memory, arXiv preprint arXiv:2305.10250, 2023. URL\\nhttps://arxiv.org/abs/2305.10250.\\n[1364] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\\nagent tree search unifies reasoning acting and planning in language models. International Conference\\non Machine Learning, 2023.\\n162'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 162}, page_content='[1365] Bin Zhou, Xingwang Shen, Yuqian Lu, Xinyu Li, B. Hua, Tianyuan Liu, and Jinsong Bao. Semantic-\\naware event link reasoning over industrial knowledge graph embedding time series data. International\\nJournal of Production Research, 2022.\\n[1366] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, D. Schuurmans,\\nO. Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large\\nlanguage models. International Conference on Learning Representations, 2022.\\n[1367] Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi,\\nand Emine Yilmaz. Trustrag: Enhancing robustness and trustworthiness in rag, arXiv preprint\\narXiv:2501.00879, 2025. URL https://arxiv.org/abs/2501.00879.\\n[1368] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan\\nBisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for\\nbuilding autonomous agents. International Conference on Learning Representations, 2023.\\n[1369] Wangchunshu Zhou, Y. Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing\\nChen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang,\\nHuajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous\\nlanguage agents, arXiv preprint arXiv:2309.07870, 2023. URL https://arxiv.org/abs/2309.\\n07870v3.\\n[1370] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang,\\nSicong Liang, Xilin Liu, Yuchi Ma, and Yixiang Fang. In-depth analysis of graph-based rag in a\\nunified framework, arXiv preprint arXiv:2503.04338, 2025. URL https://arxiv.org/abs/\\n2503.04338v1.\\n[1371] Yuhang Zhou and Wei Ai. Teaching-assistant-in-the-loop: Improving knowledge distillation from\\nimperfect teacher models in low-budget scenarios. Annual Meeting of the Association for Computational\\nLinguistics, 2024.\\n[1372] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou,\\nTsung-Yi Ho, and Philip S. Yu. Trustworthiness in retrieval-augmented generation systems: A survey,\\narXiv preprint arXiv:2409.10102, 2024. URL https://arxiv.org/abs/2409.10102v1.\\n[1373] Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, and Lei Ma. Isr-llm: Iterative self-refined\\nlarge language model for long-horizon sequential task planning. IEEE International Conference on\\nRobotics and Automation, 2023.\\n[1374] Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi,\\nZhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, and Maosong Sun. Llm×mapreduce: Simplified\\nlong-sequence processing using large language models, arXiv preprint arXiv:2410.09342, 2024. URL\\nhttps://arxiv.org/abs/2410.09342v1.\\n[1375] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, B. Low,\\nand P. Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents,\\narXiv preprint arXiv:2506.15841, 2025. URL https://arxiv.org/abs/2506.15841v1.\\n[1376] Andrew Zhu, Liam Dugan, and Christopher Callison-Burch. Redel: A toolkit for llm-powered recursive\\nmulti-agent systems. Conference on Empirical Methods in Natural Language Processing, 2024.\\n163'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 163}, page_content='[1377] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient\\ncontext window extension of llms via positional skip-wise training. International Conference on\\nLearning Representations, 2023.\\n[1378] Hongyin Zhu. Metaaid 2.5: A secure framework for developing metaverse applications via large\\nlanguage models. arXiv preprint, 2023.\\n[1379] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan,\\nRuofei Zhang, and Huasha Zhao. Textgnn: Improving text encoder via graph neural network in\\nsponsored search. The Web Conference, 2021.\\n[1380] Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao\\nLin, Weiwen Liu, Ruiming Tang, Yong Yu, and Weinan Zhang. Evolutionary perspectives on the\\nevaluation of llm-based ai agents: A comprehensive survey, arXiv preprint arXiv:2506.11102, 2025.\\nURL https://arxiv.org/abs/2506.11102v1.\\n[1381] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao\\nTian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Haomin Wang, Weiye\\nXu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo,\\nYi Wang, Cong He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Ying Xiong, Wenwen Qu,\\nPeng Sun, Penglong Jiao, Lijun Wu, Kai Zhang, Hui Deng, Jiaye Ge, Kaiming Chen, Limin Wang, Min\\nDou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3:\\nExploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint\\narXiv:2504.10479v3, 2025. URL https://arxiv.org/abs/2504.10479v3.\\n[1382] Mingwei Zhu, Leigang Sha, Yu Shu, Kangjia Zhao, Tiancheng Zhao, and Jianwei Yin. Benchmarking\\nsequential visual input reasoning and prediction in multimodal large language models, arXiv preprint\\narXiv:2310.13473v1, 2023. URL https://arxiv.org/abs/2310.13473v1.\\n[1383] Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. Mitigating lost-in-retrieval problems\\nin retrieval augmented multi-hop question answering. 2025.\\n[1384] Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, and Wei Hu. When can large reasoning models save\\nthinking? mechanistic analysis of behavioral divergence in reasoning. 2025.\\n[1385] Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu,\\nand Conghui He. Grait: Gradient-driven refusal-aware instruction tuning for effective hallucination\\nmitigation. 2025.\\n[1386] Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, and Conghui He.\\nUtilize the flow before stepping into the same river twice: Certainty represented knowledge flow\\nfor refusal-aware instruction tuning. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nvolume 39, pages 26157–26165, 2025.\\n[1387] Tongyao Zhu, Qian Liu, L. Pang, Zhengbao Jiang, Min-Yen Kan, and Min Lin. Beyond memorization:\\nThe challenge of random memory access in language models. Annual Meeting of the Association for\\nComputational Linguistics, 2024.\\n[1388] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\\nLewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\\nGenerally capable agents for open-world environments via large language models with text-based\\n164'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'total_pages': 165, 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 164}, page_content='knowledge and memory, arXiv preprint arXiv:2305.17144, 2023. URL https://arxiv.org/\\nabs/2305.17144.\\n[1389] Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, and Eun Kyung Lee. Towards efficient key-value cache\\nmanagement for prefix prefilling in llm inference, arXiv preprint arXiv:2505.21919, 2025. URL\\nhttps://arxiv.org/abs/2505.21919v1.\\n[1390] Zhengqiu Zhu, Yong Zhao, Bin Chen, S. Qiu, Kai Xu, Quanjun Yin, Jin-Yu Huang, Zhong Liu, and Fei\\nWang. Conversational crowdsensing: A parallel intelligence powered novel sensing approach, arXiv\\npreprint arXiv:2402.06654, 2024. URL https://arxiv.org/abs/2402.06654v1.\\n[1391] Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, and Siqiang Luo. Graph-based\\napproaches and functionalities in retrieval-augmented generation: A comprehensive survey, arXiv\\npreprint arXiv:2504.10499, 2025. URL https://arxiv.org/abs/2504.10499v1.\\n[1392] Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang,\\nJie Fu, Xiang Yue, and Wenhu Chen. Structlm: Towards building generalist models for structured\\nknowledge grounding, arXiv preprint arXiv:2402.16671, 2024. URL https://arxiv.org/abs/\\n2402.16671v7.\\n[1393] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: A dataset for llm\\nquestion answering with external tools. Neural Information Processing Systems, 2023.\\n[1394] Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, and Mario Fritz. Proxyprompt: Securing\\nsystem prompts against prompt extraction attacks, arXiv preprint arXiv:2505.11459, 2025. URL\\nhttps://arxiv.org/abs/2505.11459v1.\\n[1395] Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin,\\nSaravan Rajmohan, Dongmei Zhang, and Qi Zhang. Efficientrag: Efficient retriever for multi-hop\\nquestion answering. In In Proceedings of the 2024 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 3392–3411, 2024.\\n[1396] Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, and Y. Zhuang. Triad: A framework\\nleveraging a multi-role llm-based agent to solve knowledge base question answering. Conference on\\nEmpirical Methods in Natural Language Processing, 2024.\\n[1397] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details\\nof benchmarking multimodal in-context learning. arXiv preprint, 2024.\\n[1398] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details\\nof multimodal in-context learning. International Conference on Learning Representations, 2024.\\n[1399] Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue\\nZhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe\\nJiang, and Philip S. Yu. A survey on large language model based human-agent systems. arXiv\\npreprint, 2025.\\n[1400] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bo Du. Pretraining language models with text-attributed\\nheterogeneous graphs. Conference on Empirical Methods in Natural Language Processing, 2023.\\n[1401] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting\\nlanguage models, arXiv preprint arXiv:2506.10943, 2025. URL https://arxiv.org/abs/2506.\\n10943v1.\\n165')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45dec7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12abee0",
   "metadata": {},
   "source": [
    "####  semantic Chunking and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "274b84b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "010ff22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0993cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8db6a795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embed_query(\"hi\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91965cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    min_chunk_size =200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60e92a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_doc = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2bb11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d8d2d",
   "metadata": {},
   "source": [
    "#### Vectorsdb : Zilliz(cloud)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592119d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a96c6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZILLIZ_CLOUD_URI = os.getenv(\"ZILLIZ_CLOUD_URI\")\n",
    "ZILLIZ_CLOUD_API_KEY = os.getenv(\"ZILLIZ_CLOUD_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e688f9",
   "metadata": {},
   "source": [
    "creating index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48f21f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import  Milvus\n",
    "\n",
    "vectorstore_FLAT = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": ZILLIZ_CLOUD_URI,\n",
    "        \"token\": ZILLIZ_CLOUD_API_KEY,\n",
    "        \"db_name\": \"default\"  # optional — use \"default\" unless you're using multiple databases\n",
    "    },\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "    consistency_level=\"Strong\",\n",
    "    drop_old=True,  # set to True if seeking to drop the collection with that name if it exists\n",
    "    collection_name=\"DATA_FLAT_INDEX\",  # this is  index name\n",
    ")\n",
    "\n",
    "vectorstore_HNSW = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": ZILLIZ_CLOUD_URI,\n",
    "        \"token\": ZILLIZ_CLOUD_API_KEY,\n",
    "        \"db_name\": \"default\"  # optional — use \"default\" unless you're using multiple databases\n",
    "    },\n",
    "    index_params={\"index_type\": \"HNSW\", \"metric_type\": \"L2\"},\n",
    "    consistency_level=\"Strong\",\n",
    "    drop_old=False,  # set to True if seeking to drop the collection with that name if it exists\n",
    "    collection_name=\"DATA_HNSW_INDEX\",  # this is  index name\n",
    ")\n",
    "\n",
    "\n",
    "vectorstore_IVF = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": ZILLIZ_CLOUD_URI,\n",
    "        \"token\": ZILLIZ_CLOUD_API_KEY,\n",
    "        \"db_name\": \"default\"  # optional — use \"default\" unless you're using multiple databases\n",
    "    },\n",
    "    index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"L2\"},\n",
    "    consistency_level=\"Strong\",\n",
    "    drop_old=False,  # set to True if seeking to drop the collection with that name if it exists\n",
    "    collection_name=\"DATA_IVF_INDEX\",  # this is  index name\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae6cfc",
   "metadata": {},
   "source": [
    "uploading data into vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fc14932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for _ in range(len(chunked_doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff29f6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e51aa6e7-eced-4582-bac5-96bb6bc714c5',\n",
       " '97844ef8-7c22-4602-9b20-656b767b7280',\n",
       " '3b5d1276-e844-4037-b1f9-85b78fb21f5c',\n",
       " 'b7bb6b03-1322-469f-81c3-ee58f225a820',\n",
       " '3d9cbf5a-b295-4fc5-b76f-b9bdc60b1a24',\n",
       " '0b6e885e-ac10-4d5e-b530-1832aa9bf677',\n",
       " 'deeec32a-05d0-4a37-b697-9a1d75444823',\n",
       " '60cb8ae3-66ac-46d4-997f-00ea1fe9adff',\n",
       " '0b90c69d-db54-4a46-8fc0-6f052f5e605f',\n",
       " 'ddbc7901-c25d-4d40-93da-ce935f48afad',\n",
       " 'edb7b300-922f-4a6b-97c6-e76c3fa2e675',\n",
       " 'a243d02f-69da-437e-82e0-adcceb7e847b',\n",
       " '3a7b7409-5902-4893-9d23-2132dfe551fe',\n",
       " 'a1b8ede7-55ed-4b1a-ad3f-6d24f503f823',\n",
       " '9c926aec-7311-4ab8-be6b-c5ad3ebd93f3',\n",
       " '77012593-ddf7-40f5-a1f9-d4b3fa3f9450',\n",
       " 'f6628e1f-9d29-474d-8684-72a9f5a8066b',\n",
       " '9cd6b341-2a06-4a0b-bbf4-af2b6a3eec89',\n",
       " 'eed98fac-d005-4fbc-a2b3-bc4fde1c60cd',\n",
       " '977e2f86-659b-4bb0-8031-a087bcfdb35c',\n",
       " '7d326c44-c9ee-415f-ba7e-825f1b17d94a',\n",
       " '43ca3a2e-8af8-4855-8b85-f1acd0cc6374',\n",
       " '4d86040a-de02-46b7-ab4c-af000dfd1b2b',\n",
       " '945695f7-ab47-431f-b1af-30e0ea15e7bb',\n",
       " '799c0aa9-9851-4e46-8bb0-2675a952ad49',\n",
       " '845d607e-5b04-47a5-ac48-d06b9aa87d6a',\n",
       " '41b9e7dd-1e3c-48fb-91a2-edcde7a2bab9',\n",
       " 'ed3c7318-d4b0-423b-a9ef-910b7b463176',\n",
       " '17082b52-f16f-42cb-a5fa-27335e247008',\n",
       " 'c8d55b17-a102-4dd9-985a-aa6ba453f283',\n",
       " 'af8c8d3e-87d7-466b-8618-0f3391e8fba0',\n",
       " '6b6e19f6-6478-422d-9e17-73f1128590da',\n",
       " '4a9141cb-69b3-4c2e-997c-fabcb6ab0d75',\n",
       " '99e6933f-1a69-48c0-83a5-4bc8b3739f9c',\n",
       " 'e12ffa58-6cec-451d-b442-018775c72077',\n",
       " 'fd222eb2-b53c-44f5-9218-4664f20616fd',\n",
       " '5dffc1db-b408-4115-b894-344d3e3288df',\n",
       " 'c2ffa049-ace5-4ee6-b0a6-cfe8eea8d6fd',\n",
       " '90431b41-3914-4e22-b5c5-a6117833ad5c',\n",
       " '639c3bc2-3089-40f5-8a66-a5db9c7b1fcd',\n",
       " 'f462040a-2a16-4163-936f-2b5c7d994496',\n",
       " 'e16e6cac-9535-47ce-80a8-c7c6d3e1beb9',\n",
       " 'e912dfb8-b3fc-4dde-b4a0-6ad980df299d',\n",
       " '87012f19-e06f-4cfe-b56e-d35e9e456f07',\n",
       " '41e57205-f815-4341-9dd6-d78063502ebc',\n",
       " 'e01808a2-5847-4943-84b7-05e076af3239',\n",
       " '0b25b279-9530-4bf5-bc03-41b08d72135a',\n",
       " '07fd31bc-8df7-4320-b3e2-94cd693aca0c',\n",
       " '578d12e4-5e0f-43d8-864e-0ec3f3127780',\n",
       " '7ac2856c-04b6-4b58-b354-8982faf81834',\n",
       " 'f9c9dc3c-f748-4712-bc25-48f0a480b135',\n",
       " '43a675e6-be2f-4c87-a487-209e83c9b016',\n",
       " '828fe60a-fb55-412d-9a94-50b67e09141d',\n",
       " '70eb3dcb-ca9a-45bc-a864-c4f4de9de4b7',\n",
       " '49009e61-c64d-4520-99d0-f7359c6f5829',\n",
       " 'f0c2dde1-9b6c-4901-83dd-68101a32ef19',\n",
       " 'c2e0b191-0209-4482-8004-e51903be66d9',\n",
       " 'cd52b6ce-e362-4f87-8f3c-b0d4a908dbe5',\n",
       " 'd402c78c-619c-4441-b88e-6f6b6ed79e61',\n",
       " '8be697c2-3e2a-4dd1-bc67-4fae261bf924',\n",
       " 'f2910066-6c5e-48ef-be76-c0349f91bf11',\n",
       " '8e488722-b3c6-4b8d-a388-8f1c92a0c5f8',\n",
       " '6a4c359f-0de3-466b-8821-6976c5c73240',\n",
       " '732dd337-b62a-40c8-9627-4e41c573e684',\n",
       " '3fd6a1d6-08b5-4d70-95d1-a3f67560d254',\n",
       " 'da2d0be8-92f3-4c7e-bb66-5e0995b305d7',\n",
       " '67962ef6-48d7-459d-babb-d0a87fbe2ed3',\n",
       " '7456c7aa-8982-4f47-9624-3d3308bc3b5a',\n",
       " '3b7fde6f-c440-40ab-ad3c-9d6540c95bb3',\n",
       " '15d18564-c129-4fc3-9128-1db6b40f87e3',\n",
       " 'e71d5261-e387-42a5-b06b-0a32bc9d166e',\n",
       " '90eb3f67-f374-45ae-a80d-a9e33da5334d',\n",
       " '488454fd-29f8-48ec-8842-9e1197360c46',\n",
       " 'dea8be79-59ba-46a6-bc62-9137c26e586f',\n",
       " '9ba6a5c9-60bf-4e5a-bfe1-740edd143404',\n",
       " '1ee0593e-6290-4b97-9677-7e7b7cd33f55',\n",
       " '0138f444-4f4b-416b-897a-2e4a16c77702',\n",
       " 'a302c440-e009-4f7b-bac8-be24b3dd66d3',\n",
       " 'cba4910c-7c4f-4df9-9932-3f0cf311df61',\n",
       " 'f5707fdd-0820-4b09-8d27-164258b323c9',\n",
       " 'dd8bb575-ea3d-4202-8bc2-a2a46475163c',\n",
       " '36019688-e43c-4ddd-bce1-69d71e38dc7d',\n",
       " 'ccb4054f-0447-442b-96f8-25759b7dd960',\n",
       " 'b09a93eb-6d10-431b-a4f6-28d14a6a4a91',\n",
       " 'ad5bd211-a022-40d3-b854-36bf75e82fb4',\n",
       " '92a04265-28cb-4c33-99e9-4662c2ec5bf8',\n",
       " '0d66c964-2b40-435d-8ad1-3ba454f396d5',\n",
       " '5efad879-e1dd-42e2-9126-77cba90395fc',\n",
       " '9ab10101-e7c9-4b1e-b763-df87094964e9',\n",
       " 'bd75ffe1-fccf-4fc6-8bff-14c4d3713947',\n",
       " 'c96a0e3e-a0c5-42b5-8717-0994c264b6a8',\n",
       " 'e93f00b7-ca4a-4aec-812b-8b396fe62571',\n",
       " 'e8ef3d43-16cf-40f3-9731-011bbad4001a',\n",
       " '2074668b-df63-40c2-a075-e5af321499f0',\n",
       " '75d9ddf0-9fce-4c2b-b067-d986586642f1',\n",
       " '5a9dfc52-6d74-4a15-b199-fc9f852544ae',\n",
       " '5143cd5f-8faa-47e3-8695-d144d2f488ea',\n",
       " '1f5e3f92-ade8-4f4e-bc61-36313bb8ab2f',\n",
       " 'c2e2e1a9-5ec7-4c68-a863-2ebd575bcbf5',\n",
       " '3b0cc4fb-a82c-4370-bf7d-1ffae8ebb6bc',\n",
       " '830c2bbb-4157-4cd6-a0a2-1caa2483a97c',\n",
       " 'db7c06fb-7aeb-41ca-bf9e-65914ea31038',\n",
       " 'a6eb751a-0018-4859-934f-cd4b0da75453',\n",
       " '8c618c49-1e98-4ede-8dec-f45894514773',\n",
       " '3071f69d-2f7f-420c-8ed1-b65a600fcfae',\n",
       " 'fd320a8e-2e7a-4023-b67f-3f0b53aac99c',\n",
       " '527ef0d5-4351-41ad-9b31-55bd25b440b5',\n",
       " '81efc8ae-bc68-49de-9da1-7cf6a129556e',\n",
       " '3eed0af4-e995-44d3-8e16-8cb0fabfc186',\n",
       " 'cf48229a-d7cb-4995-b5c3-03ce57be571f',\n",
       " 'f35569dd-ebfc-4b0c-a702-d52d5f4b3718',\n",
       " '09d6ad7c-9a36-4383-bd09-4e57054f95eb',\n",
       " '7307956a-9be5-49df-a7e6-838b99fa2d5a',\n",
       " 'd5bed910-0a25-442f-822a-19bd86234692',\n",
       " '34f93120-ced6-4d81-a591-9a995dc11bf1',\n",
       " '60eb3f27-f176-468a-9515-d7df87c7cf69',\n",
       " '2a3d9d91-ec56-427b-9d71-d78fc90efd76',\n",
       " '5e9ccbf3-bf8c-45bf-a93b-7ae3cbffb328',\n",
       " 'a0cf5844-f919-41a2-8d98-e8a5c49b145e',\n",
       " '92716618-1e63-4624-ae54-255f987dddfb',\n",
       " '27d29f25-f4e7-4ead-a87a-69ea17f46315',\n",
       " '5bce2683-6f76-4709-b78b-9d1d8ff3e220',\n",
       " 'a6364c96-d96f-4841-a46f-8a313758e002',\n",
       " 'e2adc659-3ba8-4b2e-a41b-c0afd362f90f',\n",
       " '278375b4-f93b-41fe-bcc2-9dc087f8861f',\n",
       " '463e5ac5-26e5-456e-bc1f-9f4592579cbc',\n",
       " '87a47995-bab0-4c5e-ae45-3f61e72c90ec',\n",
       " '3e4a0bc6-4a58-4764-ae8e-f1d10d5a2e0e',\n",
       " '9233aa76-3dc5-4da2-98b6-bad78faee03e',\n",
       " '7d9fdcb4-4bf4-40ed-beb1-043b0c0b991a',\n",
       " 'd867e099-7484-40ff-a140-c2070b260b95',\n",
       " 'f0bdf4db-e947-44d6-b97f-a298dd9b8fa3',\n",
       " '5848b812-a92a-4a75-9f1d-85efa0307bb9',\n",
       " 'b9dbd7de-0de0-481f-b4b9-93143c3eff36',\n",
       " 'a4dede05-08e6-4d4b-a6d5-5fe077fe7099',\n",
       " '650f4765-6c6d-4e5e-952d-accde584c750',\n",
       " '014d082a-7c2f-452c-9f9b-60f58e80b15a',\n",
       " '1d6472b5-ff3d-4182-9a99-70f78172e64c',\n",
       " 'e73ebaf1-4933-4c88-9cd4-c19e7fc29db1',\n",
       " 'd9abb608-c9bc-44ef-ba79-8e4003fcadde',\n",
       " 'fbeac743-abbd-4db3-879e-574eda9e0206',\n",
       " '8dd822b8-adda-49c5-bc47-833ccce6f860',\n",
       " '34ca2dc6-667c-426d-b25c-de96f8ad6890',\n",
       " '109602cc-ce3d-4790-a3ec-22552d75b312',\n",
       " '6e5566df-5a97-44d2-bb3d-08c981e29c1a',\n",
       " '5cc5a92a-16de-4386-b5ac-65bf9c39a4ae',\n",
       " '144b2f72-c297-4344-aba4-7c85c08c79b2',\n",
       " '9244d31b-47ba-4c98-8b00-1d6de5b7d45f',\n",
       " '159b56ab-2a12-476a-9eb6-2ff402511893',\n",
       " '3f075afa-d966-4314-a6b3-a3ca7f0ddbec',\n",
       " '80377614-cc3d-4715-96be-2e805a007851',\n",
       " '1afb37a5-32bf-4552-abfd-cde37ffcec50',\n",
       " '6f9188ae-3441-44ac-afd8-ef5a60af84cd',\n",
       " '48b4367a-8572-41de-96f2-337716b36f09',\n",
       " '464f655b-9da6-48d3-a10b-e9a048d9de14',\n",
       " 'ec5f20b1-6ff5-4d6c-a2db-afbacf25f071',\n",
       " '321c5184-45f9-4630-badc-20e06bee691b',\n",
       " '38b9a1c5-c317-429e-a43f-e05ce0083952',\n",
       " '35bc7e7f-6b58-4e74-8109-75dfe3f5b926',\n",
       " 'a22b1a56-16b7-40a6-948a-520afcf446e6',\n",
       " 'fac267ff-ea5a-4e0e-9f55-a7376baa3277',\n",
       " '33a5dfd6-a7f1-43b7-9f55-d4f70d2fe135',\n",
       " 'f9441455-7761-4dbf-bb31-443227f0b758',\n",
       " 'dfdaba37-f3a6-408b-a792-6ba2de498d40',\n",
       " 'a1653a34-a7e0-4594-ae2e-e5455d23a90a',\n",
       " '4913598a-ef72-439c-9c18-c6e0877213fd',\n",
       " 'c5d84c26-1c5f-4bf8-a44c-9bdcde268c48',\n",
       " 'c95f8657-0c9b-446a-a5ef-bd2bc5eac9b2',\n",
       " '4e530179-495d-4565-97b5-f71136a53041',\n",
       " '5aae49c6-29fc-4017-afcd-a285878b4660',\n",
       " '18456101-abc1-40fa-982b-cd4ad409df20',\n",
       " '53bd399d-19a9-482d-8c11-4b209a0409b8',\n",
       " '729997bb-632a-4a69-babf-6069cec69da8',\n",
       " '94fb7a6e-113b-43f6-9616-def9f8f2dcf1',\n",
       " 'fa3a6077-043b-484f-999b-656a784d4314',\n",
       " '5227b031-79f8-4828-bb00-7b60f1cb9fc4',\n",
       " 'dd642da1-7851-4d01-8e7b-942e3ad598a2',\n",
       " 'eab8b621-7336-4be9-b183-971d53e6db34',\n",
       " '7c9b17a5-bc3e-4857-99b9-90bff1ee97e5',\n",
       " '226450d5-ee50-44c8-a775-6e5090e8effc',\n",
       " '5bf11ae4-aef1-41c2-b8f2-9ca510c82e26',\n",
       " '3a87c5d2-bd55-488e-9e94-102a5190d6b6',\n",
       " 'dd22cab0-aa79-4d29-9cf9-cee1b964b4d4',\n",
       " 'b61131ae-826b-4073-b4b6-abad400cf05d',\n",
       " 'd8c137dd-be4f-4793-86ce-add26ec0e5b3',\n",
       " '887d18c2-0cb6-4ec6-9f6f-5e451b5e81a6',\n",
       " '8369f30f-ae6b-4f98-830c-62435979413e',\n",
       " '888259c2-2b8e-422a-b283-6022b038749a',\n",
       " 'b7bbc7c2-dabb-4a23-8d02-fd42afcc43db',\n",
       " '4e10894a-b18f-4895-a735-77d38f5ab39b',\n",
       " 'eee0ead8-efa9-4a86-958c-0b2638ac22f0',\n",
       " '07aa0e8d-4665-4529-9481-aebe6b4a6d64',\n",
       " '21f57257-8130-4a46-a6d4-3ae90123ac59',\n",
       " '64f09804-1c83-47f7-a58d-f64f6dc69df9',\n",
       " 'e856408d-5dd7-4c6b-8c59-2e833e4f6f3e',\n",
       " '9cb90680-f3c0-43cd-8a3e-a1129aaedf74',\n",
       " '864855e8-c17c-4c34-bd5a-ed47600bb409',\n",
       " 'f51a2fe6-37c7-49f3-86d5-e07adba3aa89',\n",
       " '2760254a-b321-43db-ba80-2731e4154f3d',\n",
       " '6aefb2aa-ee33-47a0-8186-708f39cd4173',\n",
       " '3297706f-421e-41ba-bd54-e34a45148216',\n",
       " 'b1ffc4c8-5ca4-4e89-8efd-0d847bc39318',\n",
       " '3e518cb1-7042-40dc-aae0-211a9aacc753',\n",
       " '59710036-8993-463d-9a9d-4f768b2a925c',\n",
       " 'e05a8bc4-0100-4e6f-8757-1ee9ad8b6069',\n",
       " 'fd7a2ad4-2013-4f2b-96f7-a9d1525116a8',\n",
       " '72654a2f-7d1e-490e-855d-c1d93b3e3db9',\n",
       " '29eecbb6-d657-42bf-9678-ce98ef68af42',\n",
       " '3d8663f0-f3fb-4e3e-8fc5-c0b65867f162',\n",
       " '76b6c1f9-2c34-4ea9-b270-308fa76589db',\n",
       " '45326c3f-3540-4d6b-b671-37884a134a1e',\n",
       " 'c795e3b1-c0d0-415d-8ecd-d1a2f0970d1d',\n",
       " 'decd8e78-6899-4652-8fa5-b28362e91aa9',\n",
       " '45835c10-ab19-49f4-ad1c-e445abc4f691',\n",
       " '1c84a883-8f77-4934-a014-e6be962ec67c',\n",
       " '914e8759-14e2-4b57-8154-ae9f5695b0f6',\n",
       " '811a9845-c296-4c53-b46e-fd56d08f4de8',\n",
       " '0e500d48-8124-4a60-a49b-3683f7ec37a1',\n",
       " '497c0112-6403-4c44-8590-93053dbb47f7',\n",
       " 'd6108dc2-8120-410a-8bc5-832603f832dd',\n",
       " 'c838ff78-e636-40a1-b710-9b74f47586cf',\n",
       " 'aa809a0c-196d-4e4c-9b5c-ba8a5df3522c',\n",
       " '084a5ecf-397e-4eaa-ac13-89e2b105ec40',\n",
       " 'acb9f018-e85d-4bdd-ab03-8e4d2d518047',\n",
       " '05b9a646-b849-4a8d-be27-ef721d93c9a8',\n",
       " '358ca084-a740-4e0a-a78b-41eb1e16053e',\n",
       " '689ce9a7-29c2-486a-8cd1-21db484b136a',\n",
       " '9adf77a6-691b-4546-b012-5c045f8d1860',\n",
       " 'dcd42e61-f087-4313-b542-ea77c78b193f',\n",
       " 'ecd7ba40-eb00-4397-866d-a221546edeab',\n",
       " '43ca556f-0130-4a93-9b50-903643ad62e2',\n",
       " '8d452118-76ce-42ef-bf28-048e2aec8c82',\n",
       " '95f55a2c-2536-4c57-b305-c3d6b8457354',\n",
       " '3582ea94-cb23-444b-8ae3-00ad98c8d690',\n",
       " '900ece3e-554a-4dc6-b883-190659fe22b9',\n",
       " '4451d21d-3aee-4acc-bc59-af506e01c409',\n",
       " '5217f4bd-7b72-4fa1-8cac-d37b780a42f7',\n",
       " 'c5c56301-6a07-40e3-9a11-90aad33571a5',\n",
       " '8711e807-6dc9-4e77-b801-b99718785744',\n",
       " '711ab9db-243a-4f67-a3b8-b261c80fe06d',\n",
       " '841f20b1-b95d-452d-9c7d-70543e4ec417',\n",
       " 'ea1aefac-746d-469d-9310-522f21701a4d',\n",
       " '75101445-6e00-4dfd-a9f7-e7401fd484ff',\n",
       " '53f3ff9e-bbc4-4ccd-81d7-abae1f39a10d',\n",
       " '20e00eea-ccec-4eaa-8436-597b99e84d9b',\n",
       " '1fe59ff5-fae5-4fb0-b30f-955b61d613d7',\n",
       " '01a2604e-a96f-4fee-b9ab-7f7d608a50b6',\n",
       " '8aadea6d-1022-4087-9dec-0ad517cd4aaf',\n",
       " '3ea56667-a890-4f71-99af-f42a566cf52c',\n",
       " 'eb209163-d127-4094-b6d7-7b41d638be04',\n",
       " '7f6d8253-eb02-4414-be26-811d8b8a8edd',\n",
       " 'e76af521-3ef4-4b20-a5cc-bd7e08bae688',\n",
       " '66ee68b2-f0cd-43c5-81b4-b3f9994afc7b',\n",
       " 'bef9b983-f116-4ef4-a5ba-49de925b8b8c',\n",
       " '43566037-6e9d-4276-b22e-92ce34a6c5f4',\n",
       " '4161462f-44e2-4f55-9fc2-47b3cbf272b5',\n",
       " 'ca6b277e-08ac-4d8d-85d6-ef0b08c05b39',\n",
       " '66882964-7840-49da-ad4b-b9948c39f1eb',\n",
       " 'c874e028-80e8-475a-93f0-8471b4d5c1e5',\n",
       " 'e53c426d-5aab-4a52-a013-d37a939fc750',\n",
       " '1ac854e7-106b-4ccb-adc2-e0ad54a690a3',\n",
       " '6f93bc2c-623c-4c51-b886-48d433b4c486',\n",
       " '2152b571-4bc6-4d44-81c6-5c4e84e1e012',\n",
       " 'b5813490-3621-412c-9637-6974e39831a6',\n",
       " '5d15ac28-4866-4042-b402-beaf0dd14b42',\n",
       " '87e50025-9c82-4eb4-99dd-dc555cdd15c5',\n",
       " '80a27c86-7744-4eca-9a26-a8480d265ed6',\n",
       " '421a5cdb-09f6-47eb-a1d8-1a2ed2ccd91f',\n",
       " '1ae1f88f-52a6-4091-b3a8-ab0f6fff118e',\n",
       " 'b310ff84-93a5-4486-a385-58f139241e81',\n",
       " '956a1fef-e56c-4e69-b4be-e0a6edd59c22',\n",
       " '7ebe8df7-4b2d-424c-a3b3-435f25ee45c1',\n",
       " '29800c6f-7a42-4fb1-8cf9-c8463e8ad936',\n",
       " '94fbba37-8fa9-4b15-9041-335ff79935b1',\n",
       " '1b2baecf-4622-4a85-8b83-d724d3348008',\n",
       " 'de4759e1-b246-4b67-a614-e7f5007b534d',\n",
       " 'f15feae4-b3a9-43aa-aa5f-f693fd63b910',\n",
       " '4a40926e-8bca-469e-8912-066dbbfe2acb',\n",
       " 'd7dfa524-213f-4aaf-8892-6997ab274d7e',\n",
       " '2227b3c3-b1a5-4d76-8dc5-c919b913c310',\n",
       " '0925c9ea-c614-4c11-9ca0-43db31242bd7',\n",
       " 'b5fcd7d0-53f4-494a-86b2-72b788d90df8',\n",
       " 'f7fc3f27-baca-49f2-bb48-7e962219756c',\n",
       " '9e682c4e-f794-47bc-980e-eba1841bead3',\n",
       " 'ccb6faf9-b323-4134-b3da-b832acdecb4a',\n",
       " 'f841365b-be9c-4d42-a237-83168a6e0ba8',\n",
       " 'd4aa87d7-8b81-450a-9bab-a8e06124f08d',\n",
       " '43a7ee2b-fd07-4550-9b19-28d74348c7ef',\n",
       " '6f165721-e194-4d90-ba7b-58942093d6b1',\n",
       " 'a0c864ff-0642-4d03-ba04-afb3bab6eb8c',\n",
       " 'c8a20214-8bf5-4a37-8ee7-1da54933ab17',\n",
       " 'c0f4b2df-3790-4cd5-959b-62157a0d816e',\n",
       " '7dd56b21-9741-4efa-a3ed-eb4b692c4779',\n",
       " '7de74b6b-3622-4cc7-994b-1212a4a4f17c',\n",
       " '5ddd8542-14e7-43be-a9ff-fd3887de8f78',\n",
       " '875a1c08-4978-4871-b1c1-d5216fd56083',\n",
       " '4b44dc5f-457e-4a9c-942f-4dd19bb94dca',\n",
       " 'bc581d65-d34a-4eac-a97f-ca1634a5d54c',\n",
       " '13f712b2-4a0f-423b-86cf-763ed091ec11',\n",
       " 'ea74f8c7-ff5e-4413-b84f-f228680e23f8',\n",
       " 'ecef10e3-b681-4739-8dad-3aab33e2ba2e',\n",
       " 'e8e363d1-af63-4e51-adb7-4f6627fb738e',\n",
       " '93390f7f-e951-43f8-8f99-1cab88c81675',\n",
       " 'a7a78201-df1d-4a70-8947-422585185bf9',\n",
       " 'fbe8f429-c038-4f41-8c4b-540854720bb9',\n",
       " '6234c251-ef52-462a-9059-af18cb820e33',\n",
       " '53da88e6-cc85-44e9-8dbf-78669c8602d2',\n",
       " 'c9fa739e-d21a-4bc7-911e-c5bec1443dab',\n",
       " '045200be-984f-4e16-bbee-f9b725ce477b',\n",
       " 'c1c08f2d-8b20-4668-9d19-39f4fc939648',\n",
       " '89811c3e-0e8d-498f-b0a5-88be9bff4cd7',\n",
       " 'e30d92df-2d1d-4732-b495-f4bd692d494c',\n",
       " '539833c0-3f9a-4704-b0e6-3b6dcc7fe608',\n",
       " '7515cec1-e69f-4777-8ae3-1dab6025278e',\n",
       " 'f17d2c6b-5747-46b8-b8fa-5f0135fc675b',\n",
       " 'a3fcf085-3194-49ec-84f9-6d11604b48c9',\n",
       " '0dd06431-b3d0-4086-b090-c8457158ab3b',\n",
       " '574c5934-0ce9-44f2-8a15-be6163132933',\n",
       " '9077811b-0477-4f6f-a2b9-293df1816a48',\n",
       " 'fe518658-67a2-47a9-ae5a-71a22346a862',\n",
       " '382f1553-fe51-4f7f-81df-fb02d748681b',\n",
       " '8db73372-0e8d-4698-96bc-7f25afcf50d9',\n",
       " '9fa22d16-8dce-44f8-b3f1-4067416ca2f6',\n",
       " 'efe2ba5b-e594-4af4-957a-17addf73d1d9',\n",
       " '326e3bd9-ec0b-4c62-b83f-334b72b175c5',\n",
       " '02591ed4-3c89-4f23-9d8f-ca4759eed6fc',\n",
       " '5d5f22a3-da34-4834-9aa6-38d55fc4a03c',\n",
       " '2babb7e1-7330-4ca7-b506-1bfed2b7f8c6',\n",
       " '8242ee4e-ef61-41a3-b613-7da19e6b229d',\n",
       " '8893d92a-4d07-4991-b35c-530f250cf8f7',\n",
       " '5ec3647c-9c80-4d0c-a32f-89b1cf1b5e77',\n",
       " 'd4530f2f-9c82-4421-8bbc-cd5e19aa0661',\n",
       " 'b4e078e9-b6cc-4c86-b153-00e336a1b512',\n",
       " '407bcd0e-56fc-46a5-9dc2-721e86f3b5f5',\n",
       " '1d5a86a8-2900-41c9-965e-e44cba24b0ad',\n",
       " '48b359b3-6fee-488c-a6bb-34bffa5fedd8',\n",
       " '56084199-0fbc-4999-baef-4ce90b80f036',\n",
       " 'ce789a2c-ac5d-4592-9d49-0df1b0342efa',\n",
       " '26a85d65-8915-4f07-8b5b-ee25e94215f9',\n",
       " 'a4646926-2ef9-4636-95b7-66391d2240a5',\n",
       " '900dcefe-1b21-43c9-bbe9-da5e2000f6f3',\n",
       " 'c01fda49-7bbf-4feb-9c09-1eae69673ff5',\n",
       " '4bd7fb37-706a-42c8-82f3-208af1c29377',\n",
       " 'd12513d1-5eff-40a1-90b1-61ca6004fc30',\n",
       " '40368b97-4396-4968-a7d5-78c6aa57a692',\n",
       " 'aa1d7e2d-7270-4750-a11c-7a4997901e16',\n",
       " '314423e2-2bef-4663-b631-2f39fd3393d7',\n",
       " '3c4f9f80-0b5e-4873-9e72-5bc3c638b401',\n",
       " 'de2f7a35-20e3-4bc7-a310-567b6387c175',\n",
       " 'dd2e392a-f22b-49f6-9d54-deb7753796ff',\n",
       " '3cab31a4-7d8c-4088-9a4d-a1b96e75193e',\n",
       " '9a366cda-d266-46a3-a391-39f37ba182c8',\n",
       " '8952e7f4-6162-4cdd-9ace-d8df235bb88c',\n",
       " '84251d20-4922-4904-be29-6f642427709b',\n",
       " 'ca2bd16b-5958-4a5b-b751-1e412df0f822',\n",
       " '0f1e625f-62d8-4d89-8431-d44e89017f56',\n",
       " '76853d8b-687d-49be-9c5c-b01c156773a3',\n",
       " 'ef7522d3-bf6e-4425-a43b-791d3fa62467',\n",
       " 'a6ac92cf-6bfa-4b80-8751-d41cbe1065bc',\n",
       " 'f1437424-799f-4b50-ad4f-d9ab98cd1eb6',\n",
       " 'cbd99cbd-69a9-44a3-b82b-17c3af34b04b',\n",
       " '1d674831-09f3-4f94-9eb2-563d67c476d3',\n",
       " '38a3a7a1-8d5a-43ad-a2d5-fdda82a6de18',\n",
       " '7401c96c-eb18-4b12-baba-8738f15fae2b',\n",
       " '50ff651d-9a2c-4938-ac50-0fd09ebb9d94',\n",
       " '194e0d0e-e452-44ff-9afa-6991106f24b5',\n",
       " '372940c6-a839-4e7d-b5de-fc9138f18507',\n",
       " '0c12c3b1-9e1b-44e0-83e9-47e2cbbf04f7',\n",
       " 'c04b2924-2d49-4812-925b-373f0f6aca48',\n",
       " '9150c5bc-6e64-4790-aa1e-47fe75d58c08',\n",
       " '97fbfa45-ebd2-4901-95cf-284b3701597d',\n",
       " '5f58f775-bcc8-49fb-8fb8-e920351841a4',\n",
       " '3bebf4fe-0f2e-4dba-bba9-3a0d1d75bf27',\n",
       " '299e67f0-24d5-409d-8c42-fa3e4a4e3e1e',\n",
       " 'e5e40a3f-d702-4284-ace1-de4c27ef22bb',\n",
       " 'bc572090-1d62-4d41-a0b6-811668c33ede',\n",
       " 'c13ac14c-4953-4f3b-93a0-9b0076f3d2b7',\n",
       " '93e007e4-aaa7-4162-9f1e-8b1cb8c3768f',\n",
       " '28934943-23df-4e97-ab32-73d1223dc899',\n",
       " '7982a677-c22b-4c7e-8544-f8afae9b60a7',\n",
       " '4ce566dd-f2e5-4f85-8552-ce10caa000d4',\n",
       " '61450ec9-ad9b-4c12-bec4-e787ec6eb560',\n",
       " '18e410d0-2b98-449b-b4d4-772794df6a42',\n",
       " '9951d174-7ad2-498f-a803-6531fb8449b5',\n",
       " '3e506f59-0079-418e-ae4c-8eae038ec2b0',\n",
       " 'd0b147f5-4553-47c0-b92b-d19113bcd605',\n",
       " '510dcaa6-8ccb-418d-9bd2-db17fa7bcb5b',\n",
       " '70148f59-e190-4e36-9600-cfd9f718aa73',\n",
       " 'd647f65a-06cd-4302-b121-2fb44b86e82a',\n",
       " '61f13d68-6e36-4def-954b-4427a67d2020',\n",
       " '4ed94e4f-6fe6-4a17-a204-5b88862152c5',\n",
       " 'b40e5f4f-560b-49a0-b2a6-8a51ffd4d035',\n",
       " 'a6b292cb-2db1-461d-921f-c0ced3539231',\n",
       " '75ec4b79-3236-4972-9e5b-68b3b0df65c3',\n",
       " 'dfa9009f-9e3a-48d9-9d1a-5d90172d2a7c',\n",
       " '5f15dd41-cc65-404f-b024-c6d6c9bec73f',\n",
       " '67ef66a2-43fe-4120-a60c-f1b9711572c0',\n",
       " '8e88d239-14f7-4c29-ac36-ec68debe62b7',\n",
       " '83f79dd6-062b-4089-b491-e9928b8158ca',\n",
       " 'd3d6ac11-479d-4761-97f7-294c7d30fd87',\n",
       " '00279393-24d8-43e9-8cd7-1fe4a411036f',\n",
       " '65a4b8e7-fa95-4f93-b90c-b2aea1bccb1d',\n",
       " '628f08fb-c0a1-4f07-bff5-2818853863f4',\n",
       " '61f294fb-f23d-42b2-b17c-27dee3b2c93d',\n",
       " '11d9985a-3f88-44ca-878e-c3a4abe69e88',\n",
       " 'ecdadbbb-e415-405e-a064-0aa45031816c',\n",
       " '16f92ee1-f2ce-402e-8cf2-6ddc95241020',\n",
       " 'b897ee09-ba35-4a03-a191-5040cbef1cdb',\n",
       " 'a3d334e5-2885-4bab-a440-cd788576a954',\n",
       " '37f054b5-397f-4f6d-b8e5-daa465921469',\n",
       " 'b524b795-292d-4d2d-ad64-cd6f94e8d7f6',\n",
       " 'b5434e4e-fdc6-40a1-a217-8604a6c1f6c0',\n",
       " '576ef67e-1484-4a76-ac2f-82cbe595116e',\n",
       " '41e745af-91f1-4fc3-98b1-85540b8c9169',\n",
       " '4234cdb7-77fb-4e0a-97ef-fe605769bbe2',\n",
       " '4fbfcbc6-d71c-4364-8912-5a4c72039d7c',\n",
       " '39023499-72f0-4a70-85ac-5d73d0c2b8b0',\n",
       " '8dcaf9cf-7650-468f-8ca0-19149c1129d6',\n",
       " '59c94489-6363-4311-a76d-08eb04947b2d',\n",
       " 'dc41e578-344a-4ef4-a215-cfc1af238b50',\n",
       " 'da917ab6-8de5-41ad-a677-b61ab43e4ae1',\n",
       " '0c71d4ca-889f-46eb-a412-3e854a1bf53d',\n",
       " '080acf2d-803a-4f28-8784-e108b591889b',\n",
       " 'bc04f87e-21d4-41c3-9343-475ec816a44b',\n",
       " 'e3d95bdb-19ea-4a9a-a3bf-9265c7e6e395',\n",
       " '0d4d9148-e347-43fb-a24b-8bb5cc73ead6',\n",
       " '2e95861e-c7f5-4f25-a856-ad006acf847a',\n",
       " '0da8313f-77e3-4831-9442-768257001695',\n",
       " 'ce927228-afe7-406b-87b0-2b53b12b989d',\n",
       " 'c8ddc386-2e62-482a-b979-7cc77ff71da8',\n",
       " 'f68ec89d-0262-4f31-bd58-4ca9414706f3',\n",
       " '7ec65ec0-bdaf-4682-805b-229f1f807fdc',\n",
       " '14cf08ff-f7ff-4c25-b4bc-d26eb64333f6',\n",
       " 'f34a841d-6bfb-4815-ba27-5251605f19cb',\n",
       " 'd5213c80-559d-4a2e-92cb-1644b928b3ff',\n",
       " '32b8ca38-b0ab-4bf4-b0fa-5a9fc7f85b0c',\n",
       " '49768024-c8aa-466b-9d4e-4ec6c8344d8b',\n",
       " '715f609d-b4de-409e-9a63-24a8a8350c23',\n",
       " '4579b415-a671-43e4-93c2-f3faee17f0cf',\n",
       " 'f58363e4-58a1-4b57-a4ed-ba5e094f28fc',\n",
       " '39ac8e13-cae5-42da-95db-3d0ca00b4b94',\n",
       " '46cdffe4-dd98-415d-9bc3-07b8d876fb19',\n",
       " '8886910d-52a4-4dc9-9f89-55770e5a534a',\n",
       " 'cff656f4-208f-4641-8e94-b94d87bc8224',\n",
       " '8344fa38-d708-497a-a083-ed77c0b659db',\n",
       " 'f444bbe8-6583-49c2-8286-765b07c4af95',\n",
       " 'cd916c2d-1fe4-463c-9980-8af62337ae08',\n",
       " '477c4bf6-3571-4821-bce8-c6aac09d868f',\n",
       " 'e3eee328-3864-462b-95b0-0c92d2e95134',\n",
       " 'a48d9450-4055-4d33-8b38-05d9a3684af4',\n",
       " '2aa28999-fa3f-44b0-96bc-8e07541941bf',\n",
       " '5ebd2df7-6660-4c8a-a322-9c1dae474cad',\n",
       " '42bc1a4e-2bd6-4e89-bf2b-7f830ee59245',\n",
       " 'cbcb0bf2-25c7-47b6-a8df-a8e7d1f17d08',\n",
       " 'd794f71e-2f43-4250-b6f8-5321a11a9d0e',\n",
       " '3b69edb1-27c2-40a6-bb39-fa4a43cb9e60',\n",
       " 'cb4ba505-fee9-43cb-bbf6-a43c789c11b8',\n",
       " 'b5337e78-d004-4ba2-a218-105082a79fc6',\n",
       " '5a51cf36-c7fa-4f83-866f-7286be4078b4',\n",
       " '48b66a6d-d34c-4b30-bb88-79cbcaad919b',\n",
       " '68ef774c-0c16-4395-9e34-94abdc5fb55b',\n",
       " 'f7ca7a2b-19da-4caa-abd9-8bc3e161d4bf',\n",
       " '2a6d9c2c-5926-46bb-a628-4cb7765c94ff',\n",
       " '0b6a11f2-780e-47dd-bb0a-5a12ebf82ff8',\n",
       " 'ba8842e6-31cc-4e47-bd04-0820d4ea2a01',\n",
       " '30e584b0-c42d-4398-94d7-0e045efc6f39',\n",
       " 'ace220bf-7bda-4f48-9e2e-97e4f3853e6d',\n",
       " 'c7cb26c9-060e-4ba7-8168-908196df9c3c',\n",
       " 'a3a90911-b339-4b0c-b8e2-ed996d076c7d',\n",
       " 'b44d30f1-2560-481c-8365-59f0eb7ac9c1',\n",
       " 'b1fed2e6-4989-4891-b624-b80db03f0242',\n",
       " '92045a5d-050d-4136-8b55-9ba0d2fd85e9',\n",
       " '72be7a7c-2e08-4302-b1be-aac30d672a50',\n",
       " '41c4fa50-b4d4-4f7f-8bdc-0dd24a25119c',\n",
       " '2c4c63ae-2d11-4285-a003-0ab9b05932f0',\n",
       " 'e856b182-638a-4406-9e2a-b2b1c087a8e7',\n",
       " '8ae53d27-cd3b-449e-aef6-4b4c3cd13143',\n",
       " '92c16ea1-bde7-4748-809d-87ffcf9ffc7b',\n",
       " '113714be-de1e-4b39-a5ab-27ed7389172d',\n",
       " 'b17fbe2b-6bfd-4167-b416-4e009e3c6cda',\n",
       " '59924c0b-a63a-45d6-8ca8-6a92002b3988',\n",
       " 'fff7aabc-fc65-4355-8e5f-e1111541f476',\n",
       " '8582dd5c-1343-4fed-8395-242d64b0aec3',\n",
       " '3ed13dca-aa99-4291-a5d5-8488581c5741',\n",
       " '74fbcaf1-c5b0-48ab-8a35-16592d78d815',\n",
       " 'cc15f6a9-6fa4-4c88-844c-4de99c91848a',\n",
       " '05a8ca59-e367-41ea-8d08-c750f5648b3b',\n",
       " 'c6b17483-fedd-41e2-82fe-735702c488a1',\n",
       " 'e1da55a6-3d2e-4b47-9469-3f2d9ab05d3c',\n",
       " '426b33e1-c7cc-4322-bc6b-6250c7744dcc',\n",
       " 'ab3bc78d-a010-4407-a9a8-14b749ea672d',\n",
       " 'f1756aa1-11aa-48f5-99fd-6a13af01042b',\n",
       " '04650c11-d26e-454d-8eaa-28b318f33985',\n",
       " 'a627834a-c0e9-4cbc-abe7-6b1272094aba',\n",
       " '9207b548-e0ae-480c-b2af-2959885eb595',\n",
       " '071edfea-34bf-4e3c-ad07-04c669912b94',\n",
       " '3be90402-1a3f-457f-9104-6c93d3d28be7',\n",
       " '786a14fc-da97-466e-b7f1-69f8359cb90f',\n",
       " 'c2d9dd4f-862c-4252-9762-6c5edfa31fdf',\n",
       " '169d88a0-7a67-49e0-9a81-dfa567472c16',\n",
       " '3c8cd8c6-cc43-432c-ae58-e8492821f239',\n",
       " '1aba8738-9857-48ce-8b2f-6715d3d187d2',\n",
       " '016e3d85-db01-4572-9903-777b2a3fb2dc',\n",
       " '45d05aaa-7c7a-49d7-861b-3064ce90a1ad',\n",
       " '6db533a8-b5ea-43b1-b43e-f91517fc82dc',\n",
       " '2b8e7cfb-93bf-4335-bb22-facaf8a6ac6e',\n",
       " '4c73479c-4d83-410f-a261-4ac36983bf8e',\n",
       " '1317733c-bca8-43f2-ba65-ed96bee497ee',\n",
       " '719b1779-6d24-4c05-a25c-93b05dc99e64',\n",
       " 'a3d5ac6a-da94-4674-bbad-0a8f67caf83d',\n",
       " '5ba4caac-cb94-4b44-af72-0b99b39c45ff',\n",
       " '49ef5739-a2d9-4cc3-a1f4-e998dcac5139',\n",
       " '402dfc9a-6c38-4762-ab35-359aee266408',\n",
       " '97bb5b71-d9d0-41db-bf00-bd34e4890850',\n",
       " '209056fe-92bd-4989-a0a9-b81cdec87bee',\n",
       " '180195d6-f47a-4a7d-bf6d-948f9924bd6a',\n",
       " 'dc038a6d-0111-4632-904b-d8da32dcf65a',\n",
       " 'eb194365-5907-4a86-a36a-b20600692318',\n",
       " '47c4f9aa-ce78-4924-bdb3-199a1d07642a',\n",
       " 'd60a20ec-2690-44e9-96eb-06b4c7a231eb',\n",
       " '5897bff9-cf5d-4875-a683-12f090083fe8',\n",
       " '47670ef2-45a4-4294-b938-ef0533f7e413',\n",
       " '7e4ee4b2-4336-4d94-a4a0-5fca8d64a02f',\n",
       " '8b8e2456-db93-435b-a8ff-ec4caa2b4b22']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_FLAT.add_documents(chunked_doc,ids=uuids)\n",
    "vectorstore_IVF.add_documents(chunked_doc,ids=uuids)\n",
    "vectorstore_HNSW.add_documents(chunked_doc,ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0dd75c",
   "metadata": {},
   "source": [
    "#### Creation of retriever  for vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ca74e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver_flat = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,             # total results to return\n",
    "        \"lambda_mult\": 0.5   # 0 = max diversity, 1 = max relevance\n",
    "    }\n",
    ")\n",
    "retreiver_HNSW = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,             # total results to return\n",
    "        \"lambda_mult\": 0.5   # 0 = max diversity, 1 = max relevance\n",
    "    }\n",
    ")\n",
    "retreiver_IVF = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 10,             # total results to return\n",
    "        \"lambda_mult\": 0.5   # 0 = max diversity, 1 = max relevance\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca1f1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAT took 2.5279 seconds\n",
      "[Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 128, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '67ef66a2-43fe-4120-a60c-f1b9711572c0'}, page_content='Taylor. The evolving landscape of llm- and vlm-\\nintegrated reinforcement learning. arXiv preprint, 2025. [935] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large\\nlanguage models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504. 02441v2. [936] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:\\nQuery, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv. org/abs/2404.14809v2. [937] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,\\narXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 50, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '87a47995-bab0-4c5e-ae45-3f61e72c90ec'}, page_content='Scaling Laws and Computational Efficiency\\nThe fundamental asymmetry between LLMs’ remarkable comprehension capabilities and their pronounced\\ngeneration limitations represents one of the most critical challenges in Context Engineering research. This comprehension-generation gap manifests across multiple dimensions including long-form output\\ncoherence, factual consistency maintenance, and planning sophistication, requiring investigation into whether\\nlimitations stem from architectural constraints, training methodologies, or fundamental computational\\nboundaries [835, 1132]. Long-form generation capabilities demand systematic investigation into planning mechanisms that can\\nmaintain coherence across thousands of tokens while preserving factual accuracy and logical consistency. Current systems exhibit significant performance degradation in extended generation tasks, highlighting the\\nneed for architectural innovations beyond traditional transformer paradigms. State space models including\\nMamba demonstrate potential for more efficient long sequence processing through linear scaling properties,\\nthough current implementations require substantial development to match transformer performance across\\ndiverse tasks [731, 1258, 347, 216]. 51'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 157, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': 'c2d9dd4f-862c-4252-9762-6c5edfa31fdf'}, page_content='[1313] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin\\nKing, Xue Liu, and Chen Ma. A survey on test-time scaling in large language models: What, how,\\nwhere, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/\\n2503.24235v3. 158'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 124, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '18e410d0-2b98-449b-b4d4-772794df6a42'}, page_content='Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami\\nAzam. A review on large language models: Architectures, applications, taxonomies, open issues and\\nchallenges. IEEE Access, 2024. [884] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian,\\nand S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv\\npreprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 90, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': 'b5813490-3621-412c-9637-6974e39831a6'}, page_content='[424] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. International Conference on Learning\\nRepresentations, 2021. [425] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen,\\nTao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning,\\narXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2. [426] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: A\\nnano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL\\nhttps://arxiv.org/abs/2504.15989v2. [427] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 84, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '53f3ff9e-bbc4-4ccd-81d7-abae1f39a10d'}, page_content='[352] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language\\ndescriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024. URL https://arxiv.org/abs/2406.13250v1. [353] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin,\\nFeng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei\\nYe, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek\\nRathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang\\nXu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby,\\nAndrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf,\\nChinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\\nFang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P. Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet\\nSingh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell,\\nMeng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi,\\nRamsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen\\nMa, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek\\nKumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun\\nMeng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin,\\nArsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang\\nYang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen\\nYang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho,\\nNikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam\\nXu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui,\\nVivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu,\\nYang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models,\\narXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1. [354] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph\\nstructured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066,\\n2023. URL https://arxiv.org/abs/2305.15066v2. [355] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents, arXiv preprint arXiv:2312.17259,\\n2024. URL https://arxiv.org/abs/2312.17259. 85'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 115, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '9a366cda-d266-46a3-a391-39f37ba182c8'}, page_content='Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small\\nlanguage models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506. 09342v2. [757] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent\\noperating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403. 16971v4. [758] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept\\ncomprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, page 12558–12575. Association for Computational Linguistics,\\n2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/\\n2024.emnlp-main.698. [759] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard:\\nFine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684,\\n2024. URL https://arxiv.org/abs/2410.02684. [760] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi\\nCheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint\\narXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597. [761] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. \"not aligned\" is not\\n\"malicious\": Being careful about hallucinations of large language models’ jailbreak, arXiv preprint\\narXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 121, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '299e67f0-24d5-409d-8c42-fa3e4a4e3e1e'}, page_content='D. Putten, and K. Batenburg. Agentic large\\nlanguage models, a survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/\\nabs/2503.23037v2. [844] Moritz Plenz and Anette Frank. Graph language models. Annual Meeting of the Association for\\nComputational Linguistics, 2024. [845] Sean M. Polyn, K. Norman, and M. Kahana. A context maintenance and retrieval model of organiza-\\ntional processes in free recall. Psychology Review, 2009. [846] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chain-\\nof-thought prompting: Pedagogical strategies for machines. International Conference on Computer\\nSupported Education, 2025. [847] V Porcu. The role of memory in llms: Persistent context for smarter conversations. Int.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 89, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '6f93bc2c-623c-4c51-b886-48d433b4c486'}, page_content='Storkey. Meta-learning in neural\\nnetworks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [416] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an\\nintriguing failure of llms: Long context window does not mean llms can analyze long sequences\\nflawlessly. International Conference on Computational Linguistics, 2024. [417] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language\\nmodels via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL\\nhttps://arxiv.org/abs/2408.15491v1. [418] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented\\nmultimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024. URL https://arxiv.org/abs/2411.10937v1.'), Document(metadata={'title': 'A Survey of Context Engineering for Large Language Models', 'subject': '', 'creationdate': '', 'page': 137, 'trapped': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'creationDate': '', 'creator': 'arXiv GenPDF (tex2pdf:)', 'modDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'moddate': '', 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 165, 'pk': '0d4d9148-e347-43fb-a24b-8bb5cc73ead6'}, page_content='[1044] Fernanda M De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores\\nFernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language\\nmodels. International Conference on Human Factors in Computing Systems, 2023. [1045] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken a step forward? IEEE Access, 2025. [1046] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models\\nversus fine-tuned vision transformers in image-based security applications. ACM Transactions on\\nIntelligent Systems and Technology, 2024. [1047] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms, arXiv preprint arXiv:2501.06322,\\n2025. URL https://arxiv.org/abs/2501.06322v1. [1048] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious\\ncode, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1. [1049] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta,\\nAshish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people\\nfor benchmarking interactive coding agents. Annual Meeting of the Association for Computational\\nLinguistics, 2024.')]\n",
      "HNSW took 2.0871 seconds\n",
      "[Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 128, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '67ef66a2-43fe-4120-a60c-f1b9711572c0'}, page_content='Taylor. The evolving landscape of llm- and vlm-\\nintegrated reinforcement learning. arXiv preprint, 2025. [935] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large\\nlanguage models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504. 02441v2. [936] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:\\nQuery, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv. org/abs/2404.14809v2. [937] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,\\narXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 50, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '87a47995-bab0-4c5e-ae45-3f61e72c90ec'}, page_content='Scaling Laws and Computational Efficiency\\nThe fundamental asymmetry between LLMs’ remarkable comprehension capabilities and their pronounced\\ngeneration limitations represents one of the most critical challenges in Context Engineering research. This comprehension-generation gap manifests across multiple dimensions including long-form output\\ncoherence, factual consistency maintenance, and planning sophistication, requiring investigation into whether\\nlimitations stem from architectural constraints, training methodologies, or fundamental computational\\nboundaries [835, 1132]. Long-form generation capabilities demand systematic investigation into planning mechanisms that can\\nmaintain coherence across thousands of tokens while preserving factual accuracy and logical consistency. Current systems exhibit significant performance degradation in extended generation tasks, highlighting the\\nneed for architectural innovations beyond traditional transformer paradigms. State space models including\\nMamba demonstrate potential for more efficient long sequence processing through linear scaling properties,\\nthough current implementations require substantial development to match transformer performance across\\ndiverse tasks [731, 1258, 347, 216]. 51'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 157, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': 'c2d9dd4f-862c-4252-9762-6c5edfa31fdf'}, page_content='[1313] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin\\nKing, Xue Liu, and Chen Ma. A survey on test-time scaling in large language models: What, how,\\nwhere, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/\\n2503.24235v3. 158'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 124, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '18e410d0-2b98-449b-b4d4-772794df6a42'}, page_content='Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami\\nAzam. A review on large language models: Architectures, applications, taxonomies, open issues and\\nchallenges. IEEE Access, 2024. [884] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian,\\nand S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv\\npreprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 90, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': 'b5813490-3621-412c-9637-6974e39831a6'}, page_content='[424] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. International Conference on Learning\\nRepresentations, 2021. [425] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen,\\nTao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning,\\narXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2. [426] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: A\\nnano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL\\nhttps://arxiv.org/abs/2504.15989v2. [427] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 84, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '53f3ff9e-bbc4-4ccd-81d7-abae1f39a10d'}, page_content='[352] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language\\ndescriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024. URL https://arxiv.org/abs/2406.13250v1. [353] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin,\\nFeng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei\\nYe, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek\\nRathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang\\nXu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby,\\nAndrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf,\\nChinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\\nFang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P. Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet\\nSingh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell,\\nMeng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi,\\nRamsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen\\nMa, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek\\nKumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun\\nMeng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin,\\nArsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang\\nYang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen\\nYang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho,\\nNikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam\\nXu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui,\\nVivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu,\\nYang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models,\\narXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1. [354] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph\\nstructured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066,\\n2023. URL https://arxiv.org/abs/2305.15066v2. [355] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents, arXiv preprint arXiv:2312.17259,\\n2024. URL https://arxiv.org/abs/2312.17259. 85'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 115, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '9a366cda-d266-46a3-a391-39f37ba182c8'}, page_content='Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small\\nlanguage models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506. 09342v2. [757] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent\\noperating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403. 16971v4. [758] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept\\ncomprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, page 12558–12575. Association for Computational Linguistics,\\n2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/\\n2024.emnlp-main.698. [759] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard:\\nFine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684,\\n2024. URL https://arxiv.org/abs/2410.02684. [760] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi\\nCheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint\\narXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597. [761] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. \"not aligned\" is not\\n\"malicious\": Being careful about hallucinations of large language models’ jailbreak, arXiv preprint\\narXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 121, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '299e67f0-24d5-409d-8c42-fa3e4a4e3e1e'}, page_content='D. Putten, and K. Batenburg. Agentic large\\nlanguage models, a survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/\\nabs/2503.23037v2. [844] Moritz Plenz and Anette Frank. Graph language models. Annual Meeting of the Association for\\nComputational Linguistics, 2024. [845] Sean M. Polyn, K. Norman, and M. Kahana. A context maintenance and retrieval model of organiza-\\ntional processes in free recall. Psychology Review, 2009. [846] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chain-\\nof-thought prompting: Pedagogical strategies for machines. International Conference on Computer\\nSupported Education, 2025. [847] V Porcu. The role of memory in llms: Persistent context for smarter conversations. Int.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 89, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '6f93bc2c-623c-4c51-b886-48d433b4c486'}, page_content='Storkey. Meta-learning in neural\\nnetworks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [416] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an\\nintriguing failure of llms: Long context window does not mean llms can analyze long sequences\\nflawlessly. International Conference on Computational Linguistics, 2024. [417] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language\\nmodels via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL\\nhttps://arxiv.org/abs/2408.15491v1. [418] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented\\nmultimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024. URL https://arxiv.org/abs/2411.10937v1.'), Document(metadata={'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'subject': '', 'modDate': '', 'creationDate': '', 'total_pages': 165, 'title': 'A Survey of Context Engineering for Large Language Models', 'keywords': '', 'creationdate': '', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'producer': 'pikepdf 8.15.1', 'moddate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'trapped': '', 'format': 'PDF 1.5', 'page': 137, 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '0d4d9148-e347-43fb-a24b-8bb5cc73ead6'}, page_content='[1044] Fernanda M De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores\\nFernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language\\nmodels. International Conference on Human Factors in Computing Systems, 2023. [1045] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken a step forward? IEEE Access, 2025. [1046] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models\\nversus fine-tuned vision transformers in image-based security applications. ACM Transactions on\\nIntelligent Systems and Technology, 2024. [1047] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms, arXiv preprint arXiv:2501.06322,\\n2025. URL https://arxiv.org/abs/2501.06322v1. [1048] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious\\ncode, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1. [1049] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta,\\nAshish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people\\nfor benchmarking interactive coding agents. Annual Meeting of the Association for Computational\\nLinguistics, 2024.')]\n",
      "IVF took 1.6862 seconds\n",
      "[Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 128, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '67ef66a2-43fe-4120-a60c-f1b9711572c0'}, page_content='Taylor. The evolving landscape of llm- and vlm-\\nintegrated reinforcement learning. arXiv preprint, 2025. [935] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large\\nlanguage models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504. 02441v2. [936] Wenbo Shang and Xin Huang. A survey of large language models on generative graph analytics:\\nQuery, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv. org/abs/2404.14809v2. [937] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: A trainable agent for role-playing,\\narXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 50, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '87a47995-bab0-4c5e-ae45-3f61e72c90ec'}, page_content='Scaling Laws and Computational Efficiency\\nThe fundamental asymmetry between LLMs’ remarkable comprehension capabilities and their pronounced\\ngeneration limitations represents one of the most critical challenges in Context Engineering research. This comprehension-generation gap manifests across multiple dimensions including long-form output\\ncoherence, factual consistency maintenance, and planning sophistication, requiring investigation into whether\\nlimitations stem from architectural constraints, training methodologies, or fundamental computational\\nboundaries [835, 1132]. Long-form generation capabilities demand systematic investigation into planning mechanisms that can\\nmaintain coherence across thousands of tokens while preserving factual accuracy and logical consistency. Current systems exhibit significant performance degradation in extended generation tasks, highlighting the\\nneed for architectural innovations beyond traditional transformer paradigms. State space models including\\nMamba demonstrate potential for more efficient long sequence processing through linear scaling properties,\\nthough current implementations require substantial development to match transformer performance across\\ndiverse tasks [731, 1258, 347, 216]. 51'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 157, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': 'c2d9dd4f-862c-4252-9762-6c5edfa31fdf'}, page_content='[1313] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin\\nKing, Xue Liu, and Chen Ma. A survey on test-time scaling in large language models: What, how,\\nwhere, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/\\n2503.24235v3. 158'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 124, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '18e410d0-2b98-449b-b4d4-772794df6a42'}, page_content='Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami\\nAzam. A review on large language models: Architectures, applications, taxonomies, open issues and\\nchallenges. IEEE Access, 2024. [884] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian,\\nand S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv\\npreprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 90, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': 'b5813490-3621-412c-9637-6974e39831a6'}, page_content='[424] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. International Conference on Learning\\nRepresentations, 2021. [425] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen,\\nTao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning,\\narXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2. [426] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: A\\nnano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL\\nhttps://arxiv.org/abs/2504.15989v2. [427] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 84, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '53f3ff9e-bbc4-4ccd-81d7-abae1f39a10d'}, page_content='[352] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language\\ndescriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024. URL https://arxiv.org/abs/2406.13250v1. [353] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen\\nZhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin,\\nFeng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei\\nYe, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek\\nRathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang\\nXu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby,\\nAndrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf,\\nChinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,\\nFang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P. Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet\\nSingh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell,\\nMeng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi,\\nRamsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen\\nMa, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek\\nKumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun\\nMeng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin,\\nArsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang\\nYang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen\\nYang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho,\\nNikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam\\nXu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui,\\nVivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu,\\nYang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models,\\narXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1. [354] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph\\nstructured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066,\\n2023. URL https://arxiv.org/abs/2305.15066v2. [355] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents, arXiv preprint arXiv:2312.17259,\\n2024. URL https://arxiv.org/abs/2312.17259. 85'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 115, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '9a366cda-d266-46a3-a391-39f37ba182c8'}, page_content='Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small\\nlanguage models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506. 09342v2. [757] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent\\noperating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403. 16971v4. [758] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept\\ncomprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods\\nin Natural Language Processing, page 12558–12575. Association for Computational Linguistics,\\n2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/\\n2024.emnlp-main.698. [759] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard:\\nFine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684,\\n2024. URL https://arxiv.org/abs/2410.02684. [760] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi\\nCheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint\\narXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597. [761] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. \"not aligned\" is not\\n\"malicious\": Being careful about hallucinations of large language models’ jailbreak, arXiv preprint\\narXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 121, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '299e67f0-24d5-409d-8c42-fa3e4a4e3e1e'}, page_content='D. Putten, and K. Batenburg. Agentic large\\nlanguage models, a survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/\\nabs/2503.23037v2. [844] Moritz Plenz and Anette Frank. Graph language models. Annual Meeting of the Association for\\nComputational Linguistics, 2024. [845] Sean M. Polyn, K. Norman, and M. Kahana. A context maintenance and retrieval model of organiza-\\ntional processes in free recall. Psychology Review, 2009. [846] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chain-\\nof-thought prompting: Pedagogical strategies for machines. International Conference on Computer\\nSupported Education, 2025. [847] V Porcu. The role of memory in llms: Persistent context for smarter conversations. Int.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 89, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '6f93bc2c-623c-4c51-b886-48d433b4c486'}, page_content='Storkey. Meta-learning in neural\\nnetworks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [416] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an\\nintriguing failure of llms: Long context window does not mean llms can analyze long sequences\\nflawlessly. International Conference on Computational Linguistics, 2024. [417] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language\\nmodels via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL\\nhttps://arxiv.org/abs/2408.15491v1. [418] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented\\nmultimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024. URL https://arxiv.org/abs/2411.10937v1.'), Document(metadata={'total_pages': 165, 'trapped': '', 'creationdate': '', 'file_path': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'source': '/Users/ebythomas/Github_Projects/Multimodal-RAG/data/2507.13334v1.pdf', 'format': 'PDF 1.5', 'title': 'A Survey of Context Engineering for Large Language Models', 'moddate': '', 'keywords': '', 'modDate': '', 'creationDate': '', 'author': 'Lingrui Mei; Jiayu Yao; Yuyao Ge; Yiwei Wang; Baolong Bi; Yujun Cai; Jiazhi Liu; Mingyu Li; Zhong-Zhi Li; Duzhen Zhang; Chenlin Zhou; Jiayi Mao; Tianze Xia; Jiafeng Guo; Shenghua Liu', 'page': 137, 'subject': '', 'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'pk': '0d4d9148-e347-43fb-a24b-8bb5cc73ead6'}, page_content='[1044] Fernanda M De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores\\nFernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language\\nmodels. International Conference on Human Factors in Computing Systems, 2023. [1045] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken a step forward? IEEE Access, 2025. [1046] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models\\nversus fine-tuned vision transformers in image-based security applications. ACM Transactions on\\nIntelligent Systems and Technology, 2024. [1047] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: A survey of llms, arXiv preprint arXiv:2501.06322,\\n2025. URL https://arxiv.org/abs/2501.06322v1. [1048] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious\\ncode, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1. [1049] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta,\\nAshish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps and people\\nfor benchmarking interactive coding agents. Annual Meeting of the Association for Computational\\nLinguistics, 2024.')]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query = \"performance of Large Language Models (LLMs)\"\n",
    "\n",
    "def benchmark_retriever(name, retriever):\n",
    "    start = time.time()\n",
    "    results = retriever.invoke(query)\n",
    "    duration = time.time() - start\n",
    "    print(f\"{name} took {duration:.4f} seconds\")\n",
    "    print(results)\n",
    "\n",
    "# Run benchmarks\n",
    "docs_flat = benchmark_retriever(\"FLAT\", retreiver_flat)\n",
    "docs_hnsw = benchmark_retriever(\"HNSW\", retreiver_HNSW)\n",
    "docs_ivf = benchmark_retriever(\"IVF\", retreiver_IVF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea9658",
   "metadata": {},
   "source": [
    "### RAG  pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model =ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
